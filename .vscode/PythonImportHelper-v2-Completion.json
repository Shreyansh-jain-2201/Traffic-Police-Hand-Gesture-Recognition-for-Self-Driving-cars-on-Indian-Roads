[
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "audio",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "core",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "processor",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "core",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "processor",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "vision",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "core",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "processor",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "vision",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "processor",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "core",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "processor",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "vision",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "processor",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "audio",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "core",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "processor",
        "importPath": "tflite_support.task",
        "description": "tflite_support.task",
        "isExtraImport": true,
        "detail": "tflite_support.task",
        "documentation": {}
    },
    {
        "label": "utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "utils",
        "description": "utils",
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "Plotter",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "Plotter",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "tf_roll",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "data_gen",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "rcParams",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "rcParams",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "flatbuffers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "flatbuffers",
        "description": "flatbuffers",
        "detail": "flatbuffers",
        "documentation": {}
    },
    {
        "label": "metadata_schema_py_generated",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "metadata",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "metadata_schema_py_generated",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "metadata_schema_py_generated",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "metadata_schema_py_generated",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "metadata",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "metadata_schema_py_generated",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "metadata",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "schema_py_generated",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "metadata",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "metadata_schema_py_generated",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "metadata",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "metadata",
        "importPath": "tflite_support",
        "description": "tflite_support",
        "isExtraImport": true,
        "detail": "tflite_support",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AnyStr",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Collection",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Collection",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AnyStr",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AnyStr",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AnyStr",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Text",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Text",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Text",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Text",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Text",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Text",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Text",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "Category",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Person",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "BodyPart",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Person",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "person_from_keypoints_with_scores",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "BodyPart",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "KeyPoint",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Person",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Point",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Rectangle",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "BodyPart",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "KeyPoint",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "BodyPart",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Person",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "person_from_keypoints_with_scores",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "BodyPart",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Person",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Person",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Point",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Rectangle",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Person",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "BodyPart",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "KeyPoint",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Person",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Point",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Rectangle",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Person",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "Person",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "BodyPart",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "person_from_keypoints_with_scores",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "example_generation_movielens",
        "importPath": "data",
        "description": "data",
        "isExtraImport": true,
        "detail": "data",
        "documentation": {}
    },
    {
        "label": "unittest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unittest",
        "description": "unittest",
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "mock",
        "importPath": "unittest",
        "description": "unittest",
        "isExtraImport": true,
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "mock",
        "importPath": "unittest",
        "description": "unittest",
        "isExtraImport": true,
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "Classifier",
        "importPath": "ml.classifier",
        "description": "ml.classifier",
        "isExtraImport": true,
        "detail": "ml.classifier",
        "documentation": {}
    },
    {
        "label": "Movenet",
        "importPath": "ml.movenet",
        "description": "ml.movenet",
        "isExtraImport": true,
        "detail": "ml.movenet",
        "documentation": {}
    },
    {
        "label": "Movenet",
        "importPath": "ml.movenet",
        "description": "ml.movenet",
        "isExtraImport": true,
        "detail": "ml.movenet",
        "documentation": {}
    },
    {
        "label": "BoundingBoxTracker",
        "importPath": "tracker",
        "description": "tracker",
        "isExtraImport": true,
        "detail": "tracker",
        "documentation": {}
    },
    {
        "label": "KeypointTracker",
        "importPath": "tracker",
        "description": "tracker",
        "isExtraImport": true,
        "detail": "tracker",
        "documentation": {}
    },
    {
        "label": "TrackerConfig",
        "importPath": "tracker",
        "description": "tracker",
        "isExtraImport": true,
        "detail": "tracker",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "Posenet",
        "importPath": "ml.posenet",
        "description": "ml.posenet",
        "isExtraImport": true,
        "detail": "ml.posenet",
        "documentation": {}
    },
    {
        "label": "Track",
        "importPath": "tracker.tracker",
        "description": "tracker.tracker",
        "isExtraImport": true,
        "detail": "tracker.tracker",
        "documentation": {}
    },
    {
        "label": "Tracker",
        "importPath": "tracker.tracker",
        "description": "tracker.tracker",
        "isExtraImport": true,
        "detail": "tracker.tracker",
        "documentation": {}
    },
    {
        "label": "Track",
        "importPath": "tracker.tracker",
        "description": "tracker.tracker",
        "isExtraImport": true,
        "detail": "tracker.tracker",
        "documentation": {}
    },
    {
        "label": "Track",
        "importPath": "tracker.tracker",
        "description": "tracker.tracker",
        "isExtraImport": true,
        "detail": "tracker.tracker",
        "documentation": {}
    },
    {
        "label": "Tracker",
        "importPath": "tracker.tracker",
        "description": "tracker.tracker",
        "isExtraImport": true,
        "detail": "tracker.tracker",
        "documentation": {}
    },
    {
        "label": "Track",
        "importPath": "tracker.tracker",
        "description": "tracker.tracker",
        "isExtraImport": true,
        "detail": "tracker.tracker",
        "documentation": {}
    },
    {
        "label": "BoundingBoxTracker",
        "importPath": "tracker.bounding_box_tracker",
        "description": "tracker.bounding_box_tracker",
        "isExtraImport": true,
        "detail": "tracker.bounding_box_tracker",
        "documentation": {}
    },
    {
        "label": "KeypointTrackerConfig",
        "importPath": "tracker.config",
        "description": "tracker.config",
        "isExtraImport": true,
        "detail": "tracker.config",
        "documentation": {}
    },
    {
        "label": "TrackerConfig",
        "importPath": "tracker.config",
        "description": "tracker.config",
        "isExtraImport": true,
        "detail": "tracker.config",
        "documentation": {}
    },
    {
        "label": "KeypointTrackerConfig",
        "importPath": "tracker.config",
        "description": "tracker.config",
        "isExtraImport": true,
        "detail": "tracker.config",
        "documentation": {}
    },
    {
        "label": "TrackerConfig",
        "importPath": "tracker.config",
        "description": "tracker.config",
        "isExtraImport": true,
        "detail": "tracker.config",
        "documentation": {}
    },
    {
        "label": "TrackerConfig",
        "importPath": "tracker.config",
        "description": "tracker.config",
        "isExtraImport": true,
        "detail": "tracker.config",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "KeypointTracker",
        "importPath": "tracker.keypoint_tracker",
        "description": "tracker.keypoint_tracker",
        "isExtraImport": true,
        "detail": "tracker.keypoint_tracker",
        "documentation": {}
    },
    {
        "label": "abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "abc",
        "description": "abc",
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractproperty",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "enum",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "enum",
        "description": "enum",
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Classifier",
        "importPath": "ml",
        "description": "ml",
        "isExtraImport": true,
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "Movenet",
        "importPath": "ml",
        "description": "ml",
        "isExtraImport": true,
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "MoveNetMultiPose",
        "importPath": "ml",
        "description": "ml",
        "isExtraImport": true,
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "Posenet",
        "importPath": "ml",
        "description": "ml",
        "isExtraImport": true,
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "Movenet",
        "importPath": "ml",
        "description": "ml",
        "isExtraImport": true,
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "Posenet",
        "importPath": "ml",
        "description": "ml",
        "isExtraImport": true,
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "enum_type_wrapper",
        "importPath": "google.protobuf.internal",
        "description": "google.protobuf.internal",
        "isExtraImport": true,
        "detail": "google.protobuf.internal",
        "documentation": {}
    },
    {
        "label": "enum_type_wrapper",
        "importPath": "google.protobuf.internal",
        "description": "google.protobuf.internal",
        "isExtraImport": true,
        "detail": "google.protobuf.internal",
        "documentation": {}
    },
    {
        "label": "descriptor",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "reflection",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "symbol_database",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "text_format",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "text_format",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "text_format",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "text_format",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "text_format",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "text_format",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "text_format",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "descriptor",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "reflection",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "symbol_database",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "text_format",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "text_format",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "text_format",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "text_format",
        "importPath": "google.protobuf",
        "description": "google.protobuf",
        "isExtraImport": true,
        "detail": "google.protobuf",
        "documentation": {}
    },
    {
        "label": "attr",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "attr",
        "description": "attr",
        "detail": "attr",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "abc",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "input_config_generated_pb2",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "model_config",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "input_config_generated_pb2",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "model_config",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "input_config_generated_pb2",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "input_config_generated_pb2",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "input_config_generated_pb2",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "input_config_generated_pb2",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "input_config_generated_pb2",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "model_config",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "input_config_generated_pb2",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "model_config",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "input_config_generated_pb2",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "model_config",
        "importPath": "configs",
        "description": "configs",
        "isExtraImport": true,
        "detail": "configs",
        "documentation": {}
    },
    {
        "label": "context_encoder",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "dotproduct_similarity",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "input_pipeline",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "label_encoder",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "losses",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "metrics",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "context_encoder",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "dotproduct_similarity",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "label_encoder",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "input_pipeline",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "losses",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "metrics",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "recommendation_model",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "input_pipeline",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "recommendation_model_launcher",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "recommendation_model",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "conv_1d_time_stacked_model",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "speech_model",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "prepare_model_settings",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "common",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "common",
        "description": "common",
        "detail": "common",
        "documentation": {}
    },
    {
        "label": "py_environment",
        "importPath": "tf_agents.environments",
        "description": "tf_agents.environments",
        "isExtraImport": true,
        "detail": "tf_agents.environments",
        "documentation": {}
    },
    {
        "label": "tf_py_environment",
        "importPath": "tf_agents.environments",
        "description": "tf_agents.environments",
        "isExtraImport": true,
        "detail": "tf_agents.environments",
        "documentation": {}
    },
    {
        "label": "array_spec",
        "importPath": "tf_agents.specs",
        "description": "tf_agents.specs",
        "isExtraImport": true,
        "detail": "tf_agents.specs",
        "documentation": {}
    },
    {
        "label": "tensor_spec",
        "importPath": "tf_agents.specs",
        "description": "tf_agents.specs",
        "isExtraImport": true,
        "detail": "tf_agents.specs",
        "documentation": {}
    },
    {
        "label": "time_step",
        "importPath": "tf_agents.trajectories",
        "description": "tf_agents.trajectories",
        "isExtraImport": true,
        "detail": "tf_agents.trajectories",
        "documentation": {}
    },
    {
        "label": "reverb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "reverb",
        "description": "reverb",
        "detail": "reverb",
        "documentation": {}
    },
    {
        "label": "planestrike_py_environment",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "planestrike_py_environment",
        "description": "planestrike_py_environment",
        "detail": "planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "tensorflow_probability",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow_probability",
        "description": "tensorflow_probability",
        "detail": "tensorflow_probability",
        "documentation": {}
    },
    {
        "label": "tf_agents",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tf_agents",
        "description": "tf_agents",
        "detail": "tf_agents",
        "documentation": {}
    },
    {
        "label": "reinforce_agent",
        "importPath": "tf_agents.agents.reinforce",
        "description": "tf_agents.agents.reinforce",
        "isExtraImport": true,
        "detail": "tf_agents.agents.reinforce",
        "documentation": {}
    },
    {
        "label": "py_driver",
        "importPath": "tf_agents.drivers",
        "description": "tf_agents.drivers",
        "isExtraImport": true,
        "detail": "tf_agents.drivers",
        "documentation": {}
    },
    {
        "label": "policy_saver",
        "importPath": "tf_agents.policies",
        "description": "tf_agents.policies",
        "isExtraImport": true,
        "detail": "tf_agents.policies",
        "documentation": {}
    },
    {
        "label": "py_tf_eager_policy",
        "importPath": "tf_agents.policies",
        "description": "tf_agents.policies",
        "isExtraImport": true,
        "detail": "tf_agents.policies",
        "documentation": {}
    },
    {
        "label": "reverb_replay_buffer",
        "importPath": "tf_agents.replay_buffers",
        "description": "tf_agents.replay_buffers",
        "isExtraImport": true,
        "detail": "tf_agents.replay_buffers",
        "documentation": {}
    },
    {
        "label": "reverb_utils",
        "importPath": "tf_agents.replay_buffers",
        "description": "tf_agents.replay_buffers",
        "isExtraImport": true,
        "detail": "tf_agents.replay_buffers",
        "documentation": {}
    },
    {
        "label": "common",
        "importPath": "tf_agents.utils",
        "description": "tf_agents.utils",
        "isExtraImport": true,
        "detail": "tf_agents.utils",
        "documentation": {}
    },
    {
        "label": "gym",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gym",
        "description": "gym",
        "detail": "gym",
        "documentation": {}
    },
    {
        "label": "spaces",
        "importPath": "gym",
        "description": "gym",
        "isExtraImport": true,
        "detail": "gym",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "rmtree",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_namespace_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "linen",
        "importPath": "flax",
        "description": "flax",
        "isExtraImport": true,
        "detail": "flax",
        "documentation": {}
    },
    {
        "label": "tensorboard",
        "importPath": "flax.metrics",
        "description": "flax.metrics",
        "isExtraImport": true,
        "detail": "flax.metrics",
        "documentation": {}
    },
    {
        "label": "jax",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jax",
        "description": "jax",
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "random",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "jax2tf",
        "importPath": "jax.experimental",
        "description": "jax.experimental",
        "isExtraImport": true,
        "detail": "jax.experimental",
        "documentation": {}
    },
    {
        "label": "jax.numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jax.numpy",
        "description": "jax.numpy",
        "detail": "jax.numpy",
        "documentation": {}
    },
    {
        "label": "optax",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "optax",
        "description": "optax",
        "detail": "optax",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tensorflow.compat.v1",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow.compat.v1",
        "description": "tensorflow.compat.v1",
        "detail": "tensorflow.compat.v1",
        "documentation": {}
    },
    {
        "label": "estimator",
        "importPath": "tensorflow.compat.v1",
        "description": "tensorflow.compat.v1",
        "isExtraImport": true,
        "detail": "tensorflow.compat.v1",
        "documentation": {}
    },
    {
        "label": "estimator",
        "importPath": "tensorflow.compat.v1",
        "description": "tensorflow.compat.v1",
        "isExtraImport": true,
        "detail": "tensorflow.compat.v1",
        "documentation": {}
    },
    {
        "label": "keras",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "keras",
        "description": "keras",
        "detail": "keras",
        "documentation": {}
    },
    {
        "label": "backend",
        "importPath": "keras",
        "description": "keras",
        "isExtraImport": true,
        "detail": "keras",
        "documentation": {}
    },
    {
        "label": "backend",
        "importPath": "keras",
        "description": "keras",
        "isExtraImport": true,
        "detail": "keras",
        "documentation": {}
    },
    {
        "label": "pathlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pathlib",
        "description": "pathlib",
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "ConfusionMatrix",
        "importPath": "pandas_ml",
        "description": "pandas_ml",
        "isExtraImport": true,
        "detail": "pandas_ml",
        "documentation": {}
    },
    {
        "label": "Callback",
        "importPath": "keras.callbacks",
        "description": "keras.callbacks",
        "isExtraImport": true,
        "detail": "keras.callbacks",
        "documentation": {}
    },
    {
        "label": "ModelCheckpoint",
        "importPath": "keras.callbacks",
        "description": "keras.callbacks",
        "isExtraImport": true,
        "detail": "keras.callbacks",
        "documentation": {}
    },
    {
        "label": "ReduceLROnPlateau",
        "importPath": "keras.callbacks",
        "description": "keras.callbacks",
        "isExtraImport": true,
        "detail": "keras.callbacks",
        "documentation": {}
    },
    {
        "label": "TensorBoard",
        "importPath": "keras.callbacks",
        "description": "keras.callbacks",
        "isExtraImport": true,
        "detail": "keras.callbacks",
        "documentation": {}
    },
    {
        "label": "prepare_words_list",
        "importPath": "generator",
        "description": "generator",
        "isExtraImport": true,
        "detail": "generator",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "generator",
        "description": "generator",
        "isExtraImport": true,
        "detail": "generator",
        "documentation": {}
    },
    {
        "label": "prepare_words_list",
        "importPath": "generator",
        "description": "generator",
        "isExtraImport": true,
        "detail": "generator",
        "documentation": {}
    },
    {
        "label": "wget",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wget",
        "description": "wget",
        "detail": "wget",
        "documentation": {}
    },
    {
        "label": "tarfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tarfile",
        "description": "tarfile",
        "detail": "tarfile",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "xrange",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "xrange",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "range",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "range",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "zip",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "range",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "zip",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "keras.layers",
        "description": "keras.layers",
        "isExtraImport": true,
        "detail": "keras.layers",
        "documentation": {}
    },
    {
        "label": "l2",
        "importPath": "keras.regularizers",
        "description": "keras.regularizers",
        "isExtraImport": true,
        "detail": "keras.regularizers",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "keras.models",
        "description": "keras.models",
        "isExtraImport": true,
        "detail": "keras.models",
        "documentation": {}
    },
    {
        "label": "ConfusionMatrixCallback",
        "importPath": "callbacks",
        "description": "callbacks",
        "isExtraImport": true,
        "detail": "callbacks",
        "documentation": {}
    },
    {
        "label": "get_classes",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "VideoClassifier",
        "importPath": "video_classifier",
        "description": "video_classifier",
        "isExtraImport": true,
        "detail": "video_classifier",
        "documentation": {}
    },
    {
        "label": "VideoClassifierOptions",
        "importPath": "video_classifier",
        "description": "video_classifier",
        "isExtraImport": true,
        "detail": "video_classifier",
        "documentation": {}
    },
    {
        "label": "Category",
        "importPath": "video_classifier",
        "description": "video_classifier",
        "isExtraImport": true,
        "detail": "video_classifier",
        "documentation": {}
    },
    {
        "label": "VideoClassifier",
        "importPath": "video_classifier",
        "description": "video_classifier",
        "isExtraImport": true,
        "detail": "video_classifier",
        "documentation": {}
    },
    {
        "label": "VideoClassifierOptions",
        "importPath": "video_classifier",
        "description": "video_classifier",
        "isExtraImport": true,
        "detail": "video_classifier",
        "documentation": {}
    },
    {
        "label": "generate_lib",
        "importPath": "tensorflow_docs.api_generator",
        "description": "tensorflow_docs.api_generator",
        "isExtraImport": true,
        "detail": "tensorflow_docs.api_generator",
        "documentation": {}
    },
    {
        "label": "public_api",
        "importPath": "tensorflow_docs.api_generator",
        "description": "tensorflow_docs.api_generator",
        "isExtraImport": true,
        "detail": "tensorflow_docs.api_generator",
        "documentation": {}
    },
    {
        "label": "Children",
        "importPath": "tensorflow_docs.api_generator.public_api",
        "description": "tensorflow_docs.api_generator.public_api",
        "isExtraImport": true,
        "detail": "tensorflow_docs.api_generator.public_api",
        "documentation": {}
    },
    {
        "label": "tensorflow_examples",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow_examples",
        "description": "tensorflow_examples",
        "detail": "tensorflow_examples",
        "documentation": {}
    },
    {
        "label": "tflite_model_maker",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "audio_classifier",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "audio_classifier",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "image_classifier",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "image_classifier",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "question_answer",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "recommendation",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "recommendation",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "text_classifier",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "text_classifier",
        "importPath": "tflite_model_maker",
        "description": "tflite_model_maker",
        "isExtraImport": true,
        "detail": "tflite_model_maker",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "fire",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fire",
        "description": "fire",
        "detail": "fire",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "compat",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "test_util",
        "importPath": "tensorflow_examples.lite.model_maker.core",
        "description": "tensorflow_examples.lite.model_maker.core",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "configs",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "audio_classifier",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "configs",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "image_classifier",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "object_detector",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "question_answer",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "recommendation",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "text_classifier",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "searcher",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "configs",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "configs",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "configs",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "configs",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "hub_loader",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "classification_model",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "audio_classifier",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "configs",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "custom_model",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "classification_model",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "custom_model",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "hub_loader",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "metadata_writer_for_image_classifier",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "classification_model",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "hub_loader",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "image_preprocessing",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "train_image_classifier_lib",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "image_classifier",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "image_classifier_test",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "image_preprocessing",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "configs",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util_test",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "configs",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "custom_model",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "object_detector",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "custom_model",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "question_answer",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "question_answer_test",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "custom_model",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "recommendation",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "searcher",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "classification_model",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "text_classifier",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "text_classifier_test",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "custom_model",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "model_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task",
        "description": "tensorflow_examples.lite.model_maker.core.task",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task",
        "documentation": {}
    },
    {
        "label": "image_classification_demo",
        "importPath": "tensorflow_examples.lite.model_maker.demo",
        "description": "tensorflow_examples.lite.model_maker.demo",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.demo",
        "documentation": {}
    },
    {
        "label": "question_answer_demo",
        "importPath": "tensorflow_examples.lite.model_maker.demo",
        "description": "tensorflow_examples.lite.model_maker.demo",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.demo",
        "documentation": {}
    },
    {
        "label": "text_classification_demo",
        "importPath": "tensorflow_examples.lite.model_maker.demo",
        "description": "tensorflow_examples.lite.model_maker.demo",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.demo",
        "documentation": {}
    },
    {
        "label": "image_classification_demo",
        "importPath": "tensorflow_examples.lite.model_maker.demo",
        "description": "tensorflow_examples.lite.model_maker.demo",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.demo",
        "documentation": {}
    },
    {
        "label": "question_answer_demo",
        "importPath": "tensorflow_examples.lite.model_maker.demo",
        "description": "tensorflow_examples.lite.model_maker.demo",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.demo",
        "documentation": {}
    },
    {
        "label": "text_classification_demo",
        "importPath": "tensorflow_examples.lite.model_maker.demo",
        "description": "tensorflow_examples.lite.model_maker.demo",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.demo",
        "documentation": {}
    },
    {
        "label": "audio_classification_demo",
        "importPath": "tensorflow_examples.lite.model_maker.demo",
        "description": "tensorflow_examples.lite.model_maker.demo",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.demo",
        "documentation": {}
    },
    {
        "label": "custom_model_demo",
        "importPath": "tensorflow_examples.lite.model_maker.demo",
        "description": "tensorflow_examples.lite.model_maker.demo",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.demo",
        "documentation": {}
    },
    {
        "label": "image_classification_demo",
        "importPath": "tensorflow_examples.lite.model_maker.demo",
        "description": "tensorflow_examples.lite.model_maker.demo",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.demo",
        "documentation": {}
    },
    {
        "label": "recommendation_demo",
        "importPath": "tensorflow_examples.lite.model_maker.demo",
        "description": "tensorflow_examples.lite.model_maker.demo",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.demo",
        "documentation": {}
    },
    {
        "label": "text_classification_demo",
        "importPath": "tensorflow_examples.lite.model_maker.demo",
        "description": "tensorflow_examples.lite.model_maker.demo",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.demo",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "absltest",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "parameterized",
        "importPath": "absl.testing",
        "description": "absl.testing",
        "isExtraImport": true,
        "detail": "absl.testing",
        "documentation": {}
    },
    {
        "label": "cli",
        "importPath": "tensorflow_examples.lite.model_maker.cli",
        "description": "tensorflow_examples.lite.model_maker.cli",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.cli",
        "documentation": {}
    },
    {
        "label": "api_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "deprecated_api",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "golden_api_doc",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "api_gen",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "api_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "golden_api_doc",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "include",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "api_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "api_gen",
        "importPath": "tensorflow_examples.lite.model_maker.core.api",
        "description": "tensorflow_examples.lite.model_maker.core.api",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api",
        "documentation": {}
    },
    {
        "label": "tensorflow.compat.v2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow.compat.v2",
        "description": "tensorflow.compat.v2",
        "detail": "tensorflow.compat.v2",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "dataclasses",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dataclasses",
        "description": "dataclasses",
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "ImageClassifierDataLoader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "description": "tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "documentation": {}
    },
    {
        "label": "ImageClassifierDataLoader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "description": "tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tensorflow_examples.lite.model_maker.core.export_format",
        "description": "tensorflow_examples.lite.model_maker.core.export_format",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "audio_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "image_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "object_detector_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "text_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "image_searcher_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "metadata_loader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "searcher_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "text_searcher_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "audio_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "image_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "metadata_loader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "searcher_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "image_searcher_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "metadata_loader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "metadata_loader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "object_detector_dataloader_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "object_detector_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "object_detector_dataloader_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "recommendation_config",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "recommendation_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "recommendation_testutil",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "recommendation_config",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "searcher_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "text_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "searcher_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "text_searcher_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "recommendation_testutil",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "object_detector_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "recommendation_config",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "recommendation_testutil",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "audio_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "data_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "image_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "object_detector_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "object_detector_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "text_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "data_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "recommendation_config",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "recommendation_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "recommendation_testutil",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "searcher_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "image_searcher_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "searcher_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "text_dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "data_util",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "recommendation_testutil",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util",
        "description": "tensorflow_examples.lite.model_maker.core.data_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader",
        "description": "tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader",
        "description": "tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader",
        "documentation": {}
    },
    {
        "label": "RecommendationDataLoader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util.recommendation_dataloader",
        "description": "tensorflow_examples.lite.model_maker.core.data_util.recommendation_dataloader",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util.recommendation_dataloader",
        "documentation": {}
    },
    {
        "label": "QuestionAnswerDataLoader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util.text_dataloader",
        "description": "tensorflow_examples.lite.model_maker.core.data_util.text_dataloader",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util.text_dataloader",
        "documentation": {}
    },
    {
        "label": "TextClassifierDataLoader",
        "importPath": "tensorflow_examples.lite.model_maker.core.data_util.text_dataloader",
        "description": "tensorflow_examples.lite.model_maker.core.data_util.text_dataloader",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.data_util.text_dataloader",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "importPath": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "audio_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "audio_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "audio_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "image_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "text_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "object_detector_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "recommendation_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "util",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "text_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "audio_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "audio_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "image_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "image_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "object_detector_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "text_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "recommendation_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "text_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "text_spec",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "description": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.model_spec",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "wavfile",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "wavfile",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "tensorflow_datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow_datasets",
        "description": "tensorflow_datasets",
        "detail": "tensorflow_datasets",
        "documentation": {}
    },
    {
        "label": "base_options",
        "importPath": "tensorflow_lite_support.python.task.core",
        "description": "tensorflow_lite_support.python.task.core",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.python.task.core",
        "documentation": {}
    },
    {
        "label": "base_options",
        "importPath": "tensorflow_lite_support.python.task.core",
        "description": "tensorflow_lite_support.python.task.core",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.python.task.core",
        "documentation": {}
    },
    {
        "label": "embedding_options_pb2",
        "importPath": "tensorflow_lite_support.python.task.processor.proto",
        "description": "tensorflow_lite_support.python.task.processor.proto",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.python.task.processor.proto",
        "documentation": {}
    },
    {
        "label": "embedding_options_pb2",
        "importPath": "tensorflow_lite_support.python.task.processor.proto",
        "description": "tensorflow_lite_support.python.task.processor.proto",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.python.task.processor.proto",
        "documentation": {}
    },
    {
        "label": "image_embedder",
        "importPath": "tensorflow_lite_support.python.task.vision",
        "description": "tensorflow_lite_support.python.task.vision",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.python.task.vision",
        "documentation": {}
    },
    {
        "label": "image_embedder",
        "importPath": "tensorflow_lite_support.python.task.vision",
        "description": "tensorflow_lite_support.python.task.vision",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.python.task.vision",
        "documentation": {}
    },
    {
        "label": "image_searcher",
        "importPath": "tensorflow_lite_support.python.task.vision",
        "description": "tensorflow_lite_support.python.task.vision",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.python.task.vision",
        "documentation": {}
    },
    {
        "label": "tensor_image",
        "importPath": "tensorflow_lite_support.python.task.vision.core",
        "description": "tensorflow_lite_support.python.task.vision.core",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.python.task.vision.core",
        "documentation": {}
    },
    {
        "label": "tensor_image",
        "importPath": "tensorflow_lite_support.python.task.vision.core",
        "description": "tensorflow_lite_support.python.task.vision.core",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.python.task.vision.core",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "coco_metric",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "coco_metric",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "coco_metric",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "inference",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "nms_np",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "coco_metric",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "inference",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "iou_utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "coco_metric",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "efficientdet_arch",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "nms_np",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "det_model_fn",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "det_model_fn",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "hparams_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "inference",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "inference",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet",
        "documentation": {}
    },
    {
        "label": "label_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "efficientdet_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "eval_tflite",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "label_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "train",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "train_lib",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "label_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "fpn_configs",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "tfmot",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "util_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "anchors",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "efficientdet_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "label_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "util_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "anchors",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "label_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "efficientdet_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "efficientdet_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "label_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "util_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "infer_lib",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "anchors",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "efficientdet_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "tfmot",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "train_lib",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "util_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "anchors",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "efficientdet_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "label_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "util_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "label_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "anchors",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "anchors",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "efficientdet_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "fpn_configs",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "efficientdet_keras",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "label_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.keras",
        "documentation": {}
    },
    {
        "label": "filecmp",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "filecmp",
        "description": "filecmp",
        "detail": "filecmp",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "etree",
        "importPath": "lxml",
        "description": "lxml",
        "isExtraImport": true,
        "detail": "lxml",
        "documentation": {}
    },
    {
        "label": "etree",
        "importPath": "lxml",
        "description": "lxml",
        "isExtraImport": true,
        "detail": "lxml",
        "documentation": {}
    },
    {
        "label": "JpegImagePlugin",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "PIL.Image",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL.Image",
        "description": "PIL.Image",
        "detail": "PIL.Image",
        "documentation": {}
    },
    {
        "label": "create_pascal_tfrecord",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "documentation": {}
    },
    {
        "label": "tfrecord_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "documentation": {}
    },
    {
        "label": "label_map_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "documentation": {}
    },
    {
        "label": "tfrecord_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "documentation": {}
    },
    {
        "label": "tfrecord_util",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset",
        "documentation": {}
    },
    {
        "label": "input_config_pb2",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "model_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "input_config_pb2",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "model_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "input_config_pb2",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "model_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "input_config_pb2",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "input_config_pb2",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "input_config_pb2",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "input_config_pb2",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "input_config_pb2",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "model_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "input_config_pb2",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "model_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "input_config_pb2",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "model_config",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs",
        "documentation": {}
    },
    {
        "label": "example_generation_movielens",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data",
        "documentation": {}
    },
    {
        "label": "example_generation_movielens",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data",
        "documentation": {}
    },
    {
        "label": "example_generation_movielens",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data",
        "documentation": {}
    },
    {
        "label": "input_pipeline",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "recommendation_model",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "recommendation_model",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "input_pipeline",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "metrics",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "recommendation_model_launcher",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "context_encoder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "dotproduct_similarity",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "input_pipeline",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "label_encoder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "losses",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "metrics",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "context_encoder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "dotproduct_similarity",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "label_encoder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "input_pipeline",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "losses",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "metrics",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "recommendation_model",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "input_pipeline",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "recommendation_model_launcher",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "recommendation_model",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "description": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model",
        "documentation": {}
    },
    {
        "label": "base_options_pb2",
        "importPath": "tensorflow_lite_support.python.task.core.proto",
        "description": "tensorflow_lite_support.python.task.core.proto",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.python.task.core.proto",
        "documentation": {}
    },
    {
        "label": "input_pipeline",
        "importPath": "official.nlp.bert",
        "description": "official.nlp.bert",
        "isExtraImport": true,
        "detail": "official.nlp.bert",
        "documentation": {}
    },
    {
        "label": "tokenization",
        "importPath": "official.nlp.bert",
        "description": "official.nlp.bert",
        "isExtraImport": true,
        "detail": "official.nlp.bert",
        "documentation": {}
    },
    {
        "label": "configs",
        "importPath": "official.nlp.bert",
        "description": "official.nlp.bert",
        "isExtraImport": true,
        "detail": "official.nlp.bert",
        "documentation": {}
    },
    {
        "label": "run_squad_helper",
        "importPath": "official.nlp.bert",
        "description": "official.nlp.bert",
        "isExtraImport": true,
        "detail": "official.nlp.bert",
        "documentation": {}
    },
    {
        "label": "squad_evaluate_v1_1",
        "importPath": "official.nlp.bert",
        "description": "official.nlp.bert",
        "isExtraImport": true,
        "detail": "official.nlp.bert",
        "documentation": {}
    },
    {
        "label": "squad_evaluate_v2_0",
        "importPath": "official.nlp.bert",
        "description": "official.nlp.bert",
        "isExtraImport": true,
        "detail": "official.nlp.bert",
        "documentation": {}
    },
    {
        "label": "tokenization",
        "importPath": "official.nlp.bert",
        "description": "official.nlp.bert",
        "isExtraImport": true,
        "detail": "official.nlp.bert",
        "documentation": {}
    },
    {
        "label": "classifier_data_lib",
        "importPath": "official.nlp.data",
        "description": "official.nlp.data",
        "isExtraImport": true,
        "detail": "official.nlp.data",
        "documentation": {}
    },
    {
        "label": "squad_lib",
        "importPath": "official.nlp.data",
        "description": "official.nlp.data",
        "isExtraImport": true,
        "detail": "official.nlp.data",
        "documentation": {}
    },
    {
        "label": "squad_lib",
        "importPath": "official.nlp.data",
        "description": "official.nlp.data",
        "isExtraImport": true,
        "detail": "official.nlp.data",
        "documentation": {}
    },
    {
        "label": "classifier_data_lib",
        "importPath": "official.nlp.data",
        "description": "official.nlp.data",
        "isExtraImport": true,
        "detail": "official.nlp.data",
        "documentation": {}
    },
    {
        "label": "squad_lib",
        "importPath": "official.nlp.data",
        "description": "official.nlp.data",
        "isExtraImport": true,
        "detail": "official.nlp.data",
        "documentation": {}
    },
    {
        "label": "classifier_data_lib",
        "importPath": "official.nlp.data",
        "description": "official.nlp.data",
        "isExtraImport": true,
        "detail": "official.nlp.data",
        "documentation": {}
    },
    {
        "label": "text_embedder",
        "importPath": "tensorflow_lite_support.python.task.text",
        "description": "tensorflow_lite_support.python.task.text",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.python.task.text",
        "documentation": {}
    },
    {
        "label": "text_searcher",
        "importPath": "tensorflow_lite_support.python.task.text",
        "description": "tensorflow_lite_support.python.task.text",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.python.task.text",
        "documentation": {}
    },
    {
        "label": "bert_qa_inputs",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "documentation": {}
    },
    {
        "label": "MetadataPopulatorForBert",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "documentation": {}
    },
    {
        "label": "ModelSpecificInfo",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "documentation": {}
    },
    {
        "label": "bert_qa_inputs",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "documentation": {}
    },
    {
        "label": "MetadataPopulatorForBert",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "documentation": {}
    },
    {
        "label": "ModelSpecificInfo",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "documentation": {}
    },
    {
        "label": "metadata_writer",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers",
        "documentation": {}
    },
    {
        "label": "metadata_writer",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers",
        "documentation": {}
    },
    {
        "label": "tensorflow_hub",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow_hub",
        "description": "tensorflow_hub",
        "detail": "tensorflow_hub",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "tensorflow_hub",
        "description": "tensorflow_hub",
        "isExtraImport": true,
        "detail": "tensorflow_hub",
        "documentation": {}
    },
    {
        "label": "optimization",
        "importPath": "official.nlp",
        "description": "official.nlp",
        "isExtraImport": true,
        "detail": "official.nlp",
        "documentation": {}
    },
    {
        "label": "optimization",
        "importPath": "official.nlp",
        "description": "official.nlp",
        "isExtraImport": true,
        "detail": "official.nlp",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "official.nlp.modeling",
        "description": "official.nlp.modeling",
        "isExtraImport": true,
        "detail": "official.nlp.modeling",
        "documentation": {}
    },
    {
        "label": "make_image_classifier_lib",
        "importPath": "tensorflow_hub.tools.make_image_classifier",
        "description": "tensorflow_hub.tools.make_image_classifier",
        "isExtraImport": true,
        "detail": "tensorflow_hub.tools.make_image_classifier",
        "documentation": {}
    },
    {
        "label": "make_image_classifier_lib",
        "importPath": "tensorflow_hub.tools.make_image_classifier",
        "description": "tensorflow_hub.tools.make_image_classifier",
        "isExtraImport": true,
        "detail": "tensorflow_hub.tools.make_image_classifier",
        "documentation": {}
    },
    {
        "label": "converter",
        "importPath": "tensorflowjs.converters",
        "description": "tensorflowjs.converters",
        "isExtraImport": true,
        "detail": "tensorflowjs.converters",
        "documentation": {}
    },
    {
        "label": "object_detector",
        "importPath": "tflite_support.metadata_writers",
        "description": "tflite_support.metadata_writers",
        "isExtraImport": true,
        "detail": "tflite_support.metadata_writers",
        "documentation": {}
    },
    {
        "label": "writer_utils",
        "importPath": "tflite_support.metadata_writers",
        "description": "tflite_support.metadata_writers",
        "isExtraImport": true,
        "detail": "tflite_support.metadata_writers",
        "documentation": {}
    },
    {
        "label": "metadata_writer_for_bert_question_answerer",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer",
        "documentation": {}
    },
    {
        "label": "metadata_schema_py_generated",
        "importPath": "tensorflow_lite_support.metadata",
        "description": "tensorflow_lite_support.metadata",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.metadata",
        "documentation": {}
    },
    {
        "label": "schema_py_generated",
        "importPath": "tensorflow_lite_support.metadata",
        "description": "tensorflow_lite_support.metadata",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.metadata",
        "documentation": {}
    },
    {
        "label": "metadata",
        "importPath": "tensorflow_lite_support.metadata.python",
        "description": "tensorflow_lite_support.metadata.python",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.metadata.python",
        "documentation": {}
    },
    {
        "label": "metadata",
        "importPath": "tensorflow_lite_support.metadata.python",
        "description": "tensorflow_lite_support.metadata.python",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.metadata.python",
        "documentation": {}
    },
    {
        "label": "ondevice_scann_builder",
        "importPath": "tensorflow_examples.lite.model_maker.core.utils",
        "description": "tensorflow_examples.lite.model_maker.core.utils",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.utils",
        "documentation": {}
    },
    {
        "label": "scann_converter",
        "importPath": "tensorflow_examples.lite.model_maker.core.utils",
        "description": "tensorflow_examples.lite.model_maker.core.utils",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.utils",
        "documentation": {}
    },
    {
        "label": "ondevice_scann_builder",
        "importPath": "tensorflow_examples.lite.model_maker.core.utils",
        "description": "tensorflow_examples.lite.model_maker.core.utils",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.utils",
        "documentation": {}
    },
    {
        "label": "scann_converter",
        "importPath": "tensorflow_examples.lite.model_maker.core.utils",
        "description": "tensorflow_examples.lite.model_maker.core.utils",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.utils",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "metadata_writer_for_bert_text_classifier",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier",
        "documentation": {}
    },
    {
        "label": "metadata_writer_for_text_classifier",
        "importPath": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier",
        "description": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier",
        "documentation": {}
    },
    {
        "label": "warmup",
        "importPath": "tensorflow_examples.lite.model_maker.core.optimization",
        "description": "tensorflow_examples.lite.model_maker.core.optimization",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.core.optimization",
        "documentation": {}
    },
    {
        "label": "scann_pb2",
        "importPath": "scann.proto",
        "description": "scann.proto",
        "isExtraImport": true,
        "detail": "scann.proto",
        "documentation": {}
    },
    {
        "label": "scann_pb2",
        "importPath": "scann.proto",
        "description": "scann.proto",
        "isExtraImport": true,
        "detail": "scann.proto",
        "documentation": {}
    },
    {
        "label": "centers_pb2",
        "importPath": "scann.proto",
        "description": "scann.proto",
        "isExtraImport": true,
        "detail": "scann.proto",
        "documentation": {}
    },
    {
        "label": "hash_pb2",
        "importPath": "scann.proto",
        "description": "scann.proto",
        "isExtraImport": true,
        "detail": "scann.proto",
        "documentation": {}
    },
    {
        "label": "scann_pb2",
        "importPath": "scann.proto",
        "description": "scann.proto",
        "isExtraImport": true,
        "detail": "scann.proto",
        "documentation": {}
    },
    {
        "label": "centers_pb2",
        "importPath": "scann.proto",
        "description": "scann.proto",
        "isExtraImport": true,
        "detail": "scann.proto",
        "documentation": {}
    },
    {
        "label": "scann_builder",
        "importPath": "scann.scann_ops.py",
        "description": "scann.scann_ops.py",
        "isExtraImport": true,
        "detail": "scann.scann_ops.py",
        "documentation": {}
    },
    {
        "label": "scann_ops_pybind",
        "importPath": "scann.scann_ops.py",
        "description": "scann.scann_ops.py",
        "isExtraImport": true,
        "detail": "scann.scann_ops.py",
        "documentation": {}
    },
    {
        "label": "scann_ops_pybind",
        "importPath": "scann.scann_ops.py",
        "description": "scann.scann_ops.py",
        "isExtraImport": true,
        "detail": "scann.scann_ops.py",
        "documentation": {}
    },
    {
        "label": "serialized_searcher_pb2",
        "importPath": "tensorflow_lite_support.scann_ondevice.cc.core",
        "description": "tensorflow_lite_support.scann_ondevice.cc.core",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.scann_ondevice.cc.core",
        "documentation": {}
    },
    {
        "label": "serialized_searcher_pb2",
        "importPath": "tensorflow_lite_support.scann_ondevice.cc.core",
        "description": "tensorflow_lite_support.scann_ondevice.cc.core",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.scann_ondevice.cc.core",
        "documentation": {}
    },
    {
        "label": "features_pb2",
        "importPath": "scann.data_format",
        "description": "scann.data_format",
        "isExtraImport": true,
        "detail": "scann.data_format",
        "documentation": {}
    },
    {
        "label": "partitioner_pb2",
        "importPath": "scann.partitioning",
        "description": "scann.partitioning",
        "isExtraImport": true,
        "detail": "scann.partitioning",
        "documentation": {}
    },
    {
        "label": "partitioner_pb2",
        "importPath": "scann.partitioning",
        "description": "scann.partitioning",
        "isExtraImport": true,
        "detail": "scann.partitioning",
        "documentation": {}
    },
    {
        "label": "index_builder",
        "importPath": "tensorflow_lite_support.scann_ondevice.cc.python",
        "description": "tensorflow_lite_support.scann_ondevice.cc.python",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.scann_ondevice.cc.python",
        "documentation": {}
    },
    {
        "label": "leveldb_testing_utils",
        "importPath": "tensorflow_lite_support.scann_ondevice.cc.test.python",
        "description": "tensorflow_lite_support.scann_ondevice.cc.test.python",
        "isExtraImport": true,
        "detail": "tensorflow_lite_support.scann_ondevice.cc.test.python",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tflite_model_maker.config",
        "description": "tflite_model_maker.config",
        "isExtraImport": true,
        "detail": "tflite_model_maker.config",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "importPath": "tflite_model_maker.config",
        "description": "tflite_model_maker.config",
        "isExtraImport": true,
        "detail": "tflite_model_maker.config",
        "documentation": {}
    },
    {
        "label": "zipfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "zipfile",
        "description": "zipfile",
        "detail": "zipfile",
        "documentation": {}
    },
    {
        "label": "importlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib",
        "description": "importlib",
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "setup_util",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "setup_util",
        "description": "setup_util",
        "detail": "setup_util",
        "documentation": {}
    },
    {
        "label": "image",
        "importPath": "tensorflow_addons",
        "description": "tensorflow_addons",
        "isExtraImport": true,
        "detail": "tensorflow_addons",
        "documentation": {}
    },
    {
        "label": "image",
        "importPath": "tensorflow_addons",
        "description": "tensorflow_addons",
        "isExtraImport": true,
        "detail": "tensorflow_addons",
        "documentation": {}
    },
    {
        "label": "image",
        "importPath": "tensorflow.contrib",
        "description": "tensorflow.contrib",
        "isExtraImport": true,
        "detail": "tensorflow.contrib",
        "documentation": {}
    },
    {
        "label": "training",
        "importPath": "tensorflow.contrib",
        "description": "tensorflow.contrib",
        "isExtraImport": true,
        "detail": "tensorflow.contrib",
        "documentation": {}
    },
    {
        "label": "efficientnet_builder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite_builder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "efficientnet_model",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "efficientnet_model",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "efficientnet_builder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "efficientnet_model",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "efficientnet_builder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "efficientnet_model",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "preprocessing",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "backbone_factory",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "efficientnet_builder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "backbone_factory",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "efficientnet_builder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "six",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "six",
        "description": "six",
        "detail": "six",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "mask",
        "importPath": "pycocotools",
        "description": "pycocotools",
        "isExtraImport": true,
        "detail": "pycocotools",
        "documentation": {}
    },
    {
        "label": "vis_utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "documentation": {}
    },
    {
        "label": "vis_utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "documentation": {}
    },
    {
        "label": "static_shape",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "documentation": {}
    },
    {
        "label": "shape_utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "documentation": {}
    },
    {
        "label": "standard_fields",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "documentation": {}
    },
    {
        "label": "vis_utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize",
        "documentation": {}
    },
    {
        "label": "argmax_matcher",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "box_list",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "faster_rcnn_box_coder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "region_similarity_calculator",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "target_assigner",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "matcher",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "shape_utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "box_coder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "box_list",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "box_list",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "box_list",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "shape_utils",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "preprocessor",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "tf_example_decoder",
        "importPath": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "description": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "isExtraImport": true,
        "detail": "tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "tensorflow_model_optimization",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow_model_optimization",
        "description": "tensorflow_model_optimization",
        "detail": "tensorflow_model_optimization",
        "documentation": {}
    },
    {
        "label": "quantize_wrapper",
        "importPath": "tensorflow_model_optimization.python.core.quantization.keras",
        "description": "tensorflow_model_optimization.python.core.quantization.keras",
        "isExtraImport": true,
        "detail": "tensorflow_model_optimization.python.core.quantization.keras",
        "documentation": {}
    },
    {
        "label": "default_8bit_quantize_configs",
        "importPath": "tensorflow_model_optimization.python.core.quantization.keras.default_8bit",
        "description": "tensorflow_model_optimization.python.core.quantization.keras.default_8bit",
        "isExtraImport": true,
        "detail": "tensorflow_model_optimization.python.core.quantization.keras.default_8bit",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "neural_structured_learning",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "neural_structured_learning",
        "description": "neural_structured_learning",
        "detail": "neural_structured_learning",
        "documentation": {}
    },
    {
        "label": "AverageModelCheckpoint",
        "importPath": "tensorflow_addons.callbacks",
        "description": "tensorflow_addons.callbacks",
        "isExtraImport": true,
        "detail": "tensorflow_addons.callbacks",
        "documentation": {}
    },
    {
        "label": "pruning_wrapper",
        "importPath": "tensorflow_model_optimization.python.core.sparsity.keras",
        "description": "tensorflow_model_optimization.python.core.sparsity.keras",
        "isExtraImport": true,
        "detail": "tensorflow_model_optimization.python.core.sparsity.keras",
        "documentation": {}
    },
    {
        "label": "PIL.ImageColor",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL.ImageColor",
        "description": "PIL.ImageColor",
        "detail": "PIL.ImageColor",
        "documentation": {}
    },
    {
        "label": "PIL.ImageDraw",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL.ImageDraw",
        "description": "PIL.ImageDraw",
        "detail": "PIL.ImageDraw",
        "documentation": {}
    },
    {
        "label": "PIL.ImageFont",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL.ImageFont",
        "description": "PIL.ImageFont",
        "detail": "PIL.ImageFont",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "timeline",
        "importPath": "tensorflow.python.client",
        "description": "tensorflow.python.client",
        "isExtraImport": true,
        "detail": "tensorflow.python.client",
        "documentation": {}
    },
    {
        "label": "timeline",
        "importPath": "tensorflow.python.client",
        "description": "tensorflow.python.client",
        "isExtraImport": true,
        "detail": "tensorflow.python.client",
        "documentation": {}
    },
    {
        "label": "trt_convert",
        "importPath": "tensorflow.python.compiler.tensorrt",
        "description": "tensorflow.python.compiler.tensorrt",
        "isExtraImport": true,
        "detail": "tensorflow.python.compiler.tensorrt",
        "documentation": {}
    },
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "gin",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gin",
        "description": "gin",
        "detail": "gin",
        "documentation": {}
    },
    {
        "label": "tape",
        "importPath": "tensorflow.python.eager",
        "description": "tensorflow.python.eager",
        "isExtraImport": true,
        "detail": "tensorflow.python.eager",
        "documentation": {}
    },
    {
        "label": "tpu_function",
        "importPath": "tensorflow.python.tpu",
        "description": "tensorflow.python.tpu",
        "isExtraImport": true,
        "detail": "tensorflow.python.tpu",
        "documentation": {}
    },
    {
        "label": "dcgan",
        "importPath": "tensorflow_examples.models.dcgan",
        "description": "tensorflow_examples.models.dcgan",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.dcgan",
        "documentation": {}
    },
    {
        "label": "distributed_train",
        "importPath": "tensorflow_examples.models.densenet",
        "description": "tensorflow_examples.models.densenet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.densenet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.models.densenet",
        "description": "tensorflow_examples.models.densenet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.densenet",
        "documentation": {}
    },
    {
        "label": "densenet",
        "importPath": "tensorflow_examples.models.densenet",
        "description": "tensorflow_examples.models.densenet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.densenet",
        "documentation": {}
    },
    {
        "label": "train",
        "importPath": "tensorflow_examples.models.densenet",
        "description": "tensorflow_examples.models.densenet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.densenet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.models.densenet",
        "description": "tensorflow_examples.models.densenet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.densenet",
        "documentation": {}
    },
    {
        "label": "densenet",
        "importPath": "tensorflow_examples.models.densenet",
        "description": "tensorflow_examples.models.densenet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.densenet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.models.densenet",
        "description": "tensorflow_examples.models.densenet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.densenet",
        "documentation": {}
    },
    {
        "label": "densenet",
        "importPath": "tensorflow_examples.models.densenet",
        "description": "tensorflow_examples.models.densenet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.densenet",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.models.densenet",
        "description": "tensorflow_examples.models.densenet",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.densenet",
        "documentation": {}
    },
    {
        "label": "distributed_train",
        "importPath": "tensorflow_examples.models.nmt_with_attention",
        "description": "tensorflow_examples.models.nmt_with_attention",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.nmt_with_attention",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.models.nmt_with_attention",
        "description": "tensorflow_examples.models.nmt_with_attention",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.nmt_with_attention",
        "documentation": {}
    },
    {
        "label": "nmt",
        "importPath": "tensorflow_examples.models.nmt_with_attention",
        "description": "tensorflow_examples.models.nmt_with_attention",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.nmt_with_attention",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.models.nmt_with_attention",
        "description": "tensorflow_examples.models.nmt_with_attention",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.nmt_with_attention",
        "documentation": {}
    },
    {
        "label": "train",
        "importPath": "tensorflow_examples.models.nmt_with_attention",
        "description": "tensorflow_examples.models.nmt_with_attention",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.nmt_with_attention",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.models.nmt_with_attention",
        "description": "tensorflow_examples.models.nmt_with_attention",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.nmt_with_attention",
        "documentation": {}
    },
    {
        "label": "nmt",
        "importPath": "tensorflow_examples.models.nmt_with_attention",
        "description": "tensorflow_examples.models.nmt_with_attention",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.nmt_with_attention",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "tensorflow_examples.models.nmt_with_attention",
        "description": "tensorflow_examples.models.nmt_with_attention",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.nmt_with_attention",
        "documentation": {}
    },
    {
        "label": "Train",
        "importPath": "tensorflow_examples.models.nmt_with_attention.train",
        "description": "tensorflow_examples.models.nmt_with_attention.train",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.nmt_with_attention.train",
        "documentation": {}
    },
    {
        "label": "unicodedata",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unicodedata",
        "description": "unicodedata",
        "detail": "unicodedata",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "data_download",
        "importPath": "tensorflow_examples.models.pix2pix",
        "description": "tensorflow_examples.models.pix2pix",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.pix2pix",
        "documentation": {}
    },
    {
        "label": "pix2pix",
        "importPath": "tensorflow_examples.models.pix2pix",
        "description": "tensorflow_examples.models.pix2pix",
        "isExtraImport": true,
        "detail": "tensorflow_examples.models.pix2pix",
        "documentation": {}
    },
    {
        "label": "backend",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "initializers",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "layers",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "regularizers",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "imagenet_preprocessing_ineffecient_input_pipeline",
        "importPath": "tensorflow_examples.profiling",
        "description": "tensorflow_examples.profiling",
        "isExtraImport": true,
        "detail": "tensorflow_examples.profiling",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "mediapipe",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mediapipe",
        "description": "mediapipe",
        "detail": "mediapipe",
        "documentation": {}
    },
    {
        "label": "filename",
        "importPath": "fileinput",
        "description": "fileinput",
        "isExtraImport": true,
        "detail": "fileinput",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.lite.examples.audio_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.audio_classification.raspberry_pi.classify",
        "peekOfCode": "def run(model: str, max_results: int, score_threshold: float,\n        overlapping_factor: float, num_threads: int,\n        enable_edgetpu: bool) -> None:\n  \"\"\"Continuously run inference on audio data acquired from the device.\n  Args:\n    model: Name of the TFLite audio classification model.\n    max_results: Maximum number of classification results to display.\n    score_threshold: The score threshold of classification results.\n    overlapping_factor: Target overlapping between adjacent inferences.\n    num_threads: Number of CPU threads to run the model.",
        "detail": "examples.lite.examples.audio_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.audio_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.audio_classification.raspberry_pi.classify",
        "peekOfCode": "def main():\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      '--model',\n      help='Name of the audio classification model.',\n      required=False,\n      default='yamnet.tflite')\n  parser.add_argument(\n      '--maxResults',",
        "detail": "examples.lite.examples.audio_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "Plotter",
        "kind": 6,
        "importPath": "examples.lite.examples.audio_classification.raspberry_pi.utils",
        "description": "examples.lite.examples.audio_classification.raspberry_pi.utils",
        "peekOfCode": "class Plotter(object):\n  \"\"\"An util class to display the classification results.\"\"\"\n  _PAUSE_TIME = 0.05\n  \"\"\"Time for matplotlib to wait for UI event.\"\"\"\n  def __init__(self) -> None:\n    fig, self._axes = plt.subplots()\n    fig.canvas.manager.set_window_title('Audio classification')\n    # Stop the program when the ESC key is pressed.\n    def event_callback(event):\n      if event.key == 'escape':",
        "detail": "examples.lite.examples.audio_classification.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "ModelSpecificInfo",
        "kind": 6,
        "importPath": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "description": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "peekOfCode": "class ModelSpecificInfo(object):\n  \"\"\"Holds information that is specificly tied to an image classifier.\"\"\"\n  def __init__(self, name, version, image_width, image_height, image_min,\n               image_max, mean, std, num_classes, author):\n    self.name = name\n    self.version = version\n    self.image_width = image_width\n    self.image_height = image_height\n    self.image_min = image_min\n    self.image_max = image_max",
        "detail": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "documentation": {}
    },
    {
        "label": "MetadataPopulatorForImageClassifier",
        "kind": 6,
        "importPath": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "description": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "peekOfCode": "class MetadataPopulatorForImageClassifier(object):\n  \"\"\"Populates the metadata for an image classifier.\"\"\"\n  def __init__(self, model_file, model_info, label_file_path):\n    self.model_file = model_file\n    self.model_info = model_info\n    self.label_file_path = label_file_path\n    self.metadata_buf = None\n  def populate(self):\n    \"\"\"Creates metadata and then populates it for an image classifier.\"\"\"\n    self._create_metadata()",
        "detail": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "description": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "peekOfCode": "def define_flags():\n  flags.DEFINE_string(\"model_file\", None,\n                      \"Path and file name to the TFLite model file.\")\n  flags.DEFINE_string(\"label_file\", None, \"Path to the label file.\")\n  flags.DEFINE_string(\"export_directory\", None,\n                      \"Path to save the TFLite model files with metadata.\")\n  flags.mark_flag_as_required(\"model_file\")\n  flags.mark_flag_as_required(\"label_file\")\n  flags.mark_flag_as_required(\"export_directory\")\nclass ModelSpecificInfo(object):",
        "detail": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "description": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "peekOfCode": "def main(_):\n  model_file = FLAGS.model_file\n  model_basename = os.path.basename(model_file)\n  if model_basename not in _MODEL_INFO:\n    raise ValueError(\n        \"The model info for, {0}, is not defined yet.\".format(model_basename))\n  export_model_path = os.path.join(FLAGS.export_directory, model_basename)\n  # Copies model_file to export_path.\n  tf.io.gfile.copy(model_file, export_model_path, overwrite=False)\n  # Generate the metadata objects and put them in the model file",
        "detail": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "description": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef define_flags():\n  flags.DEFINE_string(\"model_file\", None,\n                      \"Path and file name to the TFLite model file.\")\n  flags.DEFINE_string(\"label_file\", None, \"Path to the label file.\")\n  flags.DEFINE_string(\"export_directory\", None,\n                      \"Path to save the TFLite model files with metadata.\")\n  flags.mark_flag_as_required(\"model_file\")\n  flags.mark_flag_as_required(\"label_file\")\n  flags.mark_flag_as_required(\"export_directory\")",
        "detail": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "documentation": {}
    },
    {
        "label": "_MODEL_INFO",
        "kind": 5,
        "importPath": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "description": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "peekOfCode": "_MODEL_INFO = {\n    \"mobilenet_v1_0.75_160_quantized.tflite\":\n        ModelSpecificInfo(\n            name=\"MobileNetV1 image classifier\",\n            version=\"v1\",\n            image_width=160,\n            image_height=160,\n            image_min=0,\n            image_max=255,\n            mean=[127.5],",
        "detail": "examples.lite.examples.image_classification.metadata.metadata_writer_for_image_classifier",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "peekOfCode": "def run(model: str, max_results: int, score_threshold: float, num_threads: int,\n        enable_edgetpu: bool, camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n      model: Name of the TFLite image classification model.\n      max_results: Max of classification results.\n      score_threshold: The score threshold of classification results.\n      num_threads: Number of CPU threads to run the model.\n      enable_edgetpu: Whether to run the model on EdgeTPU.\n      camera_id: The camera id to be passed to OpenCV.",
        "detail": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "peekOfCode": "def main():\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      '--model',\n      help='Name of image classification model.',\n      required=False,\n      default='efficientnet_lite0.tflite')\n  parser.add_argument(\n      '--maxResults',",
        "detail": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_ROW_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "peekOfCode": "_ROW_SIZE = 20  # pixels\n_LEFT_MARGIN = 24  # pixels\n_TEXT_COLOR = (0, 0, 255)  # red\n_FONT_SIZE = 1\n_FONT_THICKNESS = 1\n_FPS_AVERAGE_FRAME_COUNT = 10\ndef run(model: str, max_results: int, score_threshold: float, num_threads: int,\n        enable_edgetpu: bool, camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:",
        "detail": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_LEFT_MARGIN",
        "kind": 5,
        "importPath": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "peekOfCode": "_LEFT_MARGIN = 24  # pixels\n_TEXT_COLOR = (0, 0, 255)  # red\n_FONT_SIZE = 1\n_FONT_THICKNESS = 1\n_FPS_AVERAGE_FRAME_COUNT = 10\ndef run(model: str, max_results: int, score_threshold: float, num_threads: int,\n        enable_edgetpu: bool, camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n      model: Name of the TFLite image classification model.",
        "detail": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_TEXT_COLOR",
        "kind": 5,
        "importPath": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "peekOfCode": "_TEXT_COLOR = (0, 0, 255)  # red\n_FONT_SIZE = 1\n_FONT_THICKNESS = 1\n_FPS_AVERAGE_FRAME_COUNT = 10\ndef run(model: str, max_results: int, score_threshold: float, num_threads: int,\n        enable_edgetpu: bool, camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n      model: Name of the TFLite image classification model.\n      max_results: Max of classification results.",
        "detail": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_FONT_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "peekOfCode": "_FONT_SIZE = 1\n_FONT_THICKNESS = 1\n_FPS_AVERAGE_FRAME_COUNT = 10\ndef run(model: str, max_results: int, score_threshold: float, num_threads: int,\n        enable_edgetpu: bool, camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n      model: Name of the TFLite image classification model.\n      max_results: Max of classification results.\n      score_threshold: The score threshold of classification results.",
        "detail": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_FONT_THICKNESS",
        "kind": 5,
        "importPath": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "peekOfCode": "_FONT_THICKNESS = 1\n_FPS_AVERAGE_FRAME_COUNT = 10\ndef run(model: str, max_results: int, score_threshold: float, num_threads: int,\n        enable_edgetpu: bool, camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n      model: Name of the TFLite image classification model.\n      max_results: Max of classification results.\n      score_threshold: The score threshold of classification results.\n      num_threads: Number of CPU threads to run the model.",
        "detail": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_FPS_AVERAGE_FRAME_COUNT",
        "kind": 5,
        "importPath": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "peekOfCode": "_FPS_AVERAGE_FRAME_COUNT = 10\ndef run(model: str, max_results: int, score_threshold: float, num_threads: int,\n        enable_edgetpu: bool, camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n      model: Name of the TFLite image classification model.\n      max_results: Max of classification results.\n      score_threshold: The score threshold of classification results.\n      num_threads: Number of CPU threads to run the model.\n      enable_edgetpu: Whether to run the model on EdgeTPU.",
        "detail": "examples.lite.examples.image_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "def run(model: str, display_mode: str, num_threads: int, enable_edgetpu: bool,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n    model: Name of the TFLite image segmentation model.\n    display_mode: Name of mode to display image segmentation.\n    num_threads: Number of CPU threads to run the model.\n    enable_edgetpu: Whether to run the model on EdgeTPU.\n    camera_id: The camera id to be passed to OpenCV.\n    width: The width of the frame captured from the camera.",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "visualize",
        "kind": 2,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "def visualize(input_image: np.ndarray, segmentation_map_image: np.ndarray,\n              display_mode: str, fps: float,\n              colored_labels: List[processor.ColoredLabel]) -> np.ndarray:\n  \"\"\"Visualize segmentation result on image.\n  Args:\n    input_image: The [height, width, 3] RGB input image.\n    segmentation_map_image: The [height, width, 3] RGB segmentation map image.\n    display_mode: How the segmentation map should be shown. 'overlay' or\n      'side-by-side'.\n    fps: Value of fps.",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "def main():\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      '--model',\n      help='Name of image segmentation model.',\n      required=False,\n      default='deeplabv3.tflite')\n  parser.add_argument(\n      '--displayMode',",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "_FPS_AVERAGE_FRAME_COUNT",
        "kind": 5,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "_FPS_AVERAGE_FRAME_COUNT = 10\n_FPS_LEFT_MARGIN = 24  # pixels\n_LEGEND_TEXT_COLOR = (0, 0, 255)  # red\n_LEGEND_BACKGROUND_COLOR = (255, 255, 255)  # white\n_LEGEND_FONT_SIZE = 1\n_LEGEND_FONT_THICKNESS = 1\n_LEGEND_ROW_SIZE = 20  # pixels\n_LEGEND_RECT_SIZE = 16  # pixels\n_LABEL_MARGIN = 10\n_OVERLAY_ALPHA = 0.5",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "_FPS_LEFT_MARGIN",
        "kind": 5,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "_FPS_LEFT_MARGIN = 24  # pixels\n_LEGEND_TEXT_COLOR = (0, 0, 255)  # red\n_LEGEND_BACKGROUND_COLOR = (255, 255, 255)  # white\n_LEGEND_FONT_SIZE = 1\n_LEGEND_FONT_THICKNESS = 1\n_LEGEND_ROW_SIZE = 20  # pixels\n_LEGEND_RECT_SIZE = 16  # pixels\n_LABEL_MARGIN = 10\n_OVERLAY_ALPHA = 0.5\n_PADDING_WIDTH_FOR_LEGEND = 150  # pixels",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "_LEGEND_TEXT_COLOR",
        "kind": 5,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "_LEGEND_TEXT_COLOR = (0, 0, 255)  # red\n_LEGEND_BACKGROUND_COLOR = (255, 255, 255)  # white\n_LEGEND_FONT_SIZE = 1\n_LEGEND_FONT_THICKNESS = 1\n_LEGEND_ROW_SIZE = 20  # pixels\n_LEGEND_RECT_SIZE = 16  # pixels\n_LABEL_MARGIN = 10\n_OVERLAY_ALPHA = 0.5\n_PADDING_WIDTH_FOR_LEGEND = 150  # pixels\ndef run(model: str, display_mode: str, num_threads: int, enable_edgetpu: bool,",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "_LEGEND_BACKGROUND_COLOR",
        "kind": 5,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "_LEGEND_BACKGROUND_COLOR = (255, 255, 255)  # white\n_LEGEND_FONT_SIZE = 1\n_LEGEND_FONT_THICKNESS = 1\n_LEGEND_ROW_SIZE = 20  # pixels\n_LEGEND_RECT_SIZE = 16  # pixels\n_LABEL_MARGIN = 10\n_OVERLAY_ALPHA = 0.5\n_PADDING_WIDTH_FOR_LEGEND = 150  # pixels\ndef run(model: str, display_mode: str, num_threads: int, enable_edgetpu: bool,\n        camera_id: int, width: int, height: int) -> None:",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "_LEGEND_FONT_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "_LEGEND_FONT_SIZE = 1\n_LEGEND_FONT_THICKNESS = 1\n_LEGEND_ROW_SIZE = 20  # pixels\n_LEGEND_RECT_SIZE = 16  # pixels\n_LABEL_MARGIN = 10\n_OVERLAY_ALPHA = 0.5\n_PADDING_WIDTH_FOR_LEGEND = 150  # pixels\ndef run(model: str, display_mode: str, num_threads: int, enable_edgetpu: bool,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "_LEGEND_FONT_THICKNESS",
        "kind": 5,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "_LEGEND_FONT_THICKNESS = 1\n_LEGEND_ROW_SIZE = 20  # pixels\n_LEGEND_RECT_SIZE = 16  # pixels\n_LABEL_MARGIN = 10\n_OVERLAY_ALPHA = 0.5\n_PADDING_WIDTH_FOR_LEGEND = 150  # pixels\ndef run(model: str, display_mode: str, num_threads: int, enable_edgetpu: bool,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "_LEGEND_ROW_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "_LEGEND_ROW_SIZE = 20  # pixels\n_LEGEND_RECT_SIZE = 16  # pixels\n_LABEL_MARGIN = 10\n_OVERLAY_ALPHA = 0.5\n_PADDING_WIDTH_FOR_LEGEND = 150  # pixels\ndef run(model: str, display_mode: str, num_threads: int, enable_edgetpu: bool,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n    model: Name of the TFLite image segmentation model.",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "_LEGEND_RECT_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "_LEGEND_RECT_SIZE = 16  # pixels\n_LABEL_MARGIN = 10\n_OVERLAY_ALPHA = 0.5\n_PADDING_WIDTH_FOR_LEGEND = 150  # pixels\ndef run(model: str, display_mode: str, num_threads: int, enable_edgetpu: bool,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n    model: Name of the TFLite image segmentation model.\n    display_mode: Name of mode to display image segmentation.",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "_LABEL_MARGIN",
        "kind": 5,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "_LABEL_MARGIN = 10\n_OVERLAY_ALPHA = 0.5\n_PADDING_WIDTH_FOR_LEGEND = 150  # pixels\ndef run(model: str, display_mode: str, num_threads: int, enable_edgetpu: bool,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n    model: Name of the TFLite image segmentation model.\n    display_mode: Name of mode to display image segmentation.\n    num_threads: Number of CPU threads to run the model.",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "_OVERLAY_ALPHA",
        "kind": 5,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "_OVERLAY_ALPHA = 0.5\n_PADDING_WIDTH_FOR_LEGEND = 150  # pixels\ndef run(model: str, display_mode: str, num_threads: int, enable_edgetpu: bool,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n    model: Name of the TFLite image segmentation model.\n    display_mode: Name of mode to display image segmentation.\n    num_threads: Number of CPU threads to run the model.\n    enable_edgetpu: Whether to run the model on EdgeTPU.",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "_PADDING_WIDTH_FOR_LEGEND",
        "kind": 5,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "peekOfCode": "_PADDING_WIDTH_FOR_LEGEND = 150  # pixels\ndef run(model: str, display_mode: str, num_threads: int, enable_edgetpu: bool,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n    model: Name of the TFLite image segmentation model.\n    display_mode: Name of mode to display image segmentation.\n    num_threads: Number of CPU threads to run the model.\n    enable_edgetpu: Whether to run the model on EdgeTPU.\n    camera_id: The camera id to be passed to OpenCV.",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.segment",
        "documentation": {}
    },
    {
        "label": "segmentation_map_to_image",
        "kind": 2,
        "importPath": "examples.lite.examples.image_segmentation.raspberry_pi.utils",
        "description": "examples.lite.examples.image_segmentation.raspberry_pi.utils",
        "peekOfCode": "def segmentation_map_to_image(\n    segmentation: processor.SegmentationResult\n) -> (np.ndarray, List[processor.ColoredLabel]):\n  \"\"\"Convert the SegmentationResult into a RGB image.\n  Args:\n    segmentation: An output of a image segmentation model.\n  Returns:\n    seg_map_img: The visualized segmentation result as an RGB image.\n    found_colored_labels: The list of ColoredLabels found in the image.\n  \"\"\"",
        "detail": "examples.lite.examples.image_segmentation.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "TransferLearningModel",
        "kind": 6,
        "importPath": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "description": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "peekOfCode": "class TransferLearningModel(tf.Module):\n  \"\"\"TF Transfer Learning model class.\"\"\"\n  def __init__(self, learning_rate=0.001):\n    \"\"\"Initializes a transfer learning model instance.\n    Args:\n      learning_rate: A learning rate for the optimzer.\n    \"\"\"\n    self.num_features = NUM_FEATURES\n    self.num_classes = NUM_CLASSES\n    # trainable weights and bias for softmax",
        "detail": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "documentation": {}
    },
    {
        "label": "convert_and_save",
        "kind": 2,
        "importPath": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "description": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "peekOfCode": "def convert_and_save(saved_model_dir='saved_model'):\n  \"\"\"Converts and saves the TFLite Transfer Learning model.\n  Args:\n    saved_model_dir: A directory path to save a converted model.\n  \"\"\"\n  model = TransferLearningModel()\n  tf.saved_model.save(\n      model,\n      saved_model_dir,\n      signatures={",
        "detail": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "documentation": {}
    },
    {
        "label": "IMG_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "description": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "peekOfCode": "IMG_SIZE = 224\nNUM_FEATURES = 7 * 7 * 1280\nNUM_CLASSES = 4\nclass TransferLearningModel(tf.Module):\n  \"\"\"TF Transfer Learning model class.\"\"\"\n  def __init__(self, learning_rate=0.001):\n    \"\"\"Initializes a transfer learning model instance.\n    Args:\n      learning_rate: A learning rate for the optimzer.\n    \"\"\"",
        "detail": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "documentation": {}
    },
    {
        "label": "NUM_FEATURES",
        "kind": 5,
        "importPath": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "description": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "peekOfCode": "NUM_FEATURES = 7 * 7 * 1280\nNUM_CLASSES = 4\nclass TransferLearningModel(tf.Module):\n  \"\"\"TF Transfer Learning model class.\"\"\"\n  def __init__(self, learning_rate=0.001):\n    \"\"\"Initializes a transfer learning model instance.\n    Args:\n      learning_rate: A learning rate for the optimzer.\n    \"\"\"\n    self.num_features = NUM_FEATURES",
        "detail": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "documentation": {}
    },
    {
        "label": "NUM_CLASSES",
        "kind": 5,
        "importPath": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "description": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "peekOfCode": "NUM_CLASSES = 4\nclass TransferLearningModel(tf.Module):\n  \"\"\"TF Transfer Learning model class.\"\"\"\n  def __init__(self, learning_rate=0.001):\n    \"\"\"Initializes a transfer learning model instance.\n    Args:\n      learning_rate: A learning rate for the optimzer.\n    \"\"\"\n    self.num_features = NUM_FEATURES\n    self.num_classes = NUM_CLASSES",
        "detail": "examples.lite.examples.model_personalization.transfer_learning.generate_training_model",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.lite.examples.object_detection.raspberry_pi.detect",
        "description": "examples.lite.examples.object_detection.raspberry_pi.detect",
        "peekOfCode": "def run(model: str, camera_id: int, width: int, height: int, num_threads: int,\n        enable_edgetpu: bool) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n    model: Name of the TFLite object detection model.\n    camera_id: The camera id to be passed to OpenCV.\n    width: The width of the frame captured from the camera.\n    height: The height of the frame captured from the camera.\n    num_threads: The number of CPU threads to run the model.\n    enable_edgetpu: True/False whether the model is a EdgeTPU model.",
        "detail": "examples.lite.examples.object_detection.raspberry_pi.detect",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.object_detection.raspberry_pi.detect",
        "description": "examples.lite.examples.object_detection.raspberry_pi.detect",
        "peekOfCode": "def main():\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      '--model',\n      help='Path of the object detection model.',\n      required=False,\n      default='efficientdet_lite0.tflite')\n  parser.add_argument(\n      '--cameraId', help='Id of camera.', required=False, type=int, default=0)",
        "detail": "examples.lite.examples.object_detection.raspberry_pi.detect",
        "documentation": {}
    },
    {
        "label": "visualize",
        "kind": 2,
        "importPath": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "description": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "peekOfCode": "def visualize(\n    image: np.ndarray,\n    detection_result: processor.DetectionResult,\n) -> np.ndarray:\n  \"\"\"Draws bounding boxes on the input image and return it.\n  Args:\n    image: The input RGB image.\n    detection_result: The list of all \"Detection\" entities to be visualize.\n  Returns:\n    Image with bounding boxes.",
        "detail": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "_MARGIN",
        "kind": 5,
        "importPath": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "description": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "peekOfCode": "_MARGIN = 10  # pixels\n_ROW_SIZE = 10  # pixels\n_FONT_SIZE = 1\n_FONT_THICKNESS = 1\n_TEXT_COLOR = (0, 0, 255)  # red\ndef visualize(\n    image: np.ndarray,\n    detection_result: processor.DetectionResult,\n) -> np.ndarray:\n  \"\"\"Draws bounding boxes on the input image and return it.",
        "detail": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "_ROW_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "description": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "peekOfCode": "_ROW_SIZE = 10  # pixels\n_FONT_SIZE = 1\n_FONT_THICKNESS = 1\n_TEXT_COLOR = (0, 0, 255)  # red\ndef visualize(\n    image: np.ndarray,\n    detection_result: processor.DetectionResult,\n) -> np.ndarray:\n  \"\"\"Draws bounding boxes on the input image and return it.\n  Args:",
        "detail": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "_FONT_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "description": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "peekOfCode": "_FONT_SIZE = 1\n_FONT_THICKNESS = 1\n_TEXT_COLOR = (0, 0, 255)  # red\ndef visualize(\n    image: np.ndarray,\n    detection_result: processor.DetectionResult,\n) -> np.ndarray:\n  \"\"\"Draws bounding boxes on the input image and return it.\n  Args:\n    image: The input RGB image.",
        "detail": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "_FONT_THICKNESS",
        "kind": 5,
        "importPath": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "description": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "peekOfCode": "_FONT_THICKNESS = 1\n_TEXT_COLOR = (0, 0, 255)  # red\ndef visualize(\n    image: np.ndarray,\n    detection_result: processor.DetectionResult,\n) -> np.ndarray:\n  \"\"\"Draws bounding boxes on the input image and return it.\n  Args:\n    image: The input RGB image.\n    detection_result: The list of all \"Detection\" entities to be visualize.",
        "detail": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "_TEXT_COLOR",
        "kind": 5,
        "importPath": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "description": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "peekOfCode": "_TEXT_COLOR = (0, 0, 255)  # red\ndef visualize(\n    image: np.ndarray,\n    detection_result: processor.DetectionResult,\n) -> np.ndarray:\n  \"\"\"Draws bounding boxes on the input image and return it.\n  Args:\n    image: The input RGB image.\n    detection_result: The list of all \"Detection\" entities to be visualize.\n  Returns:",
        "detail": "examples.lite.examples.object_detection.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "Classifier",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier",
        "peekOfCode": "class Classifier(object):\n  \"\"\"A wrapper class for a TFLite pose classification model.\"\"\"\n  def __init__(self, model_name: str, label_file: str) -> None:\n    \"\"\"Initialize a pose classification model.\n    Args:\n      model_name: Name of the TFLite pose classification model.\n      label_file: Path of the label list file.\n    \"\"\"\n    # Append TFLITE extension to model_name if there's no extension\n    _, ext = os.path.splitext(model_name)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier",
        "documentation": {}
    },
    {
        "label": "ClassifierTest",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "peekOfCode": "class ClassifierTest(unittest.TestCase):\n  def test_pose_classification(self):\n    \"\"\"Test if yoga pose classifier returns correct result on test image.\"\"\"\n    # Detect the pose from the input image\n    pose_detector = Movenet(_ESTIMATION_MODEL)\n    image = cv2.imread(_TEST_IMAGE)\n    person = pose_detector.detect(image)\n    # Initialize a pose classifier\n    classifier = Classifier(_CLASSIFIER_MODEL, _LABELS)\n    categories = classifier.classify_pose(person)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "documentation": {}
    },
    {
        "label": "_ESTIMATION_MODEL",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "peekOfCode": "_ESTIMATION_MODEL = 'movenet_lightning'\n_CLASSIFIER_MODEL = 'classifier'\n_TEST_IMAGE = 'test_data/image3.jpeg'\n_LABELS = 'labels.txt'\nclass ClassifierTest(unittest.TestCase):\n  def test_pose_classification(self):\n    \"\"\"Test if yoga pose classifier returns correct result on test image.\"\"\"\n    # Detect the pose from the input image\n    pose_detector = Movenet(_ESTIMATION_MODEL)\n    image = cv2.imread(_TEST_IMAGE)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "documentation": {}
    },
    {
        "label": "_CLASSIFIER_MODEL",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "peekOfCode": "_CLASSIFIER_MODEL = 'classifier'\n_TEST_IMAGE = 'test_data/image3.jpeg'\n_LABELS = 'labels.txt'\nclass ClassifierTest(unittest.TestCase):\n  def test_pose_classification(self):\n    \"\"\"Test if yoga pose classifier returns correct result on test image.\"\"\"\n    # Detect the pose from the input image\n    pose_detector = Movenet(_ESTIMATION_MODEL)\n    image = cv2.imread(_TEST_IMAGE)\n    person = pose_detector.detect(image)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "documentation": {}
    },
    {
        "label": "_TEST_IMAGE",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "peekOfCode": "_TEST_IMAGE = 'test_data/image3.jpeg'\n_LABELS = 'labels.txt'\nclass ClassifierTest(unittest.TestCase):\n  def test_pose_classification(self):\n    \"\"\"Test if yoga pose classifier returns correct result on test image.\"\"\"\n    # Detect the pose from the input image\n    pose_detector = Movenet(_ESTIMATION_MODEL)\n    image = cv2.imread(_TEST_IMAGE)\n    person = pose_detector.detect(image)\n    # Initialize a pose classifier",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "documentation": {}
    },
    {
        "label": "_LABELS",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "peekOfCode": "_LABELS = 'labels.txt'\nclass ClassifierTest(unittest.TestCase):\n  def test_pose_classification(self):\n    \"\"\"Test if yoga pose classifier returns correct result on test image.\"\"\"\n    # Detect the pose from the input image\n    pose_detector = Movenet(_ESTIMATION_MODEL)\n    image = cv2.imread(_TEST_IMAGE)\n    person = pose_detector.detect(image)\n    # Initialize a pose classifier\n    classifier = Classifier(_CLASSIFIER_MODEL, _LABELS)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.classifier_test",
        "documentation": {}
    },
    {
        "label": "Movenet",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet",
        "peekOfCode": "class Movenet(object):\n  \"\"\"A wrapper class for a Movenet TFLite pose estimation model.\"\"\"\n  # Configure how confidence the model should be on the detected keypoints to\n  # proceed with using smart cropping logic.\n  _MIN_CROP_KEYPOINT_SCORE = 0.2\n  _TORSO_EXPANSION_RATIO = 1.9\n  _BODY_EXPANSION_RATIO = 1.2\n  def __init__(self, model_name: str) -> None:\n    \"\"\"Initialize a MoveNet pose estimation model.\n    Args:",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet",
        "documentation": {}
    },
    {
        "label": "MoveNetMultiPose",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose",
        "peekOfCode": "class MoveNetMultiPose(object):\n  \"\"\"A wrapper class for a MultiPose TFLite pose estimation model.\"\"\"\n  def __init__(self,\n               model_name: str,\n               tracker_type: str = 'bounding_box',\n               input_size: int = 256) -> None:\n    \"\"\"Initialize a MultiPose pose estimation model.\n    Args:\n      model_name: Name of the TFLite multipose model.\n      tracker_type: Type of Tracker('keypoint' or 'bounding_box')",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose",
        "documentation": {}
    },
    {
        "label": "MovenetMultiPoseTest",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "peekOfCode": "class MovenetMultiPoseTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    image_1 = cv2.imread(_IMAGE_TEST1)\n    image_2 = cv2.imread(_IMAGE_TEST2)\n    # Merge image_1 and image_2 into a single image for testing MultiPose model.\n    image = cv2.hconcat([image_1, image_2])\n    # Initialize the MultiPose model.\n    detector = MoveNetMultiPose(_MODEL_MOVENET_MULTILPOSE)\n    # Run detection on the merged image",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "documentation": {}
    },
    {
        "label": "_MODEL_MOVENET_MULTILPOSE",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "peekOfCode": "_MODEL_MOVENET_MULTILPOSE = 'movenet_multipose'\n_IMAGE_TEST1 = 'test_data/image1.png'\n_IMAGE_TEST2 = 'test_data/image2.jpeg'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 41\nclass MovenetMultiPoseTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    image_1 = cv2.imread(_IMAGE_TEST1)\n    image_2 = cv2.imread(_IMAGE_TEST2)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "documentation": {}
    },
    {
        "label": "_IMAGE_TEST1",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "peekOfCode": "_IMAGE_TEST1 = 'test_data/image1.png'\n_IMAGE_TEST2 = 'test_data/image2.jpeg'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 41\nclass MovenetMultiPoseTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    image_1 = cv2.imread(_IMAGE_TEST1)\n    image_2 = cv2.imread(_IMAGE_TEST2)\n    # Merge image_1 and image_2 into a single image for testing MultiPose model.",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "documentation": {}
    },
    {
        "label": "_IMAGE_TEST2",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "peekOfCode": "_IMAGE_TEST2 = 'test_data/image2.jpeg'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 41\nclass MovenetMultiPoseTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    image_1 = cv2.imread(_IMAGE_TEST1)\n    image_2 = cv2.imread(_IMAGE_TEST2)\n    # Merge image_1 and image_2 into a single image for testing MultiPose model.\n    image = cv2.hconcat([image_1, image_2])",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "documentation": {}
    },
    {
        "label": "_GROUND_TRUTH_CSV",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "peekOfCode": "_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 41\nclass MovenetMultiPoseTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    image_1 = cv2.imread(_IMAGE_TEST1)\n    image_2 = cv2.imread(_IMAGE_TEST2)\n    # Merge image_1 and image_2 into a single image for testing MultiPose model.\n    image = cv2.hconcat([image_1, image_2])\n    # Initialize the MultiPose model.",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "documentation": {}
    },
    {
        "label": "_ALLOWED_DISTANCE",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "peekOfCode": "_ALLOWED_DISTANCE = 41\nclass MovenetMultiPoseTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    image_1 = cv2.imread(_IMAGE_TEST1)\n    image_2 = cv2.imread(_IMAGE_TEST2)\n    # Merge image_1 and image_2 into a single image for testing MultiPose model.\n    image = cv2.hconcat([image_1, image_2])\n    # Initialize the MultiPose model.\n    detector = MoveNetMultiPose(_MODEL_MOVENET_MULTILPOSE)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_multipose_test",
        "documentation": {}
    },
    {
        "label": "MovenetTest",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "peekOfCode": "class MovenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)\n    self.image_2 = cv2.imread(_IMAGE_TEST2)\n    # Initialize model\n    self.movenet_lightning = Movenet(_MODEL_LIGHTNING)\n    self.movenet_thunder = Movenet(_MODEL_THUNDER)\n    # Get pose landmarks truth\n    pose_landmarks_truth = pd.read_csv(_GROUND_TRUTH_CSV)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "documentation": {}
    },
    {
        "label": "_MODEL_LIGHTNING",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "peekOfCode": "_MODEL_LIGHTNING = 'movenet_lightning'\n_MODEL_THUNDER = 'movenet_thunder'\n_IMAGE_TEST1 = 'test_data/image1.png'\n_IMAGE_TEST2 = 'test_data/image2.jpeg'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 21\nclass MovenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "documentation": {}
    },
    {
        "label": "_MODEL_THUNDER",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "peekOfCode": "_MODEL_THUNDER = 'movenet_thunder'\n_IMAGE_TEST1 = 'test_data/image1.png'\n_IMAGE_TEST2 = 'test_data/image2.jpeg'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 21\nclass MovenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)\n    self.image_2 = cv2.imread(_IMAGE_TEST2)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "documentation": {}
    },
    {
        "label": "_IMAGE_TEST1",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "peekOfCode": "_IMAGE_TEST1 = 'test_data/image1.png'\n_IMAGE_TEST2 = 'test_data/image2.jpeg'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 21\nclass MovenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)\n    self.image_2 = cv2.imread(_IMAGE_TEST2)\n    # Initialize model",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "documentation": {}
    },
    {
        "label": "_IMAGE_TEST2",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "peekOfCode": "_IMAGE_TEST2 = 'test_data/image2.jpeg'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 21\nclass MovenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)\n    self.image_2 = cv2.imread(_IMAGE_TEST2)\n    # Initialize model\n    self.movenet_lightning = Movenet(_MODEL_LIGHTNING)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "documentation": {}
    },
    {
        "label": "_GROUND_TRUTH_CSV",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "peekOfCode": "_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 21\nclass MovenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)\n    self.image_2 = cv2.imread(_IMAGE_TEST2)\n    # Initialize model\n    self.movenet_lightning = Movenet(_MODEL_LIGHTNING)\n    self.movenet_thunder = Movenet(_MODEL_THUNDER)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "documentation": {}
    },
    {
        "label": "_ALLOWED_DISTANCE",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "peekOfCode": "_ALLOWED_DISTANCE = 21\nclass MovenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)\n    self.image_2 = cv2.imread(_IMAGE_TEST2)\n    # Initialize model\n    self.movenet_lightning = Movenet(_MODEL_LIGHTNING)\n    self.movenet_thunder = Movenet(_MODEL_THUNDER)\n    # Get pose landmarks truth",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.movenet_test",
        "documentation": {}
    },
    {
        "label": "Posenet",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet",
        "peekOfCode": "class Posenet(object):\n  \"\"\"A wrapper class for a Posenet TFLite pose estimation model.\"\"\"\n  def __init__(self, model_name: str) -> None:\n    \"\"\"Initialize a PoseNet pose estimation model.\n    Args:\n        model_name: Name of the TFLite PoseNet model.\n    \"\"\"\n    # Append TFLITE extension to model_name if there's no extension\n    _, ext = os.path.splitext(model_name)\n    if not ext:",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet",
        "documentation": {}
    },
    {
        "label": "PosenetTest",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "peekOfCode": "class PosenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)\n    self.image_2 = cv2.imread(_IMAGE_TEST2)\n    # Initialize model\n    self.posenet = Posenet(_MODEL)\n    # Get pose landmarks truth\n    pose_landmarks_truth = pd.read_csv(_GROUND_TRUTH_CSV)\n    self.keypoints_truth_1 = pose_landmarks_truth.iloc[0].to_numpy().reshape(",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "documentation": {}
    },
    {
        "label": "_MODEL",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "peekOfCode": "_MODEL = 'posenet'\n_IMAGE_TEST1 = 'test_data/image1.png'\n_IMAGE_TEST2 = 'test_data/image2.jpeg'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 35\nclass PosenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)\n    self.image_2 = cv2.imread(_IMAGE_TEST2)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "documentation": {}
    },
    {
        "label": "_IMAGE_TEST1",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "peekOfCode": "_IMAGE_TEST1 = 'test_data/image1.png'\n_IMAGE_TEST2 = 'test_data/image2.jpeg'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 35\nclass PosenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)\n    self.image_2 = cv2.imread(_IMAGE_TEST2)\n    # Initialize model",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "documentation": {}
    },
    {
        "label": "_IMAGE_TEST2",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "peekOfCode": "_IMAGE_TEST2 = 'test_data/image2.jpeg'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 35\nclass PosenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)\n    self.image_2 = cv2.imread(_IMAGE_TEST2)\n    # Initialize model\n    self.posenet = Posenet(_MODEL)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "documentation": {}
    },
    {
        "label": "_GROUND_TRUTH_CSV",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "peekOfCode": "_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_ALLOWED_DISTANCE = 35\nclass PosenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)\n    self.image_2 = cv2.imread(_IMAGE_TEST2)\n    # Initialize model\n    self.posenet = Posenet(_MODEL)\n    # Get pose landmarks truth",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "documentation": {}
    },
    {
        "label": "_ALLOWED_DISTANCE",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "peekOfCode": "_ALLOWED_DISTANCE = 35\nclass PosenetTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.image_1 = cv2.imread(_IMAGE_TEST1)\n    self.image_2 = cv2.imread(_IMAGE_TEST2)\n    # Initialize model\n    self.posenet = Posenet(_MODEL)\n    # Get pose landmarks truth\n    pose_landmarks_truth = pd.read_csv(_GROUND_TRUTH_CSV)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.ml.posenet_test",
        "documentation": {}
    },
    {
        "label": "BoundingBoxTracker",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker",
        "peekOfCode": "class BoundingBoxTracker(Tracker):\n  \"\"\"Tracks objects based on bounding box similarity.\n  Similarity is currently defined as intersection-over-union (IoU).\n  \"\"\"\n  def _compute_similarity(self, persons: List[Person]) -> List[List[float]]:\n    \"\"\"Computes similarity based on intersection-over-union (IoU).\n    Args:\n      persons: An array of detected `Person`s.\n    Returns:\n      A 2D array of shape [num_det, num_tracks] with pairwise similarity scores",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker",
        "documentation": {}
    },
    {
        "label": "BoundingBoxTrackerTest",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker_test",
        "peekOfCode": "class BoundingBoxTrackerTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.tracker_config = TrackerConfig(KeypointTrackerConfig(), _MAX_TRACKS,\n                                        _MAX_AGE, _MIN_SIMILARITY)\n    self.bbox_tracker = BoundingBoxTracker(self.tracker_config)\n  def test_iou(self):\n    \"\"\"Test IoU.\"\"\"\n    person = Person([], Rectangle(Point(0, 0), Point(1, 2 / 3)), 1)\n    track = Track(",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker_test",
        "documentation": {}
    },
    {
        "label": "_MAX_TRACKS",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker_test",
        "peekOfCode": "_MAX_TRACKS = 4\n_MAX_AGE = 1000 * 1000\n_MIN_SIMILARITY = 0.5\nclass BoundingBoxTrackerTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.tracker_config = TrackerConfig(KeypointTrackerConfig(), _MAX_TRACKS,\n                                        _MAX_AGE, _MIN_SIMILARITY)\n    self.bbox_tracker = BoundingBoxTracker(self.tracker_config)\n  def test_iou(self):",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker_test",
        "documentation": {}
    },
    {
        "label": "_MAX_AGE",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker_test",
        "peekOfCode": "_MAX_AGE = 1000 * 1000\n_MIN_SIMILARITY = 0.5\nclass BoundingBoxTrackerTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.tracker_config = TrackerConfig(KeypointTrackerConfig(), _MAX_TRACKS,\n                                        _MAX_AGE, _MIN_SIMILARITY)\n    self.bbox_tracker = BoundingBoxTracker(self.tracker_config)\n  def test_iou(self):\n    \"\"\"Test IoU.\"\"\"",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker_test",
        "documentation": {}
    },
    {
        "label": "_MIN_SIMILARITY",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker_test",
        "peekOfCode": "_MIN_SIMILARITY = 0.5\nclass BoundingBoxTrackerTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.tracker_config = TrackerConfig(KeypointTrackerConfig(), _MAX_TRACKS,\n                                        _MAX_AGE, _MIN_SIMILARITY)\n    self.bbox_tracker = BoundingBoxTracker(self.tracker_config)\n  def test_iou(self):\n    \"\"\"Test IoU.\"\"\"\n    person = Person([], Rectangle(Point(0, 0), Point(1, 2 / 3)), 1)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.bounding_box_tracker_test",
        "documentation": {}
    },
    {
        "label": "KeypointTrackerConfig",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.config",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.config",
        "peekOfCode": "class KeypointTrackerConfig(NamedTuple):\n  \"\"\"Keypoint tracker specific configuration.\"\"\"\n  keypoint_confidence_threshold: float = 0.3\n  \"\"\"The minimum keypoint confidence threshold.\n    A keypoint is only compared in the OKS calculation if both the new detected\n    keypoint and the\n    corresponding track keypoint have confidences above this threshold\n  \"\"\"\n  keypoint_falloff: List[float] = [\n      0.026, 0.025, 0.025, 0.035, 0.035, 0.079, 0.079, 0.072, 0.072, 0.062,",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.config",
        "documentation": {}
    },
    {
        "label": "TrackerConfig",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.config",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.config",
        "peekOfCode": "class TrackerConfig(NamedTuple):\n  \"\"\"Shared config for pose trackers.\"\"\"\n  keypoint_tracker_params: KeypointTrackerConfig = KeypointTrackerConfig()\n  \"\"\"Keypoint tracker params.\"\"\"\n  max_tracks: int = 12\n  \"\"\"The maximum number of tracks that an internal tracker will maintain.\"\"\"\n  max_age: int = 1000 * 1000\n  \"\"\" Maximum track lifetime.\n    The maximum duration of time (in milliseconds) that a track can exist\n    without being",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.config",
        "documentation": {}
    },
    {
        "label": "KeypointTracker",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker",
        "peekOfCode": "class KeypointTracker(Tracker):\n  \"\"\"KeypointTracker, which tracks poses based on keypoint similarity.\n  This tracker assumes that keypoints are provided in normalized image\n  coordinates.\n  \"\"\"\n  def _compute_similarity(self, persons: List[Person]) -> List[List[float]]:\n    \"\"\"Computes similarity based on Object Keypoint Similarity (OKS).\n    Args:\n        persons: An array of detected `Person`s.\n    Returns:",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker",
        "documentation": {}
    },
    {
        "label": "KeypointTrackerTest",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker_test",
        "peekOfCode": "class KeypointTrackerTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    kpt_config = KeypointTrackerConfig(_KEYPOINT_CONFIDENCE_THRESHOLD,\n                                       _KEYPOINT_FALLOFF,\n                                       _MIN_NUMBER_OF_KEYPOINTS)\n    self.tracker_config = TrackerConfig(kpt_config)\n    self.kpt_tracker = KeypointTracker(self.tracker_config)\n  def test_oks(self):\n    \"\"\"Test OKS.\"\"\"",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker_test",
        "documentation": {}
    },
    {
        "label": "_KEYPOINT_CONFIDENCE_THRESHOLD",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker_test",
        "peekOfCode": "_KEYPOINT_CONFIDENCE_THRESHOLD = 0.2\n_KEYPOINT_FALLOFF = [0.1, 0.1, 0.1, 0.1]\n_MIN_NUMBER_OF_KEYPOINTS = 2\nclass KeypointTrackerTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    kpt_config = KeypointTrackerConfig(_KEYPOINT_CONFIDENCE_THRESHOLD,\n                                       _KEYPOINT_FALLOFF,\n                                       _MIN_NUMBER_OF_KEYPOINTS)\n    self.tracker_config = TrackerConfig(kpt_config)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker_test",
        "documentation": {}
    },
    {
        "label": "_KEYPOINT_FALLOFF",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker_test",
        "peekOfCode": "_KEYPOINT_FALLOFF = [0.1, 0.1, 0.1, 0.1]\n_MIN_NUMBER_OF_KEYPOINTS = 2\nclass KeypointTrackerTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    kpt_config = KeypointTrackerConfig(_KEYPOINT_CONFIDENCE_THRESHOLD,\n                                       _KEYPOINT_FALLOFF,\n                                       _MIN_NUMBER_OF_KEYPOINTS)\n    self.tracker_config = TrackerConfig(kpt_config)\n    self.kpt_tracker = KeypointTracker(self.tracker_config)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker_test",
        "documentation": {}
    },
    {
        "label": "_MIN_NUMBER_OF_KEYPOINTS",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker_test",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker_test",
        "peekOfCode": "_MIN_NUMBER_OF_KEYPOINTS = 2\nclass KeypointTrackerTest(unittest.TestCase):\n  def setUp(self):\n    super().setUp()\n    kpt_config = KeypointTrackerConfig(_KEYPOINT_CONFIDENCE_THRESHOLD,\n                                       _KEYPOINT_FALLOFF,\n                                       _MIN_NUMBER_OF_KEYPOINTS)\n    self.tracker_config = TrackerConfig(kpt_config)\n    self.kpt_tracker = KeypointTracker(self.tracker_config)\n  def test_oks(self):",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.keypoint_tracker_test",
        "documentation": {}
    },
    {
        "label": "Track",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.tracker",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.tracker",
        "peekOfCode": "class Track(NamedTuple):\n  person: Person\n  \"\"\"A person contain keypoint, bounding_box, score and id.\"\"\"\n  last_timestamp: int\n  \"\"\"The last timestamp (in milliseconds) in which a track is recorded.\"\"\"\nclass Tracker(object):\n  \"\"\"A stateful tracker for associating detections between frames.\n  This is an abstract base class that performs generic mechanics.\n  Implementations must inherit from this class.\n  \"\"\"",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.tracker",
        "documentation": {}
    },
    {
        "label": "Tracker",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.tracker",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.tracker",
        "peekOfCode": "class Tracker(object):\n  \"\"\"A stateful tracker for associating detections between frames.\n  This is an abstract base class that performs generic mechanics.\n  Implementations must inherit from this class.\n  \"\"\"\n  def __init__(self, config: TrackerConfig) -> None:\n    \"\"\"Initializes a Tracker.\"\"\"\n    self._tracks = []\n    self._config = config\n    self._next_track_id = 0",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.tracker.tracker",
        "documentation": {}
    },
    {
        "label": "BodyPart",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "peekOfCode": "class BodyPart(enum.Enum):\n  \"\"\"Enum representing human body keypoints detected by pose estimation models.\"\"\"\n  NOSE = 0\n  LEFT_EYE = 1\n  RIGHT_EYE = 2\n  LEFT_EAR = 3\n  RIGHT_EAR = 4\n  LEFT_SHOULDER = 5\n  RIGHT_SHOULDER = 6\n  LEFT_ELBOW = 7",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "documentation": {}
    },
    {
        "label": "Point",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "peekOfCode": "class Point(NamedTuple):\n  \"\"\"A point in 2D space.\"\"\"\n  x: float\n  y: float\nclass Rectangle(NamedTuple):\n  \"\"\"A rectangle in 2D space.\"\"\"\n  start_point: Point\n  end_point: Point\nclass KeyPoint(NamedTuple):\n  \"\"\"A detected human keypoint.\"\"\"",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "documentation": {}
    },
    {
        "label": "Rectangle",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "peekOfCode": "class Rectangle(NamedTuple):\n  \"\"\"A rectangle in 2D space.\"\"\"\n  start_point: Point\n  end_point: Point\nclass KeyPoint(NamedTuple):\n  \"\"\"A detected human keypoint.\"\"\"\n  body_part: BodyPart\n  coordinate: Point\n  score: float\nclass Person(NamedTuple):",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "documentation": {}
    },
    {
        "label": "KeyPoint",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "peekOfCode": "class KeyPoint(NamedTuple):\n  \"\"\"A detected human keypoint.\"\"\"\n  body_part: BodyPart\n  coordinate: Point\n  score: float\nclass Person(NamedTuple):\n  \"\"\"A pose detected by a pose estimation model.\"\"\"\n  keypoints: List[KeyPoint]\n  bounding_box: Rectangle\n  score: float",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "documentation": {}
    },
    {
        "label": "Person",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "peekOfCode": "class Person(NamedTuple):\n  \"\"\"A pose detected by a pose estimation model.\"\"\"\n  keypoints: List[KeyPoint]\n  bounding_box: Rectangle\n  score: float\n  id: int = None\ndef person_from_keypoints_with_scores(\n    keypoints_with_scores: np.ndarray,\n    image_height: float,\n    image_width: float,",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "documentation": {}
    },
    {
        "label": "Category",
        "kind": 6,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "peekOfCode": "class Category(NamedTuple):\n  \"\"\"A classification category.\"\"\"\n  label: str\n  score: float",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "documentation": {}
    },
    {
        "label": "person_from_keypoints_with_scores",
        "kind": 2,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "peekOfCode": "def person_from_keypoints_with_scores(\n    keypoints_with_scores: np.ndarray,\n    image_height: float,\n    image_width: float,\n    keypoint_score_threshold: float = 0.1) -> Person:\n  \"\"\"Creates a Person instance from single pose estimation model output.\n  Args:\n    keypoints_with_scores: Output of the TFLite pose estimation model. A numpy\n      array with shape [17, 3]. Each row represents a keypoint: [y, x, score].\n    image_height: height of the image in pixels.",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.data",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.pose_estimation",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.pose_estimation",
        "peekOfCode": "def run(estimation_model: str, tracker_type: str, classification_model: str,\n        label_file: str, camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n    estimation_model: Name of the TFLite pose estimation model.\n    tracker_type: Type of Tracker('keypoint' or 'bounding_box').\n    classification_model: Name of the TFLite pose classification model.\n      (Optional)\n    label_file: Path to the label file for the pose classification model. Class\n      names are listed one name per line, in the same order as in the",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.pose_estimation",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.pose_estimation",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.pose_estimation",
        "peekOfCode": "def main():\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      '--model',\n      help='Name of estimation model.',\n      required=False,\n      default='movenet_lightning')\n  parser.add_argument(\n      '--tracker',",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.pose_estimation",
        "documentation": {}
    },
    {
        "label": "visualize",
        "kind": 2,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.utils",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.utils",
        "peekOfCode": "def visualize(\n    image: np.ndarray,\n    list_persons: List[Person],\n    keypoint_color: Tuple[int, ...] = None,\n    keypoint_threshold: float = 0.05,\n    instance_threshold: float = 0.1,\n) -> np.ndarray:\n  \"\"\"Draws landmarks and edges on the input image and return it.\n  Args:\n    image: The input RGB image.",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "keep_aspect_ratio_resizer",
        "kind": 2,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.utils",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.utils",
        "peekOfCode": "def keep_aspect_ratio_resizer(\n    image: np.ndarray, target_size: int) -> Tuple[np.ndarray, Tuple[int, int]]:\n  \"\"\"Resizes the image.\n  The function resizes the image such that its longer side matches the required\n  target_size while keeping the image aspect ratio. Note that the resizes image\n  is padded such that both height and width are a multiple of 32, which is\n  required by the model. See\n  https://tfhub.dev/google/tfjs-model/movenet/multipose/lightning/1 for more\n  detail.\n  Args:",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "KEYPOINT_EDGE_INDS_TO_COLOR",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.utils",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.utils",
        "peekOfCode": "KEYPOINT_EDGE_INDS_TO_COLOR = {\n    (0, 1): (147, 20, 255),\n    (0, 2): (255, 255, 0),\n    (1, 3): (147, 20, 255),\n    (2, 4): (255, 255, 0),\n    (0, 5): (147, 20, 255),\n    (0, 6): (255, 255, 0),\n    (5, 7): (147, 20, 255),\n    (7, 9): (147, 20, 255),\n    (6, 8): (255, 255, 0),",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "COLOR_LIST",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.utils",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.utils",
        "peekOfCode": "COLOR_LIST = [\n    (47, 79, 79),\n    (139, 69, 19),\n    (0, 128, 0),\n    (0, 0, 139),\n    (255, 0, 0),\n    (255, 215, 0),\n    (0, 255, 0),\n    (0, 255, 255),\n    (255, 0, 255),",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "peekOfCode": "def main():\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      '--ground_truth_csv_output',\n      help='Path to generate ground truth CSV file. (Optional)',\n      required=False)\n  args = parser.parse_args()\n  # Create ground truth CSV if the ground_truth_csv parameter is set\n  if args.ground_truth_csv_output:",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "documentation": {}
    },
    {
        "label": "_MODEL_POSENET",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "peekOfCode": "_MODEL_POSENET = 'posenet'\n_MODEL_LIGHTNING = 'movenet_lightning'\n_MODEL_THUNDER = 'movenet_thunder'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_TEST_IMAGE_PATHS = ['test_data/image1.png', 'test_data/image2.jpeg']\n# Load test images\n_TEST_IMAGES = [cv2.imread(path) for path in _TEST_IMAGE_PATHS]\n# Load pose estimation models\n_POSENET = Posenet(_MODEL_POSENET)\n_MOVENET_LIGHTNING = Movenet(_MODEL_LIGHTNING)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "documentation": {}
    },
    {
        "label": "_MODEL_LIGHTNING",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "peekOfCode": "_MODEL_LIGHTNING = 'movenet_lightning'\n_MODEL_THUNDER = 'movenet_thunder'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_TEST_IMAGE_PATHS = ['test_data/image1.png', 'test_data/image2.jpeg']\n# Load test images\n_TEST_IMAGES = [cv2.imread(path) for path in _TEST_IMAGE_PATHS]\n# Load pose estimation models\n_POSENET = Posenet(_MODEL_POSENET)\n_MOVENET_LIGHTNING = Movenet(_MODEL_LIGHTNING)\n_MOVENET_THUNDER = Movenet(_MODEL_THUNDER)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "documentation": {}
    },
    {
        "label": "_MODEL_THUNDER",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "peekOfCode": "_MODEL_THUNDER = 'movenet_thunder'\n_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_TEST_IMAGE_PATHS = ['test_data/image1.png', 'test_data/image2.jpeg']\n# Load test images\n_TEST_IMAGES = [cv2.imread(path) for path in _TEST_IMAGE_PATHS]\n# Load pose estimation models\n_POSENET = Posenet(_MODEL_POSENET)\n_MOVENET_LIGHTNING = Movenet(_MODEL_LIGHTNING)\n_MOVENET_THUNDER = Movenet(_MODEL_THUNDER)\n# Load pose landmarks truth",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "documentation": {}
    },
    {
        "label": "_GROUND_TRUTH_CSV",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "peekOfCode": "_GROUND_TRUTH_CSV = 'test_data/pose_landmark_truth.csv'\n_TEST_IMAGE_PATHS = ['test_data/image1.png', 'test_data/image2.jpeg']\n# Load test images\n_TEST_IMAGES = [cv2.imread(path) for path in _TEST_IMAGE_PATHS]\n# Load pose estimation models\n_POSENET = Posenet(_MODEL_POSENET)\n_MOVENET_LIGHTNING = Movenet(_MODEL_LIGHTNING)\n_MOVENET_THUNDER = Movenet(_MODEL_THUNDER)\n# Load pose landmarks truth\n_POSE_LANDMARKS_TRUTH = pd.read_csv(_GROUND_TRUTH_CSV)",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "documentation": {}
    },
    {
        "label": "_TEST_IMAGE_PATHS",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "peekOfCode": "_TEST_IMAGE_PATHS = ['test_data/image1.png', 'test_data/image2.jpeg']\n# Load test images\n_TEST_IMAGES = [cv2.imread(path) for path in _TEST_IMAGE_PATHS]\n# Load pose estimation models\n_POSENET = Posenet(_MODEL_POSENET)\n_MOVENET_LIGHTNING = Movenet(_MODEL_LIGHTNING)\n_MOVENET_THUNDER = Movenet(_MODEL_THUNDER)\n# Load pose landmarks truth\n_POSE_LANDMARKS_TRUTH = pd.read_csv(_GROUND_TRUTH_CSV)\n_KEYPOINTS_TRUTH_LIST = [",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "documentation": {}
    },
    {
        "label": "_TEST_IMAGES",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "peekOfCode": "_TEST_IMAGES = [cv2.imread(path) for path in _TEST_IMAGE_PATHS]\n# Load pose estimation models\n_POSENET = Posenet(_MODEL_POSENET)\n_MOVENET_LIGHTNING = Movenet(_MODEL_LIGHTNING)\n_MOVENET_THUNDER = Movenet(_MODEL_THUNDER)\n# Load pose landmarks truth\n_POSE_LANDMARKS_TRUTH = pd.read_csv(_GROUND_TRUTH_CSV)\n_KEYPOINTS_TRUTH_LIST = [\n    row.to_numpy().reshape((17, 2)) for row in _POSE_LANDMARKS_TRUTH.iloc\n]",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "documentation": {}
    },
    {
        "label": "_POSENET",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "peekOfCode": "_POSENET = Posenet(_MODEL_POSENET)\n_MOVENET_LIGHTNING = Movenet(_MODEL_LIGHTNING)\n_MOVENET_THUNDER = Movenet(_MODEL_THUNDER)\n# Load pose landmarks truth\n_POSE_LANDMARKS_TRUTH = pd.read_csv(_GROUND_TRUTH_CSV)\n_KEYPOINTS_TRUTH_LIST = [\n    row.to_numpy().reshape((17, 2)) for row in _POSE_LANDMARKS_TRUTH.iloc\n]\ndef _visualize_detection_result(input_image, ground_truth):\n  \"\"\"Visualize the pose estimation result and write the output image to a file.",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "documentation": {}
    },
    {
        "label": "_MOVENET_LIGHTNING",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "peekOfCode": "_MOVENET_LIGHTNING = Movenet(_MODEL_LIGHTNING)\n_MOVENET_THUNDER = Movenet(_MODEL_THUNDER)\n# Load pose landmarks truth\n_POSE_LANDMARKS_TRUTH = pd.read_csv(_GROUND_TRUTH_CSV)\n_KEYPOINTS_TRUTH_LIST = [\n    row.to_numpy().reshape((17, 2)) for row in _POSE_LANDMARKS_TRUTH.iloc\n]\ndef _visualize_detection_result(input_image, ground_truth):\n  \"\"\"Visualize the pose estimation result and write the output image to a file.\n  The detected keypoints follow these color codes:",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "documentation": {}
    },
    {
        "label": "_MOVENET_THUNDER",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "peekOfCode": "_MOVENET_THUNDER = Movenet(_MODEL_THUNDER)\n# Load pose landmarks truth\n_POSE_LANDMARKS_TRUTH = pd.read_csv(_GROUND_TRUTH_CSV)\n_KEYPOINTS_TRUTH_LIST = [\n    row.to_numpy().reshape((17, 2)) for row in _POSE_LANDMARKS_TRUTH.iloc\n]\ndef _visualize_detection_result(input_image, ground_truth):\n  \"\"\"Visualize the pose estimation result and write the output image to a file.\n  The detected keypoints follow these color codes:\n      * PoseNet: blue",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "documentation": {}
    },
    {
        "label": "_POSE_LANDMARKS_TRUTH",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "peekOfCode": "_POSE_LANDMARKS_TRUTH = pd.read_csv(_GROUND_TRUTH_CSV)\n_KEYPOINTS_TRUTH_LIST = [\n    row.to_numpy().reshape((17, 2)) for row in _POSE_LANDMARKS_TRUTH.iloc\n]\ndef _visualize_detection_result(input_image, ground_truth):\n  \"\"\"Visualize the pose estimation result and write the output image to a file.\n  The detected keypoints follow these color codes:\n      * PoseNet: blue\n      * MoveNet Lightning: red\n      * MoveNet Thunder: yellow",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "documentation": {}
    },
    {
        "label": "_KEYPOINTS_TRUTH_LIST",
        "kind": 5,
        "importPath": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "description": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "peekOfCode": "_KEYPOINTS_TRUTH_LIST = [\n    row.to_numpy().reshape((17, 2)) for row in _POSE_LANDMARKS_TRUTH.iloc\n]\ndef _visualize_detection_result(input_image, ground_truth):\n  \"\"\"Visualize the pose estimation result and write the output image to a file.\n  The detected keypoints follow these color codes:\n      * PoseNet: blue\n      * MoveNet Lightning: red\n      * MoveNet Thunder: yellow\n      * Ground truth (from CSV): green",
        "detail": "examples.lite.examples.pose_estimation.raspberry_pi.visualizer",
        "documentation": {}
    },
    {
        "label": "_sym_db",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "_sym_db = _symbol_database.Default()\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='input-config.proto',\n  package='tensorflow_examples.lite.examples.recommendation.ml_v2.configs',\n  syntax='proto2',\n  serialized_options=None,\n  serialized_pb=b'\\n\\x12input-config.proto\\x12>tensorflow_examples.lite.examples.recommendation.ml_v2.configs\\\"\\xd9\\x01\\n\\x07\\x46\\x65\\x61ture\\x12\\x14\\n\\x0c\\x66\\x65\\x61ture_name\\x18\\x01 \\x01(\\t\\x12\\x61\\n\\x0c\\x66\\x65\\x61ture_type\\x18\\x02 \\x01(\\x0e\\x32K.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureType\\x12\\x12\\n\\nvocab_name\\x18\\x03 \\x01(\\t\\x12\\x12\\n\\nvocab_size\\x18\\x04 \\x01(\\x03\\x12\\x15\\n\\rembedding_dim\\x18\\x05 \\x01(\\x03\\x12\\x16\\n\\x0e\\x66\\x65\\x61ture_length\\x18\\x06 \\x01(\\x03\\\"\\xcc\\x01\\n\\x0c\\x46\\x65\\x61tureGroup\\x12Y\\n\\x08\\x66\\x65\\x61tures\\x18\\x01 \\x03(\\x0b\\x32G.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature\\x12\\x61\\n\\x0c\\x65ncoder_type\\x18\\x02 \\x01(\\x0e\\x32K.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.EncoderType\\\"\\xc9\\x02\\n\\x0bInputConfig\\x12k\\n\\x15global_feature_groups\\x18\\x01 \\x03(\\x0b\\x32L.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup\\x12m\\n\\x17\\x61\\x63tivity_feature_groups\\x18\\x02 \\x03(\\x0b\\x32L.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup\\x12^\\n\\rlabel_feature\\x18\\x03 \\x01(\\x0b\\x32G.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature*-\\n\\x0b\\x46\\x65\\x61tureType\\x12\\n\\n\\x06STRING\\x10\\x00\\x12\\x07\\n\\x03INT\\x10\\x01\\x12\\t\\n\\x05\\x46LOAT\\x10\\x02*)\\n\\x0b\\x45ncoderType\\x12\\x07\\n\\x03\\x42OW\\x10\\x00\\x12\\x07\\n\\x03\\x43NN\\x10\\x01\\x12\\x08\\n\\x04LSTM\\x10\\x02'\n)\n_FEATURETYPE = _descriptor.EnumDescriptor(\n  name='FeatureType',",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "DESCRIPTOR = _descriptor.FileDescriptor(\n  name='input-config.proto',\n  package='tensorflow_examples.lite.examples.recommendation.ml_v2.configs',\n  syntax='proto2',\n  serialized_options=None,\n  serialized_pb=b'\\n\\x12input-config.proto\\x12>tensorflow_examples.lite.examples.recommendation.ml_v2.configs\\\"\\xd9\\x01\\n\\x07\\x46\\x65\\x61ture\\x12\\x14\\n\\x0c\\x66\\x65\\x61ture_name\\x18\\x01 \\x01(\\t\\x12\\x61\\n\\x0c\\x66\\x65\\x61ture_type\\x18\\x02 \\x01(\\x0e\\x32K.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureType\\x12\\x12\\n\\nvocab_name\\x18\\x03 \\x01(\\t\\x12\\x12\\n\\nvocab_size\\x18\\x04 \\x01(\\x03\\x12\\x15\\n\\rembedding_dim\\x18\\x05 \\x01(\\x03\\x12\\x16\\n\\x0e\\x66\\x65\\x61ture_length\\x18\\x06 \\x01(\\x03\\\"\\xcc\\x01\\n\\x0c\\x46\\x65\\x61tureGroup\\x12Y\\n\\x08\\x66\\x65\\x61tures\\x18\\x01 \\x03(\\x0b\\x32G.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature\\x12\\x61\\n\\x0c\\x65ncoder_type\\x18\\x02 \\x01(\\x0e\\x32K.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.EncoderType\\\"\\xc9\\x02\\n\\x0bInputConfig\\x12k\\n\\x15global_feature_groups\\x18\\x01 \\x03(\\x0b\\x32L.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup\\x12m\\n\\x17\\x61\\x63tivity_feature_groups\\x18\\x02 \\x03(\\x0b\\x32L.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup\\x12^\\n\\rlabel_feature\\x18\\x03 \\x01(\\x0b\\x32G.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature*-\\n\\x0b\\x46\\x65\\x61tureType\\x12\\n\\n\\x06STRING\\x10\\x00\\x12\\x07\\n\\x03INT\\x10\\x01\\x12\\t\\n\\x05\\x46LOAT\\x10\\x02*)\\n\\x0b\\x45ncoderType\\x12\\x07\\n\\x03\\x42OW\\x10\\x00\\x12\\x07\\n\\x03\\x43NN\\x10\\x01\\x12\\x08\\n\\x04LSTM\\x10\\x02'\n)\n_FEATURETYPE = _descriptor.EnumDescriptor(\n  name='FeatureType',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureType',",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURETYPE",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "_FEATURETYPE = _descriptor.EnumDescriptor(\n  name='FeatureType',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureType',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='STRING', index=0, number=0,\n      serialized_options=None,\n      type=None),",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "FeatureType",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "FeatureType = enum_type_wrapper.EnumTypeWrapper(_FEATURETYPE)\n_ENCODERTYPE = _descriptor.EnumDescriptor(\n  name='EncoderType',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.EncoderType',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='BOW', index=0, number=0,\n      serialized_options=None,",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "_ENCODERTYPE",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "_ENCODERTYPE = _descriptor.EnumDescriptor(\n  name='EncoderType',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.EncoderType',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='BOW', index=0, number=0,\n      serialized_options=None,\n      type=None),",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "EncoderType",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "EncoderType = enum_type_wrapper.EnumTypeWrapper(_ENCODERTYPE)\nSTRING = 0\nINT = 1\nFLOAT = 2\nBOW = 0\nCNN = 1\nLSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "STRING",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "STRING = 0\nINT = 1\nFLOAT = 2\nBOW = 0\nCNN = 1\nLSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "INT",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "INT = 1\nFLOAT = 2\nBOW = 0\nCNN = 1\nLSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,\n  file=DESCRIPTOR,",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "FLOAT",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "FLOAT = 2\nBOW = 0\nCNN = 1\nLSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "BOW",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "BOW = 0\nCNN = 1\nLSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "CNN",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "CNN = 1\nLSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "LSTM",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "LSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='feature_name', full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature.feature_name', index=0,",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURE",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='feature_name', full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature.feature_name', index=0,\n      number=1, type=9, cpp_type=9, label=1,",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATUREGROUP",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "_FEATUREGROUP = _descriptor.Descriptor(\n  name='FeatureGroup',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='features', full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup.features', index=0,\n      number=1, type=11, cpp_type=10, label=3,",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "_INPUTCONFIG",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "_INPUTCONFIG = _descriptor.Descriptor(\n  name='InputConfig',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.InputConfig',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='global_feature_groups', full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.InputConfig.global_feature_groups', index=0,\n      number=1, type=11, cpp_type=10, label=3,",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURE.fields_by_name['feature_type'].enum_type",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "_FEATURE.fields_by_name['feature_type'].enum_type = _FEATURETYPE\n_FEATUREGROUP.fields_by_name['features'].message_type = _FEATURE\n_FEATUREGROUP.fields_by_name['encoder_type'].enum_type = _ENCODERTYPE\n_INPUTCONFIG.fields_by_name['global_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['activity_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['label_feature'].message_type = _FEATURE\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATUREGROUP.fields_by_name['features'].message_type",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "_FEATUREGROUP.fields_by_name['features'].message_type = _FEATURE\n_FEATUREGROUP.fields_by_name['encoder_type'].enum_type = _ENCODERTYPE\n_INPUTCONFIG.fields_by_name['global_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['activity_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['label_feature'].message_type = _FEATURE\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATUREGROUP.fields_by_name['encoder_type'].enum_type",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "_FEATUREGROUP.fields_by_name['encoder_type'].enum_type = _ENCODERTYPE\n_INPUTCONFIG.fields_by_name['global_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['activity_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['label_feature'].message_type = _FEATURE\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "_INPUTCONFIG.fields_by_name['global_feature_groups'].message_type",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "_INPUTCONFIG.fields_by_name['global_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['activity_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['label_feature'].message_type = _FEATURE\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "_INPUTCONFIG.fields_by_name['activity_feature_groups'].message_type",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "_INPUTCONFIG.fields_by_name['activity_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['label_feature'].message_type = _FEATURE\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "_INPUTCONFIG.fields_by_name['label_feature'].message_type",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "_INPUTCONFIG.fields_by_name['label_feature'].message_type = _FEATURE\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['Feature']",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature)",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['FeatureGroup']",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature)\n  })",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['InputConfig']",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature)\n  })\n_sym_db.RegisterMessage(Feature)",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.enum_types_by_name['FeatureType']",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "DESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature)\n  })\n_sym_db.RegisterMessage(Feature)\nFeatureGroup = _reflection.GeneratedProtocolMessageType('FeatureGroup', (_message.Message,), {",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.enum_types_by_name['EncoderType']",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "DESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature)\n  })\n_sym_db.RegisterMessage(Feature)\nFeatureGroup = _reflection.GeneratedProtocolMessageType('FeatureGroup', (_message.Message,), {\n  'DESCRIPTOR' : _FEATUREGROUP,",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "Feature",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "Feature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature)\n  })\n_sym_db.RegisterMessage(Feature)\nFeatureGroup = _reflection.GeneratedProtocolMessageType('FeatureGroup', (_message.Message,), {\n  'DESCRIPTOR' : _FEATUREGROUP,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup)",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "FeatureGroup",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "FeatureGroup = _reflection.GeneratedProtocolMessageType('FeatureGroup', (_message.Message,), {\n  'DESCRIPTOR' : _FEATUREGROUP,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup)\n  })\n_sym_db.RegisterMessage(FeatureGroup)\nInputConfig = _reflection.GeneratedProtocolMessageType('InputConfig', (_message.Message,), {\n  'DESCRIPTOR' : _INPUTCONFIG,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.InputConfig)",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "InputConfig",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "description": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "peekOfCode": "InputConfig = _reflection.GeneratedProtocolMessageType('InputConfig', (_message.Message,), {\n  'DESCRIPTOR' : _INPUTCONFIG,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.InputConfig)\n  })\n_sym_db.RegisterMessage(InputConfig)\n# @@protoc_insertion_point(module_scope)\n# pyformat: enable",
        "detail": "examples.lite.examples.recommendation.ml.configs.input_config_generated_pb2",
        "documentation": {}
    },
    {
        "label": "ModelConfig",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.configs.model_config",
        "description": "examples.lite.examples.recommendation.ml.configs.model_config",
        "peekOfCode": "class ModelConfig(object):\n  \"\"\"Class to hold parameters for model architecture configuration.\n  Attributes:\n      hidden_layer_dims: List of hidden layer dimensions.\n      eval_top_k: Top k to evaluate.\n      conv_num_filter_ratios: Number of filter ratios for the Conv1D layer.\n      conv_kernel_size: Size of the Conv1D layer kernel size.\n      lstm_num_units: Number of units for the LSTM layer.\n      num_predictions: Number of predictions to return with serving mode, which\n      has default value 10.",
        "detail": "examples.lite.examples.recommendation.ml.configs.model_config",
        "documentation": {}
    },
    {
        "label": "MovieInfo",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "class MovieInfo(\n    collections.namedtuple(\n        \"MovieInfo\", [\"movie_id\", \"timestamp\", \"rating\", \"title\", \"genres\"])):\n  \"\"\"Data holder of basic information of a movie.\"\"\"\n  __slots__ = ()\n  def __new__(cls,\n              movie_id=PAD_MOVIE_ID,\n              timestamp=0,\n              rating=PAD_RATING,\n              title=\"\",",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")\n  flags.DEFINE_string(\"output_dir\", None,\n                      \"Path to the directory of output files.\")\n  flags.DEFINE_bool(\"build_vocabs\", True,\n                    \"If yes, generate movie feature vocabs.\")\n  flags.DEFINE_integer(\"min_timeline_length\", 3,\n                       \"The minimum timeline length to construct examples.\")",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "download_and_extract_data",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def download_and_extract_data(data_directory,\n                              url=MOVIELENS_1M_URL,\n                              fname=MOVIELENS_ZIP_FILENAME,\n                              file_hash=MOVIELENS_ZIP_HASH,\n                              extracted_dir_name=MOVIELENS_EXTRACTED_DIR):\n  \"\"\"Download and extract zip containing MovieLens data to a given directory.\n  Args:\n    data_directory: Local path to extract dataset to.\n    url: Direct path to MovieLens dataset .zip file. See constants above for\n      examples.",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "read_data",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def read_data(data_directory, min_rating=None):\n  \"\"\"Read movielens ratings.dat and movies.dat file into dataframe.\"\"\"\n  ratings_df = pd.read_csv(\n      os.path.join(data_directory, RATINGS_FILE_NAME),\n      sep=\"::\",\n      names=RATINGS_DATA_COLUMNS,\n      encoding=\"unicode_escape\")  # May contain unicode. Need to escape.\n  ratings_df[\"Timestamp\"] = ratings_df[\"Timestamp\"].apply(int)\n  if min_rating is not None:\n    ratings_df = ratings_df[ratings_df[\"Rating\"] >= min_rating]",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "convert_to_timelines",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def convert_to_timelines(ratings_df):\n  \"\"\"Convert ratings data to user.\"\"\"\n  timelines = collections.defaultdict(list)\n  movie_counts = collections.Counter()\n  for user_id, movie_id, rating, timestamp in ratings_df.values:\n    timelines[user_id].append(\n        MovieInfo(movie_id=movie_id, timestamp=int(timestamp), rating=rating))\n    movie_counts[movie_id] += 1\n  # Sort per-user timeline by timestamp\n  for (user_id, context) in timelines.items():",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_movies_dict",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_movies_dict(movies_df):\n  \"\"\"Generates movies dictionary from movies dataframe.\"\"\"\n  movies_dict = {\n      movie_id: MovieInfo(movie_id=movie_id, title=title, genres=genres)\n      for movie_id, title, genres in movies_df.values\n  }\n  movies_dict[0] = MovieInfo()\n  return movies_dict\ndef extract_year_from_title(title):\n  year = re.search(r\"\\((\\d{4})\\)\", title)",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "extract_year_from_title",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def extract_year_from_title(title):\n  year = re.search(r\"\\((\\d{4})\\)\", title)\n  if year:\n    return int(year.group(1))\n  return 0\ndef generate_feature_of_movie_years(movies_dict, movies):\n  \"\"\"Extracts year feature for movies from movie title.\"\"\"\n  return [\n      extract_year_from_title(movies_dict[movie.movie_id].title)\n      for movie in movies",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_feature_of_movie_years",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_feature_of_movie_years(movies_dict, movies):\n  \"\"\"Extracts year feature for movies from movie title.\"\"\"\n  return [\n      extract_year_from_title(movies_dict[movie.movie_id].title)\n      for movie in movies\n  ]\ndef generate_movie_genres(movies_dict, movies):\n  \"\"\"Create a feature of the genre of each movie.\n  Save genre as a feature for the movies.\n  Args:",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_movie_genres",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_movie_genres(movies_dict, movies):\n  \"\"\"Create a feature of the genre of each movie.\n  Save genre as a feature for the movies.\n  Args:\n    movies_dict: Dict of movies, keyed by movie_id with value of (title, genre)\n    movies: list of movies to extract genres.\n  Returns:\n    movie_genres: list of genres of all input movies.\n  \"\"\"\n  movie_genres = []",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_examples_from_single_timeline",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_examples_from_single_timeline(timeline,\n                                           movies_dict,\n                                           max_context_len=100,\n                                           max_context_movie_genre_len=320):\n  \"\"\"Generate TF examples from a single user timeline.\n  Generate TF examples from a single user timeline. Timeline with length less\n  than minimum timeline length will be skipped. And if context user history\n  length is shorter than max_context_len, features will be padded with default\n  values.\n  Args:",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_examples_from_timelines",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_examples_from_timelines(timelines,\n                                     movies_df,\n                                     min_timeline_len=3,\n                                     max_context_len=100,\n                                     max_context_movie_genre_len=320,\n                                     train_data_fraction=0.9,\n                                     random_seed=None,\n                                     shuffle=True):\n  \"\"\"Convert user timelines to tf examples.\n  Convert user timelines to tf examples by adding all possible context-label",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_movie_feature_vocabs",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_movie_feature_vocabs(movies_df, movie_counts):\n  \"\"\"Generate vocabularies for movie features.\n  Generate vocabularies for movie features (movie_id, genre, year), sorted by\n  usage count. Vocab id 0 will be reserved for default padding value.\n  Args:\n    movies_df: Dataframe for movies.\n    movie_counts: Counts that each movie is rated.\n  Returns:\n    movie_id_vocab: List of all movie ids paired with movie usage count, and\n      sorted by counts.",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "write_tfrecords",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def write_tfrecords(tf_examples, filename):\n  \"\"\"Writes tf examples to tfrecord file, and returns the count.\"\"\"\n  with tf.io.TFRecordWriter(filename) as file_writer:\n    length = len(tf_examples)\n    progress_bar = tf.keras.utils.Progbar(length)\n    for example in tf_examples:\n      file_writer.write(example.SerializeToString())\n      progress_bar.add(1)\n    return length\ndef write_vocab_json(vocab, filename):",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "write_vocab_json",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def write_vocab_json(vocab, filename):\n  \"\"\"Write generated movie vocabulary to specified file.\"\"\"\n  with open(filename, \"w\", encoding=\"utf-8\") as jsonfile:\n    json.dump(vocab, jsonfile, indent=2)\ndef write_vocab_txt(vocab, filename):\n  with open(filename, \"w\", encoding=\"utf-8\") as f:\n    for item in vocab:\n      f.write(str(item) + \"\\n\")\ndef generate_datasets(extracted_data_dir,\n                      output_dir,",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "write_vocab_txt",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def write_vocab_txt(vocab, filename):\n  with open(filename, \"w\", encoding=\"utf-8\") as f:\n    for item in vocab:\n      f.write(str(item) + \"\\n\")\ndef generate_datasets(extracted_data_dir,\n                      output_dir,\n                      min_timeline_length,\n                      max_context_length,\n                      max_context_movie_genre_length,\n                      min_rating=None,",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_datasets",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_datasets(extracted_data_dir,\n                      output_dir,\n                      min_timeline_length,\n                      max_context_length,\n                      max_context_movie_genre_length,\n                      min_rating=None,\n                      build_vocabs=True,\n                      train_data_fraction=0.9,\n                      train_filename=OUTPUT_TRAINING_DATA_FILENAME,\n                      test_filename=OUTPUT_TESTING_DATA_FILENAME,",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def main(_):\n  logging.info(\"Downloading and extracting data.\")\n  extracted_data_dir = download_and_extract_data(data_directory=FLAGS.data_dir)\n  stats = generate_datasets(\n      extracted_data_dir=extracted_data_dir,\n      output_dir=FLAGS.output_dir,\n      min_timeline_length=FLAGS.min_timeline_length,\n      max_context_length=FLAGS.max_context_length,\n      max_context_movie_genre_length=FLAGS.max_context_movie_genre_length,\n      min_rating=FLAGS.min_rating,",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "FLAGS = flags.FLAGS\n# Permalinks to download movielens data.\nMOVIELENS_1M_URL = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\nMOVIELENS_ZIP_FILENAME = \"ml-1m.zip\"\nMOVIELENS_ZIP_HASH = \"a6898adb50b9ca05aa231689da44c217cb524e7ebd39d264c56e2832f2c54e20\"\nMOVIELENS_EXTRACTED_DIR = \"ml-1m\"\nRATINGS_FILE_NAME = \"ratings.dat\"\nMOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "MOVIELENS_1M_URL",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "MOVIELENS_1M_URL = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\nMOVIELENS_ZIP_FILENAME = \"ml-1m.zip\"\nMOVIELENS_ZIP_HASH = \"a6898adb50b9ca05aa231689da44c217cb524e7ebd39d264c56e2832f2c54e20\"\nMOVIELENS_EXTRACTED_DIR = \"ml-1m\"\nRATINGS_FILE_NAME = \"ratings.dat\"\nMOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "MOVIELENS_ZIP_FILENAME",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "MOVIELENS_ZIP_FILENAME = \"ml-1m.zip\"\nMOVIELENS_ZIP_HASH = \"a6898adb50b9ca05aa231689da44c217cb524e7ebd39d264c56e2832f2c54e20\"\nMOVIELENS_EXTRACTED_DIR = \"ml-1m\"\nRATINGS_FILE_NAME = \"ratings.dat\"\nMOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "MOVIELENS_ZIP_HASH",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "MOVIELENS_ZIP_HASH = \"a6898adb50b9ca05aa231689da44c217cb524e7ebd39d264c56e2832f2c54e20\"\nMOVIELENS_EXTRACTED_DIR = \"ml-1m\"\nRATINGS_FILE_NAME = \"ratings.dat\"\nMOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "MOVIELENS_EXTRACTED_DIR",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "MOVIELENS_EXTRACTED_DIR = \"ml-1m\"\nRATINGS_FILE_NAME = \"ratings.dat\"\nMOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "RATINGS_FILE_NAME",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "RATINGS_FILE_NAME = \"ratings.dat\"\nMOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "MOVIES_FILE_NAME",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "MOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "RATINGS_DATA_COLUMNS",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "RATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "MOVIES_DATA_COLUMNS",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "MOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_TRAINING_DATA_FILENAME",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_TESTING_DATA_FILENAME",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_MOVIE_VOCAB_FILENAME",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_MOVIE_YEAR_VOCAB_FILENAME",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_MOVIE_GENRE_VOCAB_FILENAME",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "PAD_MOVIE_ID",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "PAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "PAD_RATING",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "PAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")\n  flags.DEFINE_string(\"output_dir\", None,",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "PAD_MOVIE_YEAR",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "PAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")\n  flags.DEFINE_string(\"output_dir\", None,\n                      \"Path to the directory of output files.\")",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "UNKNOWN_STR",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "UNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")\n  flags.DEFINE_string(\"output_dir\", None,\n                      \"Path to the directory of output files.\")\n  flags.DEFINE_bool(\"build_vocabs\", True,",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "VOCAB_MOVIE_ID_INDEX",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "VOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")\n  flags.DEFINE_string(\"output_dir\", None,\n                      \"Path to the directory of output files.\")\n  flags.DEFINE_bool(\"build_vocabs\", True,\n                    \"If yes, generate movie feature vocabs.\")",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "VOCAB_COUNT_INDEX",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "VOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")\n  flags.DEFINE_string(\"output_dir\", None,\n                      \"Path to the directory of output files.\")\n  flags.DEFINE_bool(\"build_vocabs\", True,\n                    \"If yes, generate movie feature vocabs.\")\n  flags.DEFINE_integer(\"min_timeline_length\", 3,",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "ExampleGenerationMovielensTest",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "peekOfCode": "class ExampleGenerationMovielensTest(tf.test.TestCase):\n  def test_example_generation(self):\n    timelines, _ = example_gen.convert_to_timelines(RATINGS_DF)\n    train_examples, test_examples = example_gen.generate_examples_from_timelines(\n        timelines=timelines,\n        movies_df=MOVIES_DF,\n        min_timeline_len=2,\n        max_context_len=5,\n        max_context_movie_genre_len=10,\n        train_data_fraction=0.66,",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "documentation": {}
    },
    {
        "label": "MOVIES_DF",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "peekOfCode": "MOVIES_DF = pd.DataFrame([\n    {\n        'MovieId': int(1),\n        'Title': 'Toy Story (1995)',\n        'Genres': 'Animation|Children|Comedy'\n    },\n    {\n        'MovieId': int(2),\n        'Title': 'Four Weddings and a Funeral (1994)',\n        'Genres': 'Comedy|Romance'",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "documentation": {}
    },
    {
        "label": "RATINGS_DF",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "peekOfCode": "RATINGS_DF = pd.DataFrame([\n    {\n        'UserID': int(1),\n        'MovieId': int(1),\n        'Rating': 3.5,\n        'Timestamp': 0\n    },\n    {\n        'UserID': int(1),\n        'MovieId': int(2),",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "documentation": {}
    },
    {
        "label": "EXAMPLE1",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "peekOfCode": "EXAMPLE1 = text_format.Parse(\n    \"\"\"\n    features {\n        feature {\n          key: \"context_movie_id\"\n          value {\n            int64_list {\n              value: [1, 0, 0, 0, 0]\n            }\n          }",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "documentation": {}
    },
    {
        "label": "EXAMPLE2",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "peekOfCode": "EXAMPLE2 = text_format.Parse(\n    \"\"\"\n    features {\n        feature {\n          key: \"context_movie_id\"\n          value {\n            int64_list {\n              value: [1, 2, 0, 0, 0]\n            }\n          }",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "documentation": {}
    },
    {
        "label": "EXAMPLE3",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "description": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "peekOfCode": "EXAMPLE3 = text_format.Parse(\n    \"\"\"\n    features {\n        feature {\n          key: \"context_movie_id\"\n          value {\n            int64_list {\n              value: [1, 2, 3, 0, 0]\n            }\n          }",
        "detail": "examples.lite.examples.recommendation.ml.data.example_generation_movielens_test",
        "documentation": {}
    },
    {
        "label": "ContextEncoder",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.context_encoder",
        "description": "examples.lite.examples.recommendation.ml.model.context_encoder",
        "peekOfCode": "class ContextEncoder(tf.keras.layers.Layer):\n  \"\"\"Layer to encode context sequence.\n  This encoder layer supports three types: 1) bow: bag of words style averaging\n  sequence embeddings. 2) cnn: use convolutional neural network to encode\n  sequence. 3) rnn: use recurrent neural network to encode sequence.\n  This encoder should be initialized with a predefined embedding layer, encoder\n  type and necessary parameters corresponding to the encoder type.\n  \"\"\"\n  def __init__(self,\n               input_config: input_config_pb2.InputConfig,",
        "detail": "examples.lite.examples.recommendation.ml.model.context_encoder",
        "documentation": {}
    },
    {
        "label": "FeatureGroupEncoder",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.context_encoder",
        "description": "examples.lite.examples.recommendation.ml.model.context_encoder",
        "peekOfCode": "class FeatureGroupEncoder(tf.keras.layers.Layer):\n  \"\"\"Layer to generate encoding for the feature group.\n  Layer to generate encoding for the group of features in the feature\n  group with encoder type (BOW/CNN/LSTM) specified in the feature group config.\n  Embeddings of INT or STRING type features are concatenated first, and FLOAT\n  type feature values are appended after. Embedding vector is properly masked.\n  \"\"\"\n  def __init__(self,\n               feature_group: input_config_pb2.FeatureGroup,\n               model_config: model_config_class.ModelConfig,",
        "detail": "examples.lite.examples.recommendation.ml.model.context_encoder",
        "documentation": {}
    },
    {
        "label": "safe_div",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.context_encoder",
        "description": "examples.lite.examples.recommendation.ml.model.context_encoder",
        "peekOfCode": "def safe_div(x, y):\n  return tf.where(tf.not_equal(y, 0), tf.divide(x, y), tf.zeros_like(x))\nclass ContextEncoder(tf.keras.layers.Layer):\n  \"\"\"Layer to encode context sequence.\n  This encoder layer supports three types: 1) bow: bag of words style averaging\n  sequence embeddings. 2) cnn: use convolutional neural network to encode\n  sequence. 3) rnn: use recurrent neural network to encode sequence.\n  This encoder should be initialized with a predefined embedding layer, encoder\n  type and necessary parameters corresponding to the encoder type.\n  \"\"\"",
        "detail": "examples.lite.examples.recommendation.ml.model.context_encoder",
        "documentation": {}
    },
    {
        "label": "ContextEncoderTest",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.context_encoder_test",
        "description": "examples.lite.examples.recommendation.ml.model.context_encoder_test",
        "peekOfCode": "class ContextEncoderTest(tf.test.TestCase):\n  def _create_test_feature_group(self,\n                                 encoder_type: input_config_pb2.EncoderType):\n    \"\"\"Prepare test feature group.\"\"\"\n    feature_context_movie_id = input_config_pb2.Feature(\n        feature_name='context_movie_id',\n        feature_type=input_config_pb2.FeatureType.INT,\n        vocab_size=3952,\n        embedding_dim=4)\n    feature_context_movie_rating = input_config_pb2.Feature(",
        "detail": "examples.lite.examples.recommendation.ml.model.context_encoder_test",
        "documentation": {}
    },
    {
        "label": "DotProductSimilarity",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.dotproduct_similarity",
        "description": "examples.lite.examples.recommendation.ml.model.dotproduct_similarity",
        "peekOfCode": "class DotProductSimilarity(tf.keras.layers.Layer):\n  \"\"\"Layer to comput dotproduct similarities for context/label embedding.\n    The top_k is an integer to represent top_k ids to compute among label ids.\n    if top_k is None, top_k computation will be ignored.\n  \"\"\"\n  def call(self,\n           context_embeddings: tf.Tensor,\n           label_embeddings: tf.Tensor,\n           top_k: Optional[int] = None):\n    \"\"\"Generate dotproduct similarity matrix and top values/indices.",
        "detail": "examples.lite.examples.recommendation.ml.model.dotproduct_similarity",
        "documentation": {}
    },
    {
        "label": "DotproductSimilarityTest",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.dotproduct_similarity_test",
        "description": "examples.lite.examples.recommendation.ml.model.dotproduct_similarity_test",
        "peekOfCode": "class DotproductSimilarityTest(tf.test.TestCase):\n  def test_dotproduct(self):\n    context_embeddings = tf.constant([[0.1, 0.1, 0.1, 0.1],\n                                      [0.2, 0.2, 0.2, 0.2]])\n    label_embeddings = tf.constant([[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.1, 0.1]])\n    similarity_layer = dotproduct_similarity.DotProductSimilarity()\n    dotproduct = similarity_layer(context_embeddings, label_embeddings)\n    dotproduct, top_ids, top_scores = similarity_layer(context_embeddings,\n                                                       label_embeddings, 1)\n    self.assertAllClose(tf.constant([[0.04, 0.04], [0.08, 0.08]]), dotproduct)",
        "detail": "examples.lite.examples.recommendation.ml.model.dotproduct_similarity_test",
        "documentation": {}
    },
    {
        "label": "FeaturesAndVocabsByName",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "description": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "peekOfCode": "class FeaturesAndVocabsByName(\n    collections.namedtuple(\n        'FeaturesAndVocabsByName', ['features_by_name', 'vocabs_by_name'])):\n  \"\"\"Holder for intermediate data in input processing pipeline.\"\"\"\n  __slots__ = ()\n  def __new__(cls, features_by_name=None, vocabs_by_name=None):\n    return super(FeaturesAndVocabsByName, cls).__new__(cls,\n                                                       features_by_name,\n                                                       vocabs_by_name)\ndef _prepare_feature_vocab_table(",
        "detail": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "get_features_and_vocabs_by_name",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "description": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "peekOfCode": "def get_features_and_vocabs_by_name(\n    input_config: input_config_pb2.InputConfig,\n    vocab_file_dir: str = '') -> FeaturesAndVocabsByName:\n  \"\"\"Get feature and vocabulary dictionaries according to input config.\n  Args:\n    input_config: The input config input_config_pb2.InputConfig proto.\n    vocab_file_dir: The directory storing vocabulary files.\n  Returns:\n    A FeaturesAndVocabsByName object containing features and vocabs\n    dictionaries keyed feature name for all features according to input",
        "detail": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "get_serving_input_specs",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "description": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "peekOfCode": "def get_serving_input_specs(\n    input_config: input_config_pb2.InputConfig) -> Dict[str, tf.TensorSpec]:\n  features_by_name = get_features_and_vocabs_by_name(\n      input_config).features_by_name\n  input_specs = collections.OrderedDict()\n  for feature_name, feature in sorted(features_by_name.items()):\n    input_specs[feature_name] = _get_serving_feature_spec(\n        feature_name, feature.feature_type, feature.feature_length)\n  return input_specs\ndef decode_example(",
        "detail": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "decode_example",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "description": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "peekOfCode": "def decode_example(\n    serialized_proto: str, features_and_vocabs_by_name: FeaturesAndVocabsByName,\n    label_feature_name: str) -> Tuple[Dict[str, tf.Tensor], tf.Tensor]:\n  \"\"\"Decode single serialized example.\n  Decode single serialized example, accoring to specified features in input\n  config. Perform vocabulary lookup if vocabulary is specified.\n  Args:\n    serialized_proto: The serialized proto that needs to be decoded.\n    features_and_vocabs_by_name: A FeaturesAndVocabsByName object containing\n      features and vocabs dictionaries by feature names.",
        "detail": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "get_input_dataset",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "description": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "peekOfCode": "def get_input_dataset(data_filepattern: str,\n                      input_config: input_config_pb2.InputConfig,\n                      vocab_file_dir: str,\n                      batch_size: int) -> tf.data.Dataset:\n  \"\"\"An input_fn to create input datasets.\n  Args:\n    data_filepattern: The file pattern of the input data.\n    input_config: The input config input_config_pb2.InputConfig proto.\n    vocab_file_dir: The path to the directory storing the vocabulary files.\n    batch_size: Batch size of to-be generated dataset.",
        "detail": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "INT_DEFAULT_VALUE",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "description": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "peekOfCode": "INT_DEFAULT_VALUE = 0\nSTRING_DEFAULT_VALUE = 'UNK'\nFLOAT_DEFAULT_VALUE = 0.0\nclass FeaturesAndVocabsByName(\n    collections.namedtuple(\n        'FeaturesAndVocabsByName', ['features_by_name', 'vocabs_by_name'])):\n  \"\"\"Holder for intermediate data in input processing pipeline.\"\"\"\n  __slots__ = ()\n  def __new__(cls, features_by_name=None, vocabs_by_name=None):\n    return super(FeaturesAndVocabsByName, cls).__new__(cls,",
        "detail": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "STRING_DEFAULT_VALUE",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "description": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "peekOfCode": "STRING_DEFAULT_VALUE = 'UNK'\nFLOAT_DEFAULT_VALUE = 0.0\nclass FeaturesAndVocabsByName(\n    collections.namedtuple(\n        'FeaturesAndVocabsByName', ['features_by_name', 'vocabs_by_name'])):\n  \"\"\"Holder for intermediate data in input processing pipeline.\"\"\"\n  __slots__ = ()\n  def __new__(cls, features_by_name=None, vocabs_by_name=None):\n    return super(FeaturesAndVocabsByName, cls).__new__(cls,\n                                                       features_by_name,",
        "detail": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "FLOAT_DEFAULT_VALUE",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "description": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "peekOfCode": "FLOAT_DEFAULT_VALUE = 0.0\nclass FeaturesAndVocabsByName(\n    collections.namedtuple(\n        'FeaturesAndVocabsByName', ['features_by_name', 'vocabs_by_name'])):\n  \"\"\"Holder for intermediate data in input processing pipeline.\"\"\"\n  __slots__ = ()\n  def __new__(cls, features_by_name=None, vocabs_by_name=None):\n    return super(FeaturesAndVocabsByName, cls).__new__(cls,\n                                                       features_by_name,\n                                                       vocabs_by_name)",
        "detail": "examples.lite.examples.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "InputPipelineTest",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.input_pipeline_test",
        "description": "examples.lite.examples.recommendation.ml.model.input_pipeline_test",
        "peekOfCode": "class InputPipelineTest(tf.test.TestCase):\n  def _AssertSparseTensorValueEqual(self, a, b):\n    self.assertAllEqual(a.indices, b.indices)\n    self.assertAllEqual(a.values, b.values)\n    self.assertAllEqual(a.dense_shape, b.dense_shape)\n  def setUp(self):\n    super(InputPipelineTest, self).setUp()\n    self.tmp_dir = self.create_tempdir()\n    self.test_movie_genre_vocab_file = os.path.join(self.tmp_dir,\n                                                    'movie_genre_vocab.txt')",
        "detail": "examples.lite.examples.recommendation.ml.model.input_pipeline_test",
        "documentation": {}
    },
    {
        "label": "FAKE_MOVIE_GENRE_VOCAB",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.input_pipeline_test",
        "description": "examples.lite.examples.recommendation.ml.model.input_pipeline_test",
        "peekOfCode": "FAKE_MOVIE_GENRE_VOCAB = [\n    'UNK', 'Comedy', 'Drama', 'Romance', 'Animation', 'Children'\n]\nTEST_INPUT_CONFIG = text_format.Parse(\n    \"\"\"\n    activity_feature_groups {\n      features {\n        feature_name: \"context_movie_id\"\n        feature_type: INT\n        vocab_size: 3952",
        "detail": "examples.lite.examples.recommendation.ml.model.input_pipeline_test",
        "documentation": {}
    },
    {
        "label": "TEST_INPUT_CONFIG",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.input_pipeline_test",
        "description": "examples.lite.examples.recommendation.ml.model.input_pipeline_test",
        "peekOfCode": "TEST_INPUT_CONFIG = text_format.Parse(\n    \"\"\"\n    activity_feature_groups {\n      features {\n        feature_name: \"context_movie_id\"\n        feature_type: INT\n        vocab_size: 3952\n        embedding_dim: 32\n        feature_length: 5\n      }",
        "detail": "examples.lite.examples.recommendation.ml.model.input_pipeline_test",
        "documentation": {}
    },
    {
        "label": "EXAMPLE1",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.input_pipeline_test",
        "description": "examples.lite.examples.recommendation.ml.model.input_pipeline_test",
        "peekOfCode": "EXAMPLE1 = text_format.Parse(\n    \"\"\"\n    features {\n        feature {\n          key: \"context_movie_id\"\n          value {\n            int64_list {\n              value: [1, 2, 0, 0, 0]\n            }\n          }",
        "detail": "examples.lite.examples.recommendation.ml.model.input_pipeline_test",
        "documentation": {}
    },
    {
        "label": "LabelEncoder",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.label_encoder",
        "description": "examples.lite.examples.recommendation.ml.model.label_encoder",
        "peekOfCode": "class LabelEncoder(tf.keras.layers.Layer):\n  \"\"\"Layer to encode label feature.\n  Currently only id-based label encoder is supported. With this encoder, label\n  embedding layer will be created based on input config. And label embedding\n  will be generated by feeding input label feature to label embedding layer.\n  \"\"\"\n  def __init__(self,\n               input_config: input_config_pb2.InputConfig):\n    \"\"\"Initialize LabelEncoder layer based on input config.\n    Args:",
        "detail": "examples.lite.examples.recommendation.ml.model.label_encoder",
        "documentation": {}
    },
    {
        "label": "LabelEncoderTest",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.label_encoder_test",
        "description": "examples.lite.examples.recommendation.ml.model.label_encoder_test",
        "peekOfCode": "class LabelEncoderTest(tf.test.TestCase):\n  def _create_test_input_config(self):\n    \"\"\"Generate test input_config_pb2.InputConfig proto.\"\"\"\n    feature_context_movie_id = input_config_pb2.Feature(\n        feature_name='context_movie_id',\n        feature_type=input_config_pb2.FeatureType.INT,\n        vocab_size=3952,\n        embedding_dim=4)\n    feature_context_movie_rating = input_config_pb2.Feature(\n        feature_name='context_movie_rating',",
        "detail": "examples.lite.examples.recommendation.ml.model.label_encoder_test",
        "documentation": {}
    },
    {
        "label": "BatchSoftmax",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.losses",
        "description": "examples.lite.examples.recommendation.ml.model.losses",
        "peekOfCode": "class BatchSoftmax(tf.keras.losses.Loss):\n  \"\"\"Compute batch softmax over batch similarities.\n  This softmax loss takes in-batch negatives without considering\n  negatives out of batch.\n  \"\"\"\n  def __init__(self, name='batch_softmax', **kwargs):\n    super(BatchSoftmax, self).__init__(name=name, **kwargs)\n  @tf.function\n  def call(self, y_true: tf.Tensor, y_pred: tf.Tensor):\n    \"\"\"Compute in batch softmax loss.",
        "detail": "examples.lite.examples.recommendation.ml.model.losses",
        "documentation": {}
    },
    {
        "label": "GlobalSoftmax",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.losses",
        "description": "examples.lite.examples.recommendation.ml.model.losses",
        "peekOfCode": "class GlobalSoftmax(tf.keras.losses.Loss):\n  \"\"\"Compute softmax over similarities.\n  This loss fuction computes softmax over similarities between context and\n  full vocab label embeddings, considering full label vocab non-label\n  predictions as negatives. This is currently the default loss in the model.\n  \"\"\"\n  def __init__(self, name='global_softmax', **kwargs):\n    super(GlobalSoftmax, self).__init__(name=name, **kwargs)\n  @tf.function\n  def call(self, y_true: tf.Tensor, y_pred: tf.Tensor):",
        "detail": "examples.lite.examples.recommendation.ml.model.losses",
        "documentation": {}
    },
    {
        "label": "KerasLossesTest",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.losses_test",
        "description": "examples.lite.examples.recommendation.ml.model.losses_test",
        "peekOfCode": "class KerasLossesTest(tf.test.TestCase):\n  def test_batch_softmax_loss(self):\n    batch_softmax = losses.BatchSoftmax()\n    true_label = tf.constant([[2], [0], [1]], dtype=tf.int32)\n    logits = tf.constant([\n        [0.8, 0.1, 0.2, 0.3],\n        [0.2, 0.7, 0.1, 0.5],\n        [0.5, 0.4, 0.9, 0.2]\n    ], dtype=tf.float32)\n    self.assertBetween(",
        "detail": "examples.lite.examples.recommendation.ml.model.losses_test",
        "documentation": {}
    },
    {
        "label": "BatchRecall",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.metrics",
        "description": "examples.lite.examples.recommendation.ml.model.metrics",
        "peekOfCode": "class BatchRecall(tf.keras.metrics.Recall):\n  \"\"\"Compute batch recall for top_k.\"\"\"\n  def __init__(self, top_k=1, name='batch_recall'):\n    super().__init__(name=name, top_k=top_k)\n  def update_state(self, y_true, y_pred, sample_weight=None):\n    \"\"\"Update state of the metric.\n    Args:\n      y_true: the true labels with shape [batch_size, 1].\n      y_pred: model output, which is the similarity matrix with shape\n        [batch_size, label_embedding_vocab_size] between context and full vocab",
        "detail": "examples.lite.examples.recommendation.ml.model.metrics",
        "documentation": {}
    },
    {
        "label": "GlobalRecall",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.metrics",
        "description": "examples.lite.examples.recommendation.ml.model.metrics",
        "peekOfCode": "class GlobalRecall(tf.keras.metrics.Recall):\n  \"\"\"Compute global recall for top_k.\"\"\"\n  def __init__(self, top_k=1, name='global_recall'):\n    super().__init__(name=name, top_k=top_k)\n  def update_state(self, y_true, y_pred, sample_weight=None):\n    \"\"\"Update state of the metric.\n    Args:\n      y_true: the true labels with shape [batch_size, 1].\n      y_pred: model output, which is the similarity matrix with shape\n        [batch_size, label_embedding_vocab_size] between context and full vocab",
        "detail": "examples.lite.examples.recommendation.ml.model.metrics",
        "documentation": {}
    },
    {
        "label": "BatchMeanRank",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.metrics",
        "description": "examples.lite.examples.recommendation.ml.model.metrics",
        "peekOfCode": "class BatchMeanRank(tf.keras.metrics.Mean):\n  \"\"\"Keras metric computing mean rank of correct label within batch.\"\"\"\n  def __init__(self, name='batch_mean_rank', **kwargs):\n    super().__init__(name=name, **kwargs)\n  def update_state(self, y_true, y_pred, sample_weight=None):\n    \"\"\"Update state of the metric.\n    Args:\n      y_true: the true labels with shape [batch_size, 1].\n      y_pred: model output, which is the similarity matrix with shape\n        [batch_size, label_embedding_vocab_size] between context and full vocab",
        "detail": "examples.lite.examples.recommendation.ml.model.metrics",
        "documentation": {}
    },
    {
        "label": "GlobalMeanRank",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.metrics",
        "description": "examples.lite.examples.recommendation.ml.model.metrics",
        "peekOfCode": "class GlobalMeanRank(tf.keras.metrics.Mean):\n  \"\"\"Keras metric computing mean rank of correct label globally.\"\"\"\n  def __init__(self, name='global_mean_rank', **kwargs):\n    super().__init__(name=name, **kwargs)\n  def update_state(self, y_true, y_pred, sample_weight=None):\n    \"\"\"Update state of the metric.\n    Args:\n      y_true: the true labels with shape [batch_size, 1].\n      y_pred: model output, which is the similarity matrix with shape\n        [batch_size, label_embedding_vocab_size] between context and full vocab",
        "detail": "examples.lite.examples.recommendation.ml.model.metrics",
        "documentation": {}
    },
    {
        "label": "KerasMetricsTest",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.metrics_test",
        "description": "examples.lite.examples.recommendation.ml.model.metrics_test",
        "peekOfCode": "class KerasMetricsTest(tf.test.TestCase):\n  def test_batch_recall_and_mean_rank(self):\n    batch_recall = metrics.BatchRecall(top_k=2)\n    batch_mean_rank = metrics.BatchMeanRank()\n    true_label = tf.constant([[2], [0], [1]], dtype=tf.int32)\n    logits = tf.constant([\n        [0.8, 0.1, 1.1, 0.3],\n        [0.2, 0.7, 0.1, 0.5],\n        [0.7, 0.4, 0.9, 0.2]\n    ], dtype=tf.float32)",
        "detail": "examples.lite.examples.recommendation.ml.model.metrics_test",
        "documentation": {}
    },
    {
        "label": "RecommendationModel",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model",
        "peekOfCode": "class RecommendationModel(tf.keras.Model):\n  \"\"\"Personalized dual-encoder style recommendation model.\"\"\"\n  def __init__(self,\n               input_config: input_config_pb2.InputConfig,\n               model_config: model_config_class.ModelConfig):\n    \"\"\"Initializes RecommendationModel according to input and model configs.\n    Takes in input and model configs to initialize the recommendation model.\n    Context encoder layer, label encoder layer and dotproduct similarity layer\n    will be prepared.\n    Args:",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model",
        "documentation": {}
    },
    {
        "label": "SimpleCheckpoint",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "class SimpleCheckpoint(tf.keras.callbacks.Callback):\n  \"\"\"Keras callback to save tf.train.Checkpoints.\"\"\"\n  def __init__(self, checkpoint_manager):\n    super(SimpleCheckpoint, self).__init__()\n    self.checkpoint_manager = checkpoint_manager\n  def on_epoch_end(self, epoch, logs=None):\n    step_counter = self.checkpoint_manager._step_counter.numpy()  # pylint: disable=protected-access\n    self.checkpoint_manager.save(checkpoint_number=step_counter)\ndef _get_optimizer(learning_rate: float, gradient_clip_norm: float):\n  \"\"\"Gets model optimizer.\"\"\"",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string('training_data_filepattern', None,\n                      'File pattern of the training data.')\n  flags.DEFINE_string('testing_data_filepattern', None,\n                      'File pattern of the training data.')\n  flags.DEFINE_string('model_dir', None, 'Directory to store checkpoints.')\n  flags.DEFINE_string('export_dir', None, 'Directory for the exported model.')\n  flags.DEFINE_integer('batch_size', 1, 'Training batch size.')\n  flags.DEFINE_float('learning_rate', 0.1, 'Learning rate.')",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "compile_model",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def compile_model(model, eval_top_k, learning_rate, gradient_clip_norm):\n  \"\"\"Compile keras model.\"\"\"\n  model.compile(\n      optimizer=_get_optimizer(\n          learning_rate=learning_rate, gradient_clip_norm=gradient_clip_norm),\n      loss=losses.GlobalSoftmax(),\n      metrics=_get_metrics(eval_top_k))\ndef build_keras_model(input_config: input_config_pb2.InputConfig,\n                      model_config: model_config_class.ModelConfig):\n  \"\"\"Construct and compile recommendation keras model.",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "build_keras_model",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def build_keras_model(input_config: input_config_pb2.InputConfig,\n                      model_config: model_config_class.ModelConfig):\n  \"\"\"Construct and compile recommendation keras model.\n  Construct recommendation model according to input config and model config.\n  Compile the model with optimizer, loss function and eval metrics.\n  Args:\n    input_config: The configuration object(input_config_pb2.InputConfig) that\n      holds parameters for model input feature processing.\n    model_config: A ModelConfig object that holds parameters to set up the\n      model architecture.",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "get_callbacks",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def get_callbacks(keras_model: tf.keras.Model,\n                  model_dir: str):\n  \"\"\"Sets up callbacks for training and evaluation.\"\"\"\n  summary_dir = os.path.join(model_dir, 'summaries')\n  summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)\n  checkpoint = tf.train.Checkpoint(\n      model=keras_model, optimizer=keras_model.optimizer)\n  checkpoint_manager = tf.train.CheckpointManager(\n      checkpoint,\n      directory=model_dir,",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "train_and_eval",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def train_and_eval(model: tf.keras.Model,\n                   model_dir: str,\n                   train_input_dataset: tf.data.Dataset,\n                   eval_input_dataset: tf.data.Dataset,\n                   steps_per_epoch: int,\n                   epochs: int,\n                   eval_steps: int):\n  \"\"\"Train and evaluate.\"\"\"\n  callbacks = get_callbacks(model, model_dir)\n  history = model.fit(",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "save_model",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def save_model(checkpoint_path: str, export_dir: str,\n               input_config: input_config_pb2.InputConfig,\n               model_config: model_config_class.ModelConfig):\n  \"\"\"Export to savedmodel.\n  Args:\n    checkpoint_path: The path to the checkpoint that the model will be exported\n      based on.\n    export_dir: The directory to export models to.\n    input_config: The input config of the model.\n    model_config: The configuration to set up the model.",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "export_tflite",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def export_tflite(export_dir):\n  \"\"\"Export to TFLite model.\n  Args:\n    export_dir: the model exportation dir, where saved_model is located.\n  \"\"\"\n  converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n  tflite_model = converter.convert()\n  tflite_model_path = os.path.join(export_dir, 'model.tflite')\n  with tf.io.gfile.GFile(tflite_model_path, 'wb') as f:\n    f.write(tflite_model)",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "export",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def export(checkpoint_path: str, input_config: input_config_pb2.InputConfig,\n           model_config: model_config_class.ModelConfig, export_dir: str):\n  \"\"\"Export to tensorflow saved model and TFLite model.\n  Args:\n    checkpoint_path: The path to the checkpoint that the model will be exported\n      based on.\n    input_config: The input config of the model.\n    model_config: The configuration to set up the model.\n    export_dir: The directory to store the exported model, If not set, model is\n      exported to the model_dir with timestamp.",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "load_input_config",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def load_input_config():\n  \"\"\"Load input config.\"\"\"\n  assert FLAGS.input_config_file, 'input_config_file cannot be empty.'\n  with tf.io.gfile.GFile(FLAGS.input_config_file, 'rb') as reader:\n    return text_format.Parse(reader.read(), input_config_pb2.InputConfig())\ndef prepare_model_config():\n  \"\"\"Prepare model config.\"\"\"\n  return model_config_class.ModelConfig(\n      hidden_layer_dims=[int(x) for x in FLAGS.hidden_layer_dims],\n      eval_top_k=[int(x) for x in FLAGS.eval_top_k],",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "prepare_model_config",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def prepare_model_config():\n  \"\"\"Prepare model config.\"\"\"\n  return model_config_class.ModelConfig(\n      hidden_layer_dims=[int(x) for x in FLAGS.hidden_layer_dims],\n      eval_top_k=[int(x) for x in FLAGS.eval_top_k],\n      conv_num_filter_ratios=[int(x) for x in FLAGS.conv_num_filter_ratios],\n      conv_kernel_size=FLAGS.conv_kernel_size,\n      lstm_num_units=FLAGS.lstm_num_units,\n      num_predictions=FLAGS.num_predictions)\ndef main(_):",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def main(_):\n  logger = tf.get_logger()\n  if not tf.io.gfile.exists(FLAGS.model_dir):\n    tf.io.gfile.mkdir(FLAGS.model_dir)\n  if not tf.io.gfile.exists(FLAGS.export_dir):\n    tf.io.gfile.mkdir(FLAGS.export_dir)\n  input_config = load_input_config()\n  model_config = prepare_model_config()\n  logger.info('Setting up train and eval input datasets.')\n  train_input_dataset = input_pipeline.get_input_dataset(",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string('training_data_filepattern', None,\n                      'File pattern of the training data.')\n  flags.DEFINE_string('testing_data_filepattern', None,\n                      'File pattern of the training data.')\n  flags.DEFINE_string('model_dir', None, 'Directory to store checkpoints.')\n  flags.DEFINE_string('export_dir', None, 'Directory for the exported model.')\n  flags.DEFINE_integer('batch_size', 1, 'Training batch size.')",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "RecommendationModelLauncherTest",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "peekOfCode": "class RecommendationModelLauncherTest(tf.test.TestCase):\n  def _AssertSparseTensorValueEqual(self, a, b):\n    self.assertAllEqual(a.indices, b.indices)\n    self.assertAllEqual(a.values, b.values)\n    self.assertAllEqual(a.dense_shape, b.dense_shape)\n  def _assertInputDetail(self, input_details, index, name, shape):\n    self.assertEqual(name, input_details[index]['name'])\n    self.assertEqual(shape, input_details[index]['shape'])\n  def setUp(self):\n    super().setUp()",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "peekOfCode": "FLAGS = flags.FLAGS\nFAKE_MOVIE_GENRE_VOCAB = [\n    'UNK',\n    'Comedy',\n    'Drama',\n    'Romance',\n    'Animation',\n    'Children'\n]\nTEST_INPUT_CONFIG = \"\"\"",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "documentation": {}
    },
    {
        "label": "FAKE_MOVIE_GENRE_VOCAB",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "peekOfCode": "FAKE_MOVIE_GENRE_VOCAB = [\n    'UNK',\n    'Comedy',\n    'Drama',\n    'Romance',\n    'Animation',\n    'Children'\n]\nTEST_INPUT_CONFIG = \"\"\"\n    activity_feature_groups {",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "documentation": {}
    },
    {
        "label": "TEST_INPUT_CONFIG",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "peekOfCode": "TEST_INPUT_CONFIG = \"\"\"\n    activity_feature_groups {\n      features {\n        feature_name: \"context_movie_id\"\n        feature_type: INT\n        vocab_size: 3952\n        embedding_dim: 8\n        feature_length: 5\n      }\n      features {",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "documentation": {}
    },
    {
        "label": "EXAMPLE1",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "peekOfCode": "EXAMPLE1 = text_format.Parse(\n    \"\"\"\n    features {\n        feature {\n          key: \"context_movie_id\"\n          value {\n            int64_list {\n              value: [1, 2, 0, 0, 0]\n            }\n          }",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_launcher_test",
        "documentation": {}
    },
    {
        "label": "RecommendationModelTest",
        "kind": 6,
        "importPath": "examples.lite.examples.recommendation.ml.model.recommendation_model_test",
        "description": "examples.lite.examples.recommendation.ml.model.recommendation_model_test",
        "peekOfCode": "class RecommendationModelTest(tf.test.TestCase):\n  def _create_test_input_config(self,\n                                encoder_type: input_config_pb2.EncoderType):\n    \"\"\"Generate test input_config_pb2.InputConfig proto.\"\"\"\n    feature_context_movie_id = input_config_pb2.Feature(\n        feature_name='context_movie_id',\n        feature_type=input_config_pb2.FeatureType.INT,\n        vocab_size=20,\n        embedding_dim=4)\n    feature_context_movie_rating = input_config_pb2.Feature(",
        "detail": "examples.lite.examples.recommendation.ml.model.recommendation_model_test",
        "documentation": {}
    },
    {
        "label": "GetShardFilenames",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.utils",
        "description": "examples.lite.examples.recommendation.ml.model.utils",
        "peekOfCode": "def GetShardFilenames(filepattern):\n  \"\"\"Get a list of filenames given a pattern.\n  This will also check whether the files exist on the filesystem. The pattern\n  can be either of the glob form, or the 'basename@num_shards' form.\n  Args:\n    filepattern: File pattern.\n  Returns:\n    A list of shard patterns.\n  Raises:\n    ValueError: if using the shard pattern, if some shards don't exist.",
        "detail": "examples.lite.examples.recommendation.ml.model.utils",
        "documentation": {}
    },
    {
        "label": "ClipGradient",
        "kind": 2,
        "importPath": "examples.lite.examples.recommendation.ml.model.utils",
        "description": "examples.lite.examples.recommendation.ml.model.utils",
        "peekOfCode": "def ClipGradient(\n    grads_and_vars: Iterable[Tuple[TFGradient, tf.Variable]],\n    clip_val: Scalar = 1.0,\n    include_histogram_summary: bool = False\n) -> Tuple[Tuple[TFGradient, tf.Variable], ...]:\n  \"\"\"Clips all gradients by global norm, reducing norm to clip_val.\n  Args:\n    grads_and_vars: Gradients and vars list input.\n    clip_val: A 0-D (scalar) `Tensor` > 0. Value to clip to.\n    include_histogram_summary: Flag indicates adding clipped gradients to",
        "detail": "examples.lite.examples.recommendation.ml.model.utils",
        "documentation": {}
    },
    {
        "label": "TFGradient",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.utils",
        "description": "examples.lite.examples.recommendation.ml.model.utils",
        "peekOfCode": "TFGradient = TypeVar('TFGradient', tf.Tensor, tf.IndexedSlices)\nScalar = TypeVar('Scalar', tf.Variable, tf.Tensor, float, int)\ndef GetShardFilenames(filepattern):\n  \"\"\"Get a list of filenames given a pattern.\n  This will also check whether the files exist on the filesystem. The pattern\n  can be either of the glob form, or the 'basename@num_shards' form.\n  Args:\n    filepattern: File pattern.\n  Returns:\n    A list of shard patterns.",
        "detail": "examples.lite.examples.recommendation.ml.model.utils",
        "documentation": {}
    },
    {
        "label": "Scalar",
        "kind": 5,
        "importPath": "examples.lite.examples.recommendation.ml.model.utils",
        "description": "examples.lite.examples.recommendation.ml.model.utils",
        "peekOfCode": "Scalar = TypeVar('Scalar', tf.Variable, tf.Tensor, float, int)\ndef GetShardFilenames(filepattern):\n  \"\"\"Get a list of filenames given a pattern.\n  This will also check whether the files exist on the filesystem. The pattern\n  can be either of the glob form, or the 'basename@num_shards' form.\n  Args:\n    filepattern: File pattern.\n  Returns:\n    A list of shard patterns.\n  Raises:",
        "detail": "examples.lite.examples.recommendation.ml.model.utils",
        "documentation": {}
    },
    {
        "label": "PlaneStrikePyEnvironment",
        "kind": 6,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "class PlaneStrikePyEnvironment(py_environment.PyEnvironment):\n  \"\"\"PlaneStrike environment for TF Agents.\"\"\"\n  def __init__(self,\n               board_size=BOARD_SIZE,\n               discount=0.9,\n               max_steps=MAX_STEPS_PER_EPISODE) -> None:\n    super(PlaneStrikePyEnvironment, self).__init__()\n    assert board_size >= 4\n    self._board_size = board_size\n    self._strike_count = 0",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "BOARD_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "BOARD_SIZE = 8\nMAX_STEPS_PER_EPISODE = BOARD_SIZE**2\n# Plane direction\nPLANE_HEADING_RIGHT = 0\nPLANE_HEADING_UP = 1\nPLANE_HEADING_LEFT = 2\nPLANE_HEADING_DOWN = 3\n# Rewards for each strike\nHIT_REWARD = 1\nMISS_REWARD = 0",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "MAX_STEPS_PER_EPISODE",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "MAX_STEPS_PER_EPISODE = BOARD_SIZE**2\n# Plane direction\nPLANE_HEADING_RIGHT = 0\nPLANE_HEADING_UP = 1\nPLANE_HEADING_LEFT = 2\nPLANE_HEADING_DOWN = 3\n# Rewards for each strike\nHIT_REWARD = 1\nMISS_REWARD = 0\nREPEAT_STRIKE_REWARD = -1",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "PLANE_HEADING_RIGHT",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "PLANE_HEADING_RIGHT = 0\nPLANE_HEADING_UP = 1\nPLANE_HEADING_LEFT = 2\nPLANE_HEADING_DOWN = 3\n# Rewards for each strike\nHIT_REWARD = 1\nMISS_REWARD = 0\nREPEAT_STRIKE_REWARD = -1\n# Reward for finishing the game within MAX_STEPS_PER_EPISODE\nFINISHED_GAME_REWARD = 10",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "PLANE_HEADING_UP",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "PLANE_HEADING_UP = 1\nPLANE_HEADING_LEFT = 2\nPLANE_HEADING_DOWN = 3\n# Rewards for each strike\nHIT_REWARD = 1\nMISS_REWARD = 0\nREPEAT_STRIKE_REWARD = -1\n# Reward for finishing the game within MAX_STEPS_PER_EPISODE\nFINISHED_GAME_REWARD = 10\n# Reward for not finishing the game within MAX_STEPS_PER_EPISODE",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "PLANE_HEADING_LEFT",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "PLANE_HEADING_LEFT = 2\nPLANE_HEADING_DOWN = 3\n# Rewards for each strike\nHIT_REWARD = 1\nMISS_REWARD = 0\nREPEAT_STRIKE_REWARD = -1\n# Reward for finishing the game within MAX_STEPS_PER_EPISODE\nFINISHED_GAME_REWARD = 10\n# Reward for not finishing the game within MAX_STEPS_PER_EPISODE\nUNFINISHED_GAME_REWARD = -10",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "PLANE_HEADING_DOWN",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "PLANE_HEADING_DOWN = 3\n# Rewards for each strike\nHIT_REWARD = 1\nMISS_REWARD = 0\nREPEAT_STRIKE_REWARD = -1\n# Reward for finishing the game within MAX_STEPS_PER_EPISODE\nFINISHED_GAME_REWARD = 10\n# Reward for not finishing the game within MAX_STEPS_PER_EPISODE\nUNFINISHED_GAME_REWARD = -10\n# Hidden board cell status; 'occupied' means it's part of the plane",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "HIT_REWARD",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "HIT_REWARD = 1\nMISS_REWARD = 0\nREPEAT_STRIKE_REWARD = -1\n# Reward for finishing the game within MAX_STEPS_PER_EPISODE\nFINISHED_GAME_REWARD = 10\n# Reward for not finishing the game within MAX_STEPS_PER_EPISODE\nUNFINISHED_GAME_REWARD = -10\n# Hidden board cell status; 'occupied' means it's part of the plane\nHIDDEN_BOARD_CELL_OCCUPIED = 1\nHIDDEN_BOARD_CELL_UNOCCUPIED = 0",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "MISS_REWARD",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "MISS_REWARD = 0\nREPEAT_STRIKE_REWARD = -1\n# Reward for finishing the game within MAX_STEPS_PER_EPISODE\nFINISHED_GAME_REWARD = 10\n# Reward for not finishing the game within MAX_STEPS_PER_EPISODE\nUNFINISHED_GAME_REWARD = -10\n# Hidden board cell status; 'occupied' means it's part of the plane\nHIDDEN_BOARD_CELL_OCCUPIED = 1\nHIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "REPEAT_STRIKE_REWARD",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "REPEAT_STRIKE_REWARD = -1\n# Reward for finishing the game within MAX_STEPS_PER_EPISODE\nFINISHED_GAME_REWARD = 10\n# Reward for not finishing the game within MAX_STEPS_PER_EPISODE\nUNFINISHED_GAME_REWARD = -10\n# Hidden board cell status; 'occupied' means it's part of the plane\nHIDDEN_BOARD_CELL_OCCUPIED = 1\nHIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status\nVISIBLE_BOARD_CELL_HIT = 1",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "FINISHED_GAME_REWARD",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "FINISHED_GAME_REWARD = 10\n# Reward for not finishing the game within MAX_STEPS_PER_EPISODE\nUNFINISHED_GAME_REWARD = -10\n# Hidden board cell status; 'occupied' means it's part of the plane\nHIDDEN_BOARD_CELL_OCCUPIED = 1\nHIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status\nVISIBLE_BOARD_CELL_HIT = 1\nVISIBLE_BOARD_CELL_MISS = -1\nVISIBLE_BOARD_CELL_UNTRIED = 0",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "UNFINISHED_GAME_REWARD",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "UNFINISHED_GAME_REWARD = -10\n# Hidden board cell status; 'occupied' means it's part of the plane\nHIDDEN_BOARD_CELL_OCCUPIED = 1\nHIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status\nVISIBLE_BOARD_CELL_HIT = 1\nVISIBLE_BOARD_CELL_MISS = -1\nVISIBLE_BOARD_CELL_UNTRIED = 0\nclass PlaneStrikePyEnvironment(py_environment.PyEnvironment):\n  \"\"\"PlaneStrike environment for TF Agents.\"\"\"",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "HIDDEN_BOARD_CELL_OCCUPIED",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "HIDDEN_BOARD_CELL_OCCUPIED = 1\nHIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status\nVISIBLE_BOARD_CELL_HIT = 1\nVISIBLE_BOARD_CELL_MISS = -1\nVISIBLE_BOARD_CELL_UNTRIED = 0\nclass PlaneStrikePyEnvironment(py_environment.PyEnvironment):\n  \"\"\"PlaneStrike environment for TF Agents.\"\"\"\n  def __init__(self,\n               board_size=BOARD_SIZE,",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "HIDDEN_BOARD_CELL_UNOCCUPIED",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "HIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status\nVISIBLE_BOARD_CELL_HIT = 1\nVISIBLE_BOARD_CELL_MISS = -1\nVISIBLE_BOARD_CELL_UNTRIED = 0\nclass PlaneStrikePyEnvironment(py_environment.PyEnvironment):\n  \"\"\"PlaneStrike environment for TF Agents.\"\"\"\n  def __init__(self,\n               board_size=BOARD_SIZE,\n               discount=0.9,",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "VISIBLE_BOARD_CELL_HIT",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "VISIBLE_BOARD_CELL_HIT = 1\nVISIBLE_BOARD_CELL_MISS = -1\nVISIBLE_BOARD_CELL_UNTRIED = 0\nclass PlaneStrikePyEnvironment(py_environment.PyEnvironment):\n  \"\"\"PlaneStrike environment for TF Agents.\"\"\"\n  def __init__(self,\n               board_size=BOARD_SIZE,\n               discount=0.9,\n               max_steps=MAX_STEPS_PER_EPISODE) -> None:\n    super(PlaneStrikePyEnvironment, self).__init__()",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "VISIBLE_BOARD_CELL_MISS",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "VISIBLE_BOARD_CELL_MISS = -1\nVISIBLE_BOARD_CELL_UNTRIED = 0\nclass PlaneStrikePyEnvironment(py_environment.PyEnvironment):\n  \"\"\"PlaneStrike environment for TF Agents.\"\"\"\n  def __init__(self,\n               board_size=BOARD_SIZE,\n               discount=0.9,\n               max_steps=MAX_STEPS_PER_EPISODE) -> None:\n    super(PlaneStrikePyEnvironment, self).__init__()\n    assert board_size >= 4",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "VISIBLE_BOARD_CELL_UNTRIED",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "peekOfCode": "VISIBLE_BOARD_CELL_UNTRIED = 0\nclass PlaneStrikePyEnvironment(py_environment.PyEnvironment):\n  \"\"\"PlaneStrike environment for TF Agents.\"\"\"\n  def __init__(self,\n               board_size=BOARD_SIZE,\n               discount=0.9,\n               max_steps=MAX_STEPS_PER_EPISODE) -> None:\n    super(PlaneStrikePyEnvironment, self).__init__()\n    assert board_size >= 4\n    self._board_size = board_size",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.planestrike_py_environment",
        "documentation": {}
    },
    {
        "label": "compute_avg_return_and_steps",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "def compute_avg_return_and_steps(environment, policy, num_episodes=10):\n  \"\"\"Compute average return and # of steps.\"\"\"\n  total_return = 0.0\n  total_steps = 0.0\n  for _ in range(num_episodes):\n    time_step = environment.reset()\n    episode_return = 0.0\n    episode_steps = 0.0\n    while not time_step.is_last():\n      action_step = policy.action(time_step)",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "collect_episode",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "def collect_episode(environment, policy, num_episodes, replay_buffer_observer):\n  \"\"\"Collect game episode trajectories.\"\"\"\n  initial_time_step = environment.reset()\n  driver = py_driver.PyDriver(\n      environment,\n      py_tf_eager_policy.PyTFEagerPolicy(policy, use_tf_function=True),\n      [replay_buffer_observer],\n      max_episodes=num_episodes)\n  initial_time_step = environment.reset()\n  driver.run(initial_time_step)",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "train_agent",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "def train_agent(iterations, modeldir, logdir, policydir):\n  \"\"\"Train and convert the model using TF Agents.\"\"\"\n  train_py_env = planestrike_py_environment.PlaneStrikePyEnvironment(\n      board_size=BOARD_SIZE, discount=DISCOUNT, max_steps=BOARD_SIZE**2)\n  eval_py_env = planestrike_py_environment.PlaneStrikePyEnvironment(\n      board_size=BOARD_SIZE, discount=DISCOUNT, max_steps=BOARD_SIZE**2)\n  train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n  eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n  # Alternatively you could use ActorDistributionNetwork as actor_net\n  actor_net = tfa.networks.Sequential([",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "def main(argv: Sequence[str]) -> None:\n  if len(argv) > 1:\n    raise app.UsageError('Too many command-line arguments.')\n  train_agent(ITERATIONS, MODELDIR, LOGDIR, POLICYDIR)\nif __name__ == '__main__':\n  app.run(main)",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "BOARD_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "BOARD_SIZE = 8\nITERATIONS = 250000\nCOLLECT_EPISODES_PER_ITERATION = 1\nREPLAY_BUFFER_CAPACITY = 2000\nREPLAY_BUFFER_TABLE_NAME = 'uniform_table'\nDISCOUNT = 0.5\nFC_LAYER_PARAMS = 100\nLEARNING_RATE = 1e-3\nNUM_EVAL_EPISODES = 20\nEVAL_INTERVAL = 500",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "ITERATIONS",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "ITERATIONS = 250000\nCOLLECT_EPISODES_PER_ITERATION = 1\nREPLAY_BUFFER_CAPACITY = 2000\nREPLAY_BUFFER_TABLE_NAME = 'uniform_table'\nDISCOUNT = 0.5\nFC_LAYER_PARAMS = 100\nLEARNING_RATE = 1e-3\nNUM_EVAL_EPISODES = 20\nEVAL_INTERVAL = 500\nLOGDIR = './tf_agents_log'",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "COLLECT_EPISODES_PER_ITERATION",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "COLLECT_EPISODES_PER_ITERATION = 1\nREPLAY_BUFFER_CAPACITY = 2000\nREPLAY_BUFFER_TABLE_NAME = 'uniform_table'\nDISCOUNT = 0.5\nFC_LAYER_PARAMS = 100\nLEARNING_RATE = 1e-3\nNUM_EVAL_EPISODES = 20\nEVAL_INTERVAL = 500\nLOGDIR = './tf_agents_log'\nMODELDIR = './'",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "REPLAY_BUFFER_CAPACITY",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "REPLAY_BUFFER_CAPACITY = 2000\nREPLAY_BUFFER_TABLE_NAME = 'uniform_table'\nDISCOUNT = 0.5\nFC_LAYER_PARAMS = 100\nLEARNING_RATE = 1e-3\nNUM_EVAL_EPISODES = 20\nEVAL_INTERVAL = 500\nLOGDIR = './tf_agents_log'\nMODELDIR = './'\nPOLICYDIR = './policy'",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "REPLAY_BUFFER_TABLE_NAME",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "REPLAY_BUFFER_TABLE_NAME = 'uniform_table'\nDISCOUNT = 0.5\nFC_LAYER_PARAMS = 100\nLEARNING_RATE = 1e-3\nNUM_EVAL_EPISODES = 20\nEVAL_INTERVAL = 500\nLOGDIR = './tf_agents_log'\nMODELDIR = './'\nPOLICYDIR = './policy'\ndef compute_avg_return_and_steps(environment, policy, num_episodes=10):",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "DISCOUNT",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "DISCOUNT = 0.5\nFC_LAYER_PARAMS = 100\nLEARNING_RATE = 1e-3\nNUM_EVAL_EPISODES = 20\nEVAL_INTERVAL = 500\nLOGDIR = './tf_agents_log'\nMODELDIR = './'\nPOLICYDIR = './policy'\ndef compute_avg_return_and_steps(environment, policy, num_episodes=10):\n  \"\"\"Compute average return and # of steps.\"\"\"",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "FC_LAYER_PARAMS",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "FC_LAYER_PARAMS = 100\nLEARNING_RATE = 1e-3\nNUM_EVAL_EPISODES = 20\nEVAL_INTERVAL = 500\nLOGDIR = './tf_agents_log'\nMODELDIR = './'\nPOLICYDIR = './policy'\ndef compute_avg_return_and_steps(environment, policy, num_episodes=10):\n  \"\"\"Compute average return and # of steps.\"\"\"\n  total_return = 0.0",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "LEARNING_RATE",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "LEARNING_RATE = 1e-3\nNUM_EVAL_EPISODES = 20\nEVAL_INTERVAL = 500\nLOGDIR = './tf_agents_log'\nMODELDIR = './'\nPOLICYDIR = './policy'\ndef compute_avg_return_and_steps(environment, policy, num_episodes=10):\n  \"\"\"Compute average return and # of steps.\"\"\"\n  total_return = 0.0\n  total_steps = 0.0",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "NUM_EVAL_EPISODES",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "NUM_EVAL_EPISODES = 20\nEVAL_INTERVAL = 500\nLOGDIR = './tf_agents_log'\nMODELDIR = './'\nPOLICYDIR = './policy'\ndef compute_avg_return_and_steps(environment, policy, num_episodes=10):\n  \"\"\"Compute average return and # of steps.\"\"\"\n  total_return = 0.0\n  total_steps = 0.0\n  for _ in range(num_episodes):",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "EVAL_INTERVAL",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "EVAL_INTERVAL = 500\nLOGDIR = './tf_agents_log'\nMODELDIR = './'\nPOLICYDIR = './policy'\ndef compute_avg_return_and_steps(environment, policy, num_episodes=10):\n  \"\"\"Compute average return and # of steps.\"\"\"\n  total_return = 0.0\n  total_steps = 0.0\n  for _ in range(num_episodes):\n    time_step = environment.reset()",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "LOGDIR",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "LOGDIR = './tf_agents_log'\nMODELDIR = './'\nPOLICYDIR = './policy'\ndef compute_avg_return_and_steps(environment, policy, num_episodes=10):\n  \"\"\"Compute average return and # of steps.\"\"\"\n  total_return = 0.0\n  total_steps = 0.0\n  for _ in range(num_episodes):\n    time_step = environment.reset()\n    episode_return = 0.0",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "MODELDIR",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "MODELDIR = './'\nPOLICYDIR = './policy'\ndef compute_avg_return_and_steps(environment, policy, num_episodes=10):\n  \"\"\"Compute average return and # of steps.\"\"\"\n  total_return = 0.0\n  total_steps = 0.0\n  for _ in range(num_episodes):\n    time_step = environment.reset()\n    episode_return = 0.0\n    episode_steps = 0.0",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "POLICYDIR",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "peekOfCode": "POLICYDIR = './policy'\ndef compute_avg_return_and_steps(environment, policy, num_episodes=10):\n  \"\"\"Compute average return and # of steps.\"\"\"\n  total_return = 0.0\n  total_steps = 0.0\n  for _ in range(num_episodes):\n    time_step = environment.reset()\n    episode_return = 0.0\n    episode_steps = 0.0\n    while not time_step.is_last():",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_agents.training_tf_agents",
        "documentation": {}
    },
    {
        "label": "PlaneStrikeEnv",
        "kind": 6,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.gym_planestrike.envs.planestrike",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.gym_planestrike.envs.planestrike",
        "peekOfCode": "class PlaneStrikeEnv(gym.Env):\n  \"\"\"A class that defines the Plane Strike environement.\"\"\"\n  metadata = {'render.modes': ['human']}\n  def __init__(self, board_size) -> None:\n    super().__init__()\n    assert board_size >= 4\n    self.board_size = board_size\n    self.set_board()\n  def step(self, action):\n    if self.hit_count == self.plane_size:",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.gym_planestrike.envs.planestrike",
        "documentation": {}
    },
    {
        "label": "HIT_REWARD",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.gym_planestrike.envs.planestrike",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.gym_planestrike.envs.planestrike",
        "peekOfCode": "HIT_REWARD = 1\nMISS_REWARD = 0\nREPEAT_STRIKE_REWARD = -1\nclass PlaneStrikeEnv(gym.Env):\n  \"\"\"A class that defines the Plane Strike environement.\"\"\"\n  metadata = {'render.modes': ['human']}\n  def __init__(self, board_size) -> None:\n    super().__init__()\n    assert board_size >= 4\n    self.board_size = board_size",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.gym_planestrike.envs.planestrike",
        "documentation": {}
    },
    {
        "label": "MISS_REWARD",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.gym_planestrike.envs.planestrike",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.gym_planestrike.envs.planestrike",
        "peekOfCode": "MISS_REWARD = 0\nREPEAT_STRIKE_REWARD = -1\nclass PlaneStrikeEnv(gym.Env):\n  \"\"\"A class that defines the Plane Strike environement.\"\"\"\n  metadata = {'render.modes': ['human']}\n  def __init__(self, board_size) -> None:\n    super().__init__()\n    assert board_size >= 4\n    self.board_size = board_size\n    self.set_board()",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.gym_planestrike.envs.planestrike",
        "documentation": {}
    },
    {
        "label": "REPEAT_STRIKE_REWARD",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.gym_planestrike.envs.planestrike",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.gym_planestrike.envs.planestrike",
        "peekOfCode": "REPEAT_STRIKE_REWARD = -1\nclass PlaneStrikeEnv(gym.Env):\n  \"\"\"A class that defines the Plane Strike environement.\"\"\"\n  metadata = {'render.modes': ['human']}\n  def __init__(self, board_size) -> None:\n    super().__init__()\n    assert board_size >= 4\n    self.board_size = board_size\n    self.set_board()\n  def step(self, action):",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.gym_planestrike.envs.planestrike",
        "documentation": {}
    },
    {
        "label": "source",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.setup",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.setup",
        "peekOfCode": "source = os.path.abspath(\n    os.path.join(os.path.dirname(__file__), '../../common.py'))\ntarget = os.path.abspath(\n    os.path.join(os.path.dirname(__file__), 'gym_planestrike/envs/common.py'))\nshutil.copyfile(source, target)\nsetup(\n    name='gym_planestrike',\n    version='0.1',\n    description='Board game Plane Strike',\n    author='TensorFlow Authors',",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.setup",
        "documentation": {}
    },
    {
        "label": "target",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.setup",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.setup",
        "peekOfCode": "target = os.path.abspath(\n    os.path.join(os.path.dirname(__file__), 'gym_planestrike/envs/common.py'))\nshutil.copyfile(source, target)\nsetup(\n    name='gym_planestrike',\n    version='0.1',\n    description='Board game Plane Strike',\n    author='TensorFlow Authors',\n    license='Apache License 2.0',\n    packages=find_packages(),",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.gym_planestrike.setup",
        "documentation": {}
    },
    {
        "label": "PolicyGradient",
        "kind": 6,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "peekOfCode": "class PolicyGradient(nn.Module):\n  \"\"\"Neural network to predict the next strike position.\"\"\"\n  @nn.compact\n  def __call__(self, x):\n    dtype = jnp.float32\n    x = x.reshape((x.shape[0], -1))\n    x = nn.Dense(\n        features=2 * common.BOARD_SIZE**2, name='hidden1', dtype=dtype)(\n            x)\n    x = nn.relu(x)",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "documentation": {}
    },
    {
        "label": "create_optimizer",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "peekOfCode": "def create_optimizer(learning_rate: float):\n  return optax.sgd(learning_rate=learning_rate)\ndef compute_loss(logits, labels, rewards):\n  one_hot_labels = jax.nn.one_hot(labels, num_classes=common.BOARD_SIZE**2)\n  loss = -jnp.mean(\n      jnp.sum(one_hot_labels * jnp.log(logits), axis=-1) * jnp.asarray(rewards))\n  return loss\ndef train_step(model_optimizer, params, opt_state, game_board_log,\n               predicted_action_log, action_result_log):\n  \"\"\"Run one training step.\"\"\"",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "documentation": {}
    },
    {
        "label": "compute_loss",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "peekOfCode": "def compute_loss(logits, labels, rewards):\n  one_hot_labels = jax.nn.one_hot(labels, num_classes=common.BOARD_SIZE**2)\n  loss = -jnp.mean(\n      jnp.sum(one_hot_labels * jnp.log(logits), axis=-1) * jnp.asarray(rewards))\n  return loss\ndef train_step(model_optimizer, params, opt_state, game_board_log,\n               predicted_action_log, action_result_log):\n  \"\"\"Run one training step.\"\"\"\n  def loss_fn(model_params):\n    logits = run_inference(model_params, game_board_log)",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "documentation": {}
    },
    {
        "label": "train_step",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "peekOfCode": "def train_step(model_optimizer, params, opt_state, game_board_log,\n               predicted_action_log, action_result_log):\n  \"\"\"Run one training step.\"\"\"\n  def loss_fn(model_params):\n    logits = run_inference(model_params, game_board_log)\n    loss = compute_loss(logits, predicted_action_log, action_result_log)\n    return loss\n  def compute_grads(params):\n    return jax.grad(loss_fn)(params)\n  grads = compute_grads(params)",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "documentation": {}
    },
    {
        "label": "run_inference",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "peekOfCode": "def run_inference(model_params, board):\n  logits = PolicyGradient().apply({'params': model_params}, board)\n  return logits\ndef train_agent(iterations, modeldir, logdir):\n  \"\"\"Train and convert the model.\"\"\"\n  summary_writer = tensorboard.SummaryWriter(logdir)\n  rng = random.PRNGKey(0)\n  rng, init_rng = random.split(rng)\n  policygradient = PolicyGradient()\n  params = policygradient.init(",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "documentation": {}
    },
    {
        "label": "train_agent",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "peekOfCode": "def train_agent(iterations, modeldir, logdir):\n  \"\"\"Train and convert the model.\"\"\"\n  summary_writer = tensorboard.SummaryWriter(logdir)\n  rng = random.PRNGKey(0)\n  rng, init_rng = random.split(rng)\n  policygradient = PolicyGradient()\n  params = policygradient.init(\n      init_rng, jnp.ones([1, common.BOARD_SIZE, common.BOARD_SIZE]))['params']\n  optimizer = create_optimizer(learning_rate=LEARNING_RATE)\n  opt_state = optimizer.init(params)",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "peekOfCode": "def main(argv: Sequence[str]) -> None:\n  if len(argv) > 1:\n    raise app.UsageError('Too many command-line arguments.')\n  train_agent(ITERATIONS, MODELDIR, LOGDIR)\nif __name__ == '__main__':\n  app.run(main)",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "documentation": {}
    },
    {
        "label": "ITERATIONS",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "peekOfCode": "ITERATIONS = 500000\nLEARNING_RATE = 0.005\nMODELDIR = './'\nLOGDIR = './jax_log'\nclass PolicyGradient(nn.Module):\n  \"\"\"Neural network to predict the next strike position.\"\"\"\n  @nn.compact\n  def __call__(self, x):\n    dtype = jnp.float32\n    x = x.reshape((x.shape[0], -1))",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "documentation": {}
    },
    {
        "label": "LEARNING_RATE",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "peekOfCode": "LEARNING_RATE = 0.005\nMODELDIR = './'\nLOGDIR = './jax_log'\nclass PolicyGradient(nn.Module):\n  \"\"\"Neural network to predict the next strike position.\"\"\"\n  @nn.compact\n  def __call__(self, x):\n    dtype = jnp.float32\n    x = x.reshape((x.shape[0], -1))\n    x = nn.Dense(",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "documentation": {}
    },
    {
        "label": "MODELDIR",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "peekOfCode": "MODELDIR = './'\nLOGDIR = './jax_log'\nclass PolicyGradient(nn.Module):\n  \"\"\"Neural network to predict the next strike position.\"\"\"\n  @nn.compact\n  def __call__(self, x):\n    dtype = jnp.float32\n    x = x.reshape((x.shape[0], -1))\n    x = nn.Dense(\n        features=2 * common.BOARD_SIZE**2, name='hidden1', dtype=dtype)(",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "documentation": {}
    },
    {
        "label": "LOGDIR",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "peekOfCode": "LOGDIR = './jax_log'\nclass PolicyGradient(nn.Module):\n  \"\"\"Neural network to predict the next strike position.\"\"\"\n  @nn.compact\n  def __call__(self, x):\n    dtype = jnp.float32\n    x = x.reshape((x.shape[0], -1))\n    x = nn.Dense(\n        features=2 * common.BOARD_SIZE**2, name='hidden1', dtype=dtype)(\n            x)",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_jax",
        "documentation": {}
    },
    {
        "label": "train_agent",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "peekOfCode": "def train_agent(iterations, modeldir, logdir):\n  \"\"\"Train and convert the model.\"\"\"\n  model = tf.keras.models.Sequential([\n      tf.keras.layers.Flatten(\n          input_shape=(common.BOARD_SIZE, common.BOARD_SIZE)),\n      tf.keras.layers.Dense(2 * common.BOARD_SIZE**2, activation='relu'),\n      tf.keras.layers.Dense(common.BOARD_SIZE**2, activation='relu'),\n      tf.keras.layers.Dense(common.BOARD_SIZE**2, activation='softmax')\n  ])\n  sgd = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "peekOfCode": "def main(argv: Sequence[str]) -> None:\n  if len(argv) > 1:\n    raise app.UsageError('Too many command-line arguments.')\n  train_agent(ITERATIONS, MODELDIR, LOGDIR)\nif __name__ == '__main__':\n  app.run(main)",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "documentation": {}
    },
    {
        "label": "ITERATIONS",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "peekOfCode": "ITERATIONS = 80000\nLEARNING_RATE = 0.002\nMODELDIR = './'\nLOGDIR = './tf_log'\ndef train_agent(iterations, modeldir, logdir):\n  \"\"\"Train and convert the model.\"\"\"\n  model = tf.keras.models.Sequential([\n      tf.keras.layers.Flatten(\n          input_shape=(common.BOARD_SIZE, common.BOARD_SIZE)),\n      tf.keras.layers.Dense(2 * common.BOARD_SIZE**2, activation='relu'),",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "documentation": {}
    },
    {
        "label": "LEARNING_RATE",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "peekOfCode": "LEARNING_RATE = 0.002\nMODELDIR = './'\nLOGDIR = './tf_log'\ndef train_agent(iterations, modeldir, logdir):\n  \"\"\"Train and convert the model.\"\"\"\n  model = tf.keras.models.Sequential([\n      tf.keras.layers.Flatten(\n          input_shape=(common.BOARD_SIZE, common.BOARD_SIZE)),\n      tf.keras.layers.Dense(2 * common.BOARD_SIZE**2, activation='relu'),\n      tf.keras.layers.Dense(common.BOARD_SIZE**2, activation='relu'),",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "documentation": {}
    },
    {
        "label": "MODELDIR",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "peekOfCode": "MODELDIR = './'\nLOGDIR = './tf_log'\ndef train_agent(iterations, modeldir, logdir):\n  \"\"\"Train and convert the model.\"\"\"\n  model = tf.keras.models.Sequential([\n      tf.keras.layers.Flatten(\n          input_shape=(common.BOARD_SIZE, common.BOARD_SIZE)),\n      tf.keras.layers.Dense(2 * common.BOARD_SIZE**2, activation='relu'),\n      tf.keras.layers.Dense(common.BOARD_SIZE**2, activation='relu'),\n      tf.keras.layers.Dense(common.BOARD_SIZE**2, activation='softmax')",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "documentation": {}
    },
    {
        "label": "LOGDIR",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "description": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "peekOfCode": "LOGDIR = './tf_log'\ndef train_agent(iterations, modeldir, logdir):\n  \"\"\"Train and convert the model.\"\"\"\n  model = tf.keras.models.Sequential([\n      tf.keras.layers.Flatten(\n          input_shape=(common.BOARD_SIZE, common.BOARD_SIZE)),\n      tf.keras.layers.Dense(2 * common.BOARD_SIZE**2, activation='relu'),\n      tf.keras.layers.Dense(common.BOARD_SIZE**2, activation='relu'),\n      tf.keras.layers.Dense(common.BOARD_SIZE**2, activation='softmax')\n  ])",
        "detail": "examples.lite.examples.reinforcement_learning.ml.tf_and_jax.training_tf",
        "documentation": {}
    },
    {
        "label": "play_game",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "def play_game(predict_fn):\n  \"\"\"Play one round of game to gather logs for TF/JAX training.\"\"\"\n  # Only import gym-related libraries when absolutely needed\n  # pylint: disable=g-import-not-at-top\n  import gym\n  # pylint: disable=unused-import,g-import-not-at-top\n  import gym_planestrike\n  env = gym.make('PlaneStrike-v0', board_size=BOARD_SIZE)\n  observation = env.reset()\n  game_board_log = []",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "compute_rewards",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "def compute_rewards(game_result_log, gamma=GAMMA):\n  \"\"\"Compute discounted rewards for TF/JAX training.\"\"\"\n  discounted_rewards = []\n  discounted_sum = 0\n  for reward in game_result_log[::-1]:\n    discounted_sum = reward + gamma * discounted_sum\n    discounted_rewards.append(discounted_sum)\n  return np.asarray(discounted_rewards[::-1])\ndef initialize_random_hidden_board(board_size):\n  \"\"\"Initialize the hidden board.\"\"\"",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "initialize_random_hidden_board",
        "kind": 2,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "def initialize_random_hidden_board(board_size):\n  \"\"\"Initialize the hidden board.\"\"\"\n  hidden_board = np.ones(\n      (board_size, board_size)) * HIDDEN_BOARD_CELL_UNOCCUPIED\n  # Populate the plane's position\n  # First figure out the plane's orientation\n  #   0: heading right\n  #   1: heading up\n  #   2: heading left\n  #   3: heading down",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "BOARD_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "BOARD_SIZE = 8\nPLANE_SIZE = 8\n# Reward discount factor\nGAMMA = 0.5\n# Plane direction\nPLANE_HEADING_RIGHT = 0\nPLANE_HEADING_UP = 1\nPLANE_HEADING_LEFT = 2\nPLANE_HEADING_DOWN = 3\n# Hidden board cell status; 'occupied' means it's part of the plane",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "PLANE_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "PLANE_SIZE = 8\n# Reward discount factor\nGAMMA = 0.5\n# Plane direction\nPLANE_HEADING_RIGHT = 0\nPLANE_HEADING_UP = 1\nPLANE_HEADING_LEFT = 2\nPLANE_HEADING_DOWN = 3\n# Hidden board cell status; 'occupied' means it's part of the plane\nHIDDEN_BOARD_CELL_OCCUPIED = 1",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "GAMMA",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "GAMMA = 0.5\n# Plane direction\nPLANE_HEADING_RIGHT = 0\nPLANE_HEADING_UP = 1\nPLANE_HEADING_LEFT = 2\nPLANE_HEADING_DOWN = 3\n# Hidden board cell status; 'occupied' means it's part of the plane\nHIDDEN_BOARD_CELL_OCCUPIED = 1\nHIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "PLANE_HEADING_RIGHT",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "PLANE_HEADING_RIGHT = 0\nPLANE_HEADING_UP = 1\nPLANE_HEADING_LEFT = 2\nPLANE_HEADING_DOWN = 3\n# Hidden board cell status; 'occupied' means it's part of the plane\nHIDDEN_BOARD_CELL_OCCUPIED = 1\nHIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status\nBOARD_CELL_HIT = 1\nBOARD_CELL_MISS = -1",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "PLANE_HEADING_UP",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "PLANE_HEADING_UP = 1\nPLANE_HEADING_LEFT = 2\nPLANE_HEADING_DOWN = 3\n# Hidden board cell status; 'occupied' means it's part of the plane\nHIDDEN_BOARD_CELL_OCCUPIED = 1\nHIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status\nBOARD_CELL_HIT = 1\nBOARD_CELL_MISS = -1\nBOARD_CELL_UNTRIED = 0",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "PLANE_HEADING_LEFT",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "PLANE_HEADING_LEFT = 2\nPLANE_HEADING_DOWN = 3\n# Hidden board cell status; 'occupied' means it's part of the plane\nHIDDEN_BOARD_CELL_OCCUPIED = 1\nHIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status\nBOARD_CELL_HIT = 1\nBOARD_CELL_MISS = -1\nBOARD_CELL_UNTRIED = 0\ndef play_game(predict_fn):",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "PLANE_HEADING_DOWN",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "PLANE_HEADING_DOWN = 3\n# Hidden board cell status; 'occupied' means it's part of the plane\nHIDDEN_BOARD_CELL_OCCUPIED = 1\nHIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status\nBOARD_CELL_HIT = 1\nBOARD_CELL_MISS = -1\nBOARD_CELL_UNTRIED = 0\ndef play_game(predict_fn):\n  \"\"\"Play one round of game to gather logs for TF/JAX training.\"\"\"",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "HIDDEN_BOARD_CELL_OCCUPIED",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "HIDDEN_BOARD_CELL_OCCUPIED = 1\nHIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status\nBOARD_CELL_HIT = 1\nBOARD_CELL_MISS = -1\nBOARD_CELL_UNTRIED = 0\ndef play_game(predict_fn):\n  \"\"\"Play one round of game to gather logs for TF/JAX training.\"\"\"\n  # Only import gym-related libraries when absolutely needed\n  # pylint: disable=g-import-not-at-top",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "HIDDEN_BOARD_CELL_UNOCCUPIED",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "HIDDEN_BOARD_CELL_UNOCCUPIED = 0\n# Visible board cell status\nBOARD_CELL_HIT = 1\nBOARD_CELL_MISS = -1\nBOARD_CELL_UNTRIED = 0\ndef play_game(predict_fn):\n  \"\"\"Play one round of game to gather logs for TF/JAX training.\"\"\"\n  # Only import gym-related libraries when absolutely needed\n  # pylint: disable=g-import-not-at-top\n  import gym",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "BOARD_CELL_HIT",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "BOARD_CELL_HIT = 1\nBOARD_CELL_MISS = -1\nBOARD_CELL_UNTRIED = 0\ndef play_game(predict_fn):\n  \"\"\"Play one round of game to gather logs for TF/JAX training.\"\"\"\n  # Only import gym-related libraries when absolutely needed\n  # pylint: disable=g-import-not-at-top\n  import gym\n  # pylint: disable=unused-import,g-import-not-at-top\n  import gym_planestrike",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "BOARD_CELL_MISS",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "BOARD_CELL_MISS = -1\nBOARD_CELL_UNTRIED = 0\ndef play_game(predict_fn):\n  \"\"\"Play one round of game to gather logs for TF/JAX training.\"\"\"\n  # Only import gym-related libraries when absolutely needed\n  # pylint: disable=g-import-not-at-top\n  import gym\n  # pylint: disable=unused-import,g-import-not-at-top\n  import gym_planestrike\n  env = gym.make('PlaneStrike-v0', board_size=BOARD_SIZE)",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "BOARD_CELL_UNTRIED",
        "kind": 5,
        "importPath": "examples.lite.examples.reinforcement_learning.ml.common",
        "description": "examples.lite.examples.reinforcement_learning.ml.common",
        "peekOfCode": "BOARD_CELL_UNTRIED = 0\ndef play_game(predict_fn):\n  \"\"\"Play one round of game to gather logs for TF/JAX training.\"\"\"\n  # Only import gym-related libraries when absolutely needed\n  # pylint: disable=g-import-not-at-top\n  import gym\n  # pylint: disable=unused-import,g-import-not-at-top\n  import gym_planestrike\n  env = gym.make('PlaneStrike-v0', board_size=BOARD_SIZE)\n  observation = env.reset()",
        "detail": "examples.lite.examples.reinforcement_learning.ml.common",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.lite.examples.sound_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.sound_classification.raspberry_pi.classify",
        "peekOfCode": "def run(model: str, max_results: int, score_threshold: float,\n        overlapping_factor: float, num_threads: int,\n        enable_edgetpu: bool) -> None:\n  \"\"\"Continuously run inference on audio data acquired from the device.\n  Args:\n    model: Name of the TFLite audio classification model.\n    max_results: Maximum number of classification results to display.\n    score_threshold: The score threshold of classification results.\n    overlapping_factor: Target overlapping between adjacent inferences.\n    num_threads: Number of CPU threads to run the model.",
        "detail": "examples.lite.examples.sound_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.sound_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.sound_classification.raspberry_pi.classify",
        "peekOfCode": "def main():\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      '--model',\n      help='Name of the audio classification model.',\n      required=False,\n      default='yamnet.tflite')\n  parser.add_argument(\n      '--maxResults',",
        "detail": "examples.lite.examples.sound_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "Plotter",
        "kind": 6,
        "importPath": "examples.lite.examples.sound_classification.raspberry_pi.utils",
        "description": "examples.lite.examples.sound_classification.raspberry_pi.utils",
        "peekOfCode": "class Plotter(object):\n  \"\"\"An util class to display the classification results.\"\"\"\n  _PAUSE_TIME = 0.05\n  \"\"\"Time for matplotlib to wait for UI event.\"\"\"\n  def __init__(self) -> None:\n    fig, self._axes = plt.subplots()\n    fig.canvas.manager.set_window_title('Audio classification')\n    # Stop the program when the ESC key is pressed.\n    def event_callback(event):\n      if event.key == 'escape':",
        "detail": "examples.lite.examples.sound_classification.raspberry_pi.utils",
        "documentation": {}
    },
    {
        "label": "keras_model",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "peekOfCode": "keras_model = \"../conv_1d_time_stacked_model/ep-084-vl-0.2595.hdf5\"\ninput_arrays = [\"the_input\"]\noutput_arrays = [\"the_output\"]\nconverter = tf.lite.TFLiteConverter\nconverter = converter.from_keras_model_file(keras_model, input_arrays,\n                                            output_arrays)\ntflite_model = converter.convert()\nopen(\"converted_speed_keras_model.tflite\", \"wb\").write(tflite_model)",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "documentation": {}
    },
    {
        "label": "input_arrays",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "peekOfCode": "input_arrays = [\"the_input\"]\noutput_arrays = [\"the_output\"]\nconverter = tf.lite.TFLiteConverter\nconverter = converter.from_keras_model_file(keras_model, input_arrays,\n                                            output_arrays)\ntflite_model = converter.convert()\nopen(\"converted_speed_keras_model.tflite\", \"wb\").write(tflite_model)",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "documentation": {}
    },
    {
        "label": "output_arrays",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "peekOfCode": "output_arrays = [\"the_output\"]\nconverter = tf.lite.TFLiteConverter\nconverter = converter.from_keras_model_file(keras_model, input_arrays,\n                                            output_arrays)\ntflite_model = converter.convert()\nopen(\"converted_speed_keras_model.tflite\", \"wb\").write(tflite_model)",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "documentation": {}
    },
    {
        "label": "converter",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "peekOfCode": "converter = tf.lite.TFLiteConverter\nconverter = converter.from_keras_model_file(keras_model, input_arrays,\n                                            output_arrays)\ntflite_model = converter.convert()\nopen(\"converted_speed_keras_model.tflite\", \"wb\").write(tflite_model)",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "documentation": {}
    },
    {
        "label": "converter",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "peekOfCode": "converter = converter.from_keras_model_file(keras_model, input_arrays,\n                                            output_arrays)\ntflite_model = converter.convert()\nopen(\"converted_speed_keras_model.tflite\", \"wb\").write(tflite_model)",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "documentation": {}
    },
    {
        "label": "tflite_model",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "peekOfCode": "tflite_model = converter.convert()\nopen(\"converted_speed_keras_model.tflite\", \"wb\").write(tflite_model)",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_lite",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "peekOfCode": "parser = argparse.ArgumentParser(description='set input arguments')\nparser.add_argument(\n    '-input_fld', action='store', dest='input_fld', type=str, default='.')\nparser.add_argument(\n    '-output_fld', action='store', dest='output_fld', type=str, default='')\nparser.add_argument(\n    '-input_model_file',\n    action='store',\n    dest='input_model_file',\n    type=str,",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "peekOfCode": "args = parser.parse_args()\nparser.print_help()\nprint('input args: ', args)\nif args.theano_backend and args.quantize:\n  raise ValueError('Quantize feature does not work with theano backend.')\noutput_fld = args.input_fld if not args.output_fld else args.output_fld\nif not args.output_model_file:\n  args.output_model_file = str(Path(args.input_model_file).name) + '.pb'\nPath(output_fld).mkdir(parents=True, exist_ok=True)\nweight_file_path = str(Path(args.input_fld) / args.input_model_file)",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "documentation": {}
    },
    {
        "label": "output_fld",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "peekOfCode": "output_fld = args.input_fld if not args.output_fld else args.output_fld\nif not args.output_model_file:\n  args.output_model_file = str(Path(args.input_model_file).name) + '.pb'\nPath(output_fld).mkdir(parents=True, exist_ok=True)\nweight_file_path = str(Path(args.input_fld) / args.input_model_file)\n# Load keras model and rename output\n# In[ ]:\nK.set_learning_phase(0)\nif args.theano_backend:\n  K.set_image_data_format('channels_first')",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "documentation": {}
    },
    {
        "label": "weight_file_path",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "peekOfCode": "weight_file_path = str(Path(args.input_fld) / args.input_model_file)\n# Load keras model and rename output\n# In[ ]:\nK.set_learning_phase(0)\nif args.theano_backend:\n  K.set_image_data_format('channels_first')\nelse:\n  K.set_image_data_format('channels_last')\ntry:\n  fingerprint_size = 16000",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "documentation": {}
    },
    {
        "label": "num_output",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "peekOfCode": "num_output = args.num_outputs\npred = [None] * num_output\npred_node_names = [None] * num_output\nfor i in range(num_output):\n  pred_node_names[i] = args.output_node_prefix + str(i)\n  pred[i] = tf.identity(net_model.outputs[i], name=pred_node_names[i])\nprint('output nodes names are: ', pred_node_names)\n# [optional] write graph definition in ascii\n# In[ ]:\nsess = K.get_session()",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "documentation": {}
    },
    {
        "label": "pred",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "peekOfCode": "pred = [None] * num_output\npred_node_names = [None] * num_output\nfor i in range(num_output):\n  pred_node_names[i] = args.output_node_prefix + str(i)\n  pred[i] = tf.identity(net_model.outputs[i], name=pred_node_names[i])\nprint('output nodes names are: ', pred_node_names)\n# [optional] write graph definition in ascii\n# In[ ]:\nsess = K.get_session()\nif args.graph_def:",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "documentation": {}
    },
    {
        "label": "pred_node_names",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "peekOfCode": "pred_node_names = [None] * num_output\nfor i in range(num_output):\n  pred_node_names[i] = args.output_node_prefix + str(i)\n  pred[i] = tf.identity(net_model.outputs[i], name=pred_node_names[i])\nprint('output nodes names are: ', pred_node_names)\n# [optional] write graph definition in ascii\n# In[ ]:\nsess = K.get_session()\nif args.graph_def:\n  f = args.output_graphdef_file",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "documentation": {}
    },
    {
        "label": "sess",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "description": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "peekOfCode": "sess = K.get_session()\nif args.graph_def:\n  f = args.output_graphdef_file\n  tf.io.write_graph(sess.graph.as_graph_def(), output_fld, f, as_text=True)\n  print('saved the graph definition in ascii format at: ',\n        str(Path(output_fld) / f))\n# convert variables to constants and save\n# In[ ]:\nif args.quantize:\n  # graph_transforms will not be available for future versions.",
        "detail": "examples.lite.examples.speech_commands.ml.export.convert_keras_to_quantized",
        "documentation": {}
    },
    {
        "label": "ConfusionMatrixCallback",
        "kind": 6,
        "importPath": "examples.lite.examples.speech_commands.ml.callbacks",
        "description": "examples.lite.examples.speech_commands.ml.callbacks",
        "peekOfCode": "class ConfusionMatrixCallback(Callback):\n  def __init__(self, validation_data, validation_steps, wanted_words, all_words,\n               label2int):\n    self.validation_data = validation_data\n    self.validation_steps = validation_steps\n    self.wanted_words = wanted_words\n    self.all_words = all_words\n    self.label2int = label2int\n    self.int2label = {v: k for k, v in label2int.items()}\n    with open('confusion_matrix.txt', 'w'):",
        "detail": "examples.lite.examples.speech_commands.ml.callbacks",
        "documentation": {}
    },
    {
        "label": "log_loss",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.callbacks",
        "description": "examples.lite.examples.speech_commands.ml.callbacks",
        "peekOfCode": "def log_loss(y_true, y_pred, eps=1e-12):\n  y_pred = np.clip(y_pred, eps, 1. - eps)\n  ce = -(np.sum(y_true * np.log(y_pred), axis=1))\n  mce = ce.mean()\n  return mce\nclass ConfusionMatrixCallback(Callback):\n  def __init__(self, validation_data, validation_steps, wanted_words, all_words,\n               label2int):\n    self.validation_data = validation_data\n    self.validation_steps = validation_steps",
        "detail": "examples.lite.examples.speech_commands.ml.callbacks",
        "documentation": {}
    },
    {
        "label": "get_classes",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.classes",
        "description": "examples.lite.examples.speech_commands.ml.classes",
        "peekOfCode": "def get_classes(wanted_only=False):\n  if wanted_only:\n    classes = 'stop down off right up go on yes left no'\n    classes = classes.split(' ')\n    assert len(classes) == 10\n  else:\n    classes = ('sheila nine stop bed four six down bird marvin cat off right '\n               'seven eight up three happy go zero on wow dog yes five one tree'\n               ' house two left no')  # noqa\n    classes = classes.split(' ')",
        "detail": "examples.lite.examples.speech_commands.ml.classes",
        "documentation": {}
    },
    {
        "label": "get_int2label",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.classes",
        "description": "examples.lite.examples.speech_commands.ml.classes",
        "peekOfCode": "def get_int2label(wanted_only=False, extend_reversed=False):\n  classes = get_classes(\n      wanted_only=wanted_only, extend_reversed=extend_reversed)\n  classes = prepare_words_list(classes)\n  int2label = {i: l for i, l in enumerate(classes)}\n  int2label = OrderedDict(sorted(int2label.items(), key=lambda x: x[0]))\n  return int2label\ndef get_label2int(wanted_only=False, extend_reversed=False):\n  classes = get_classes(\n      wanted_only=wanted_only, extend_reversed=extend_reversed)",
        "detail": "examples.lite.examples.speech_commands.ml.classes",
        "documentation": {}
    },
    {
        "label": "get_label2int",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.classes",
        "description": "examples.lite.examples.speech_commands.ml.classes",
        "peekOfCode": "def get_label2int(wanted_only=False, extend_reversed=False):\n  classes = get_classes(\n      wanted_only=wanted_only, extend_reversed=extend_reversed)\n  classes = prepare_words_list(classes)\n  label2int = {l: i for i, l in enumerate(classes)}\n  label2int = OrderedDict(sorted(label2int.items(), key=lambda x: x[1]))\n  return label2int",
        "detail": "examples.lite.examples.speech_commands.ml.classes",
        "documentation": {}
    },
    {
        "label": "DATASET_URL",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.download",
        "description": "examples.lite.examples.speech_commands.ml.download",
        "peekOfCode": "DATASET_URL = 'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz'\nARCHIVE = os.path.basename(DATASET_URL)\nwget.download(DATASET_URL)\nif os.path.exists('data'):\n  rmtree('data')\nos.makedirs('data/train')\nwith tarfile.open(ARCHIVE, 'r:gz') as tar:\n  tar.extractall(path='data/train')\nos.remove(ARCHIVE)",
        "detail": "examples.lite.examples.speech_commands.ml.download",
        "documentation": {}
    },
    {
        "label": "ARCHIVE",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.download",
        "description": "examples.lite.examples.speech_commands.ml.download",
        "peekOfCode": "ARCHIVE = os.path.basename(DATASET_URL)\nwget.download(DATASET_URL)\nif os.path.exists('data'):\n  rmtree('data')\nos.makedirs('data/train')\nwith tarfile.open(ARCHIVE, 'r:gz') as tar:\n  tar.extractall(path='data/train')\nos.remove(ARCHIVE)",
        "detail": "examples.lite.examples.speech_commands.ml.download",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "kind": 6,
        "importPath": "examples.lite.examples.speech_commands.ml.generator",
        "description": "examples.lite.examples.speech_commands.ml.generator",
        "peekOfCode": "class AudioProcessor(object):\n  \"\"\"Handles loading, partitioning, and preparing audio training data.\"\"\"\n  def __init__(self,\n               data_dirs,\n               silence_percentage,\n               unknown_percentage,\n               wanted_words,\n               validation_percentage,\n               testing_percentage,\n               model_settings,",
        "detail": "examples.lite.examples.speech_commands.ml.generator",
        "documentation": {}
    },
    {
        "label": "prepare_words_list",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.generator",
        "description": "examples.lite.examples.speech_commands.ml.generator",
        "peekOfCode": "def prepare_words_list(wanted_words):\n  \"\"\"Prepends common tokens to the custom word list.\"\"\"\n  return [SILENCE_LABEL, UNKNOWN_WORD_LABEL] + wanted_words\ndef which_set(filename, validation_percentage, testing_percentage):\n  \"\"\"Determines which data partition the file should belong to.\"\"\"\n  dir_name = os.path.basename(os.path.dirname(filename))\n  if dir_name == 'unknown_unknown':\n    return 'training'\n  base_name = os.path.basename(filename)\n  hash_name = re.sub(r'_nohash_.*$', '', base_name)",
        "detail": "examples.lite.examples.speech_commands.ml.generator",
        "documentation": {}
    },
    {
        "label": "which_set",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.generator",
        "description": "examples.lite.examples.speech_commands.ml.generator",
        "peekOfCode": "def which_set(filename, validation_percentage, testing_percentage):\n  \"\"\"Determines which data partition the file should belong to.\"\"\"\n  dir_name = os.path.basename(os.path.dirname(filename))\n  if dir_name == 'unknown_unknown':\n    return 'training'\n  base_name = os.path.basename(filename)\n  hash_name = re.sub(r'_nohash_.*$', '', base_name)\n  hash_name_hashed = hashlib.sha1(tf.compat.as_bytes(hash_name)).hexdigest()\n  percentage_hash = ((int(hash_name_hashed, 16) % (MAX_NUM_WAVS_PER_CLASS + 1))\n                     * (100.0 / MAX_NUM_WAVS_PER_CLASS))",
        "detail": "examples.lite.examples.speech_commands.ml.generator",
        "documentation": {}
    },
    {
        "label": "load_wav_file",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.generator",
        "description": "examples.lite.examples.speech_commands.ml.generator",
        "peekOfCode": "def load_wav_file(filename):\n  \"\"\"Loads an audio file and returns a float PCM-encoded array of samples.\"\"\"\n  with tf.Session(graph=tf.Graph()) as sess:\n    wav_filename_placeholder = tf.placeholder(tf.string, [])\n    wav_loader = tf.io.read_file(wav_filename_placeholder)\n    wav_decoder = tf.audio.decode_wav(wav_loader, desired_channels=1)\n    return sess.run(\n        wav_decoder, feed_dict={\n            wav_filename_placeholder: filename\n        }).audio.flatten()",
        "detail": "examples.lite.examples.speech_commands.ml.generator",
        "documentation": {}
    },
    {
        "label": "save_wav_file",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.generator",
        "description": "examples.lite.examples.speech_commands.ml.generator",
        "peekOfCode": "def save_wav_file(filename, wav_data, sample_rate):\n  \"\"\"Saves audio sample data to a .wav audio file.\"\"\"\n  with tf.Session(graph=tf.Graph()) as sess:\n    wav_filename_placeholder = tf.placeholder(tf.string, [])\n    sample_rate_placeholder = tf.placeholder(tf.int32, [])\n    wav_data_placeholder = tf.placeholder(tf.float32, [None, 1])\n    wav_encoder = tf.audio.encode_wav(wav_data_placeholder,\n                                      sample_rate_placeholder)\n    wav_saver = tf.io.write_file(wav_filename_placeholder, wav_encoder)\n    sess.run(",
        "detail": "examples.lite.examples.speech_commands.ml.generator",
        "documentation": {}
    },
    {
        "label": "MAX_NUM_WAVS_PER_CLASS",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.generator",
        "description": "examples.lite.examples.speech_commands.ml.generator",
        "peekOfCode": "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\nSILENCE_LABEL = '_silence_'\nSILENCE_INDEX = 0\nUNKNOWN_WORD_LABEL = '_unknown_'\nUNKNOWN_WORD_INDEX = 1\nBACKGROUND_NOISE_DIR_NAME = '_background_noise_'\nRANDOM_SEED = 59185\ndef prepare_words_list(wanted_words):\n  \"\"\"Prepends common tokens to the custom word list.\"\"\"\n  return [SILENCE_LABEL, UNKNOWN_WORD_LABEL] + wanted_words",
        "detail": "examples.lite.examples.speech_commands.ml.generator",
        "documentation": {}
    },
    {
        "label": "SILENCE_LABEL",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.generator",
        "description": "examples.lite.examples.speech_commands.ml.generator",
        "peekOfCode": "SILENCE_LABEL = '_silence_'\nSILENCE_INDEX = 0\nUNKNOWN_WORD_LABEL = '_unknown_'\nUNKNOWN_WORD_INDEX = 1\nBACKGROUND_NOISE_DIR_NAME = '_background_noise_'\nRANDOM_SEED = 59185\ndef prepare_words_list(wanted_words):\n  \"\"\"Prepends common tokens to the custom word list.\"\"\"\n  return [SILENCE_LABEL, UNKNOWN_WORD_LABEL] + wanted_words\ndef which_set(filename, validation_percentage, testing_percentage):",
        "detail": "examples.lite.examples.speech_commands.ml.generator",
        "documentation": {}
    },
    {
        "label": "SILENCE_INDEX",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.generator",
        "description": "examples.lite.examples.speech_commands.ml.generator",
        "peekOfCode": "SILENCE_INDEX = 0\nUNKNOWN_WORD_LABEL = '_unknown_'\nUNKNOWN_WORD_INDEX = 1\nBACKGROUND_NOISE_DIR_NAME = '_background_noise_'\nRANDOM_SEED = 59185\ndef prepare_words_list(wanted_words):\n  \"\"\"Prepends common tokens to the custom word list.\"\"\"\n  return [SILENCE_LABEL, UNKNOWN_WORD_LABEL] + wanted_words\ndef which_set(filename, validation_percentage, testing_percentage):\n  \"\"\"Determines which data partition the file should belong to.\"\"\"",
        "detail": "examples.lite.examples.speech_commands.ml.generator",
        "documentation": {}
    },
    {
        "label": "UNKNOWN_WORD_LABEL",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.generator",
        "description": "examples.lite.examples.speech_commands.ml.generator",
        "peekOfCode": "UNKNOWN_WORD_LABEL = '_unknown_'\nUNKNOWN_WORD_INDEX = 1\nBACKGROUND_NOISE_DIR_NAME = '_background_noise_'\nRANDOM_SEED = 59185\ndef prepare_words_list(wanted_words):\n  \"\"\"Prepends common tokens to the custom word list.\"\"\"\n  return [SILENCE_LABEL, UNKNOWN_WORD_LABEL] + wanted_words\ndef which_set(filename, validation_percentage, testing_percentage):\n  \"\"\"Determines which data partition the file should belong to.\"\"\"\n  dir_name = os.path.basename(os.path.dirname(filename))",
        "detail": "examples.lite.examples.speech_commands.ml.generator",
        "documentation": {}
    },
    {
        "label": "UNKNOWN_WORD_INDEX",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.generator",
        "description": "examples.lite.examples.speech_commands.ml.generator",
        "peekOfCode": "UNKNOWN_WORD_INDEX = 1\nBACKGROUND_NOISE_DIR_NAME = '_background_noise_'\nRANDOM_SEED = 59185\ndef prepare_words_list(wanted_words):\n  \"\"\"Prepends common tokens to the custom word list.\"\"\"\n  return [SILENCE_LABEL, UNKNOWN_WORD_LABEL] + wanted_words\ndef which_set(filename, validation_percentage, testing_percentage):\n  \"\"\"Determines which data partition the file should belong to.\"\"\"\n  dir_name = os.path.basename(os.path.dirname(filename))\n  if dir_name == 'unknown_unknown':",
        "detail": "examples.lite.examples.speech_commands.ml.generator",
        "documentation": {}
    },
    {
        "label": "BACKGROUND_NOISE_DIR_NAME",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.generator",
        "description": "examples.lite.examples.speech_commands.ml.generator",
        "peekOfCode": "BACKGROUND_NOISE_DIR_NAME = '_background_noise_'\nRANDOM_SEED = 59185\ndef prepare_words_list(wanted_words):\n  \"\"\"Prepends common tokens to the custom word list.\"\"\"\n  return [SILENCE_LABEL, UNKNOWN_WORD_LABEL] + wanted_words\ndef which_set(filename, validation_percentage, testing_percentage):\n  \"\"\"Determines which data partition the file should belong to.\"\"\"\n  dir_name = os.path.basename(os.path.dirname(filename))\n  if dir_name == 'unknown_unknown':\n    return 'training'",
        "detail": "examples.lite.examples.speech_commands.ml.generator",
        "documentation": {}
    },
    {
        "label": "RANDOM_SEED",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.generator",
        "description": "examples.lite.examples.speech_commands.ml.generator",
        "peekOfCode": "RANDOM_SEED = 59185\ndef prepare_words_list(wanted_words):\n  \"\"\"Prepends common tokens to the custom word list.\"\"\"\n  return [SILENCE_LABEL, UNKNOWN_WORD_LABEL] + wanted_words\ndef which_set(filename, validation_percentage, testing_percentage):\n  \"\"\"Determines which data partition the file should belong to.\"\"\"\n  dir_name = os.path.basename(os.path.dirname(filename))\n  if dir_name == 'unknown_unknown':\n    return 'training'\n  base_name = os.path.basename(filename)",
        "detail": "examples.lite.examples.speech_commands.ml.generator",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.model",
        "description": "examples.lite.examples.speech_commands.ml.model",
        "peekOfCode": "def preprocess(x):\n  x = (x + 0.8) / 7.0\n  x = K.clip(x, -5, 5)\n  return x\ndef preprocess_raw(x):\n  return x\nPreprocess = Lambda(preprocess)\nPreprocessRaw = Lambda(preprocess_raw)\ndef relu6(x):\n  return K.relu(x, max_value=6)",
        "detail": "examples.lite.examples.speech_commands.ml.model",
        "documentation": {}
    },
    {
        "label": "preprocess_raw",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.model",
        "description": "examples.lite.examples.speech_commands.ml.model",
        "peekOfCode": "def preprocess_raw(x):\n  return x\nPreprocess = Lambda(preprocess)\nPreprocessRaw = Lambda(preprocess_raw)\ndef relu6(x):\n  return K.relu(x, max_value=6)\ndef conv_1d_time_stacked_model(input_size=16000, num_classes=11):\n  \"\"\" Creates a 1D model for temporal data.\n  Note: Use only\n  with compute_mfcc = False (e.g. raw waveform data).",
        "detail": "examples.lite.examples.speech_commands.ml.model",
        "documentation": {}
    },
    {
        "label": "relu6",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.model",
        "description": "examples.lite.examples.speech_commands.ml.model",
        "peekOfCode": "def relu6(x):\n  return K.relu(x, max_value=6)\ndef conv_1d_time_stacked_model(input_size=16000, num_classes=11):\n  \"\"\" Creates a 1D model for temporal data.\n  Note: Use only\n  with compute_mfcc = False (e.g. raw waveform data).\n  Args:\n    input_size: How big the input vector is.\n    num_classes: How many classes are to be recognized.\n  Returns:",
        "detail": "examples.lite.examples.speech_commands.ml.model",
        "documentation": {}
    },
    {
        "label": "conv_1d_time_stacked_model",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.model",
        "description": "examples.lite.examples.speech_commands.ml.model",
        "peekOfCode": "def conv_1d_time_stacked_model(input_size=16000, num_classes=11):\n  \"\"\" Creates a 1D model for temporal data.\n  Note: Use only\n  with compute_mfcc = False (e.g. raw waveform data).\n  Args:\n    input_size: How big the input vector is.\n    num_classes: How many classes are to be recognized.\n  Returns:\n    Compiled keras model\n  \"\"\"",
        "detail": "examples.lite.examples.speech_commands.ml.model",
        "documentation": {}
    },
    {
        "label": "speech_model",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.model",
        "description": "examples.lite.examples.speech_commands.ml.model",
        "peekOfCode": "def speech_model(model_type, input_size, num_classes=11, *args, **kwargs):\n  if model_type == 'conv_1d_time_stacked':\n    return conv_1d_time_stacked_model(input_size, num_classes)\n  else:\n    raise ValueError('Invalid model: %s' % model_type)\ndef prepare_model_settings(label_count,\n                           sample_rate,\n                           clip_duration_ms,\n                           window_size_ms,\n                           window_stride_ms,",
        "detail": "examples.lite.examples.speech_commands.ml.model",
        "documentation": {}
    },
    {
        "label": "prepare_model_settings",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.model",
        "description": "examples.lite.examples.speech_commands.ml.model",
        "peekOfCode": "def prepare_model_settings(label_count,\n                           sample_rate,\n                           clip_duration_ms,\n                           window_size_ms,\n                           window_stride_ms,\n                           dct_coefficient_count,\n                           num_log_mel_features,\n                           output_representation='raw'):\n  \"\"\"Calculates common settings needed for all models.\"\"\"\n  desired_samples = int(sample_rate * clip_duration_ms / 1000)",
        "detail": "examples.lite.examples.speech_commands.ml.model",
        "documentation": {}
    },
    {
        "label": "Preprocess",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.model",
        "description": "examples.lite.examples.speech_commands.ml.model",
        "peekOfCode": "Preprocess = Lambda(preprocess)\nPreprocessRaw = Lambda(preprocess_raw)\ndef relu6(x):\n  return K.relu(x, max_value=6)\ndef conv_1d_time_stacked_model(input_size=16000, num_classes=11):\n  \"\"\" Creates a 1D model for temporal data.\n  Note: Use only\n  with compute_mfcc = False (e.g. raw waveform data).\n  Args:\n    input_size: How big the input vector is.",
        "detail": "examples.lite.examples.speech_commands.ml.model",
        "documentation": {}
    },
    {
        "label": "PreprocessRaw",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.model",
        "description": "examples.lite.examples.speech_commands.ml.model",
        "peekOfCode": "PreprocessRaw = Lambda(preprocess_raw)\ndef relu6(x):\n  return K.relu(x, max_value=6)\ndef conv_1d_time_stacked_model(input_size=16000, num_classes=11):\n  \"\"\" Creates a 1D model for temporal data.\n  Note: Use only\n  with compute_mfcc = False (e.g. raw waveform data).\n  Args:\n    input_size: How big the input vector is.\n    num_classes: How many classes are to be recognized.",
        "detail": "examples.lite.examples.speech_commands.ml.model",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.train",
        "description": "examples.lite.examples.speech_commands.ml.train",
        "peekOfCode": "parser = argparse.ArgumentParser(description='set input arguments')\nparser.add_argument(\n    '-sample_rate',\n    action='store',\n    dest='sample_rate',\n    type=int,\n    default=16000,\n    help='Sample rate of audio')\nparser.add_argument(\n    '-batch_size',",
        "detail": "examples.lite.examples.speech_commands.ml.train",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "examples.lite.examples.speech_commands.ml.train",
        "description": "examples.lite.examples.speech_commands.ml.train",
        "peekOfCode": "args = parser.parse_args()\nparser.print_help()\nprint('input args: ', args)\nif __name__ == '__main__':\n  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n  sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n  K.set_session(sess)\n  data_dirs = args.data_dirs\n  output_representation = args.output_representation\n  sample_rate = args.sample_rate",
        "detail": "examples.lite.examples.speech_commands.ml.train",
        "documentation": {}
    },
    {
        "label": "data_gen",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.utils",
        "description": "examples.lite.examples.speech_commands.ml.utils",
        "peekOfCode": "def data_gen(audio_processor,\n             sess,\n             batch_size=128,\n             background_frequency=0.3,\n             background_volume_range=0.15,\n             foreground_frequency=0.3,\n             foreground_volume_range=0.15,\n             time_shift_frequency=0.3,\n             time_shift_range=[-500, 0],\n             mode='validation',",
        "detail": "examples.lite.examples.speech_commands.ml.utils",
        "documentation": {}
    },
    {
        "label": "tf_roll",
        "kind": 2,
        "importPath": "examples.lite.examples.speech_commands.ml.utils",
        "description": "examples.lite.examples.speech_commands.ml.utils",
        "peekOfCode": "def tf_roll(a, shift, a_len=16000):\n  # https://stackoverflow.com/questions/42651714/vector-shift-roll-in-tensorflow\n  def roll_left(a, shift, a_len):\n    shift %= a_len\n    rolled = tf.concat([a[a_len - shift:, :], a[:a_len - shift, :]], axis=0)\n    return rolled\n  def roll_right(a, shift, a_len):\n    shift = -shift\n    shift %= a_len\n    rolled = tf.concat([a[shift:, :], a[:shift, :]], axis=0)",
        "detail": "examples.lite.examples.speech_commands.ml.utils",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "peekOfCode": "def run(model: str, label: str, max_results: int, num_threads: int,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n      model: Name of the TFLite video classification model.\n      label: Name of the video classification label.\n      max_results: Max of classification results.\n      num_threads: Number of CPU threads to run the model.\n      camera_id: The camera id to be passed to OpenCV.\n      width: The width of the frame captured from the camera.",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "peekOfCode": "def main():\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      '--model',\n      help='Name of video classification model.',\n      required=False,\n      default='movinet_a0_int8.tflite')\n  parser.add_argument(\n      '--label',",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_ROW_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "peekOfCode": "_ROW_SIZE = 20  # pixels\n_LEFT_MARGIN = 24  # pixels\n_TEXT_COLOR = (0, 0, 255)  # red\n_FONT_SIZE = 1\n_FONT_THICKNESS = 1\n_MODEL_FPS = 5  # Ensure the input images are fed to the model at this fps.\n_MODEL_FPS_ERROR_RANGE = 0.1  # Acceptable error range in fps.\ndef run(model: str, label: str, max_results: int, num_threads: int,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_LEFT_MARGIN",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "peekOfCode": "_LEFT_MARGIN = 24  # pixels\n_TEXT_COLOR = (0, 0, 255)  # red\n_FONT_SIZE = 1\n_FONT_THICKNESS = 1\n_MODEL_FPS = 5  # Ensure the input images are fed to the model at this fps.\n_MODEL_FPS_ERROR_RANGE = 0.1  # Acceptable error range in fps.\ndef run(model: str, label: str, max_results: int, num_threads: int,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_TEXT_COLOR",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "peekOfCode": "_TEXT_COLOR = (0, 0, 255)  # red\n_FONT_SIZE = 1\n_FONT_THICKNESS = 1\n_MODEL_FPS = 5  # Ensure the input images are fed to the model at this fps.\n_MODEL_FPS_ERROR_RANGE = 0.1  # Acceptable error range in fps.\ndef run(model: str, label: str, max_results: int, num_threads: int,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n      model: Name of the TFLite video classification model.",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_FONT_SIZE",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "peekOfCode": "_FONT_SIZE = 1\n_FONT_THICKNESS = 1\n_MODEL_FPS = 5  # Ensure the input images are fed to the model at this fps.\n_MODEL_FPS_ERROR_RANGE = 0.1  # Acceptable error range in fps.\ndef run(model: str, label: str, max_results: int, num_threads: int,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n      model: Name of the TFLite video classification model.\n      label: Name of the video classification label.",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_FONT_THICKNESS",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "peekOfCode": "_FONT_THICKNESS = 1\n_MODEL_FPS = 5  # Ensure the input images are fed to the model at this fps.\n_MODEL_FPS_ERROR_RANGE = 0.1  # Acceptable error range in fps.\ndef run(model: str, label: str, max_results: int, num_threads: int,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n      model: Name of the TFLite video classification model.\n      label: Name of the video classification label.\n      max_results: Max of classification results.",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_MODEL_FPS",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "peekOfCode": "_MODEL_FPS = 5  # Ensure the input images are fed to the model at this fps.\n_MODEL_FPS_ERROR_RANGE = 0.1  # Acceptable error range in fps.\ndef run(model: str, label: str, max_results: int, num_threads: int,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n      model: Name of the TFLite video classification model.\n      label: Name of the video classification label.\n      max_results: Max of classification results.\n      num_threads: Number of CPU threads to run the model.",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "_MODEL_FPS_ERROR_RANGE",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "description": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "peekOfCode": "_MODEL_FPS_ERROR_RANGE = 0.1  # Acceptable error range in fps.\ndef run(model: str, label: str, max_results: int, num_threads: int,\n        camera_id: int, width: int, height: int) -> None:\n  \"\"\"Continuously run inference on images acquired from the camera.\n  Args:\n      model: Name of the TFLite video classification model.\n      label: Name of the video classification label.\n      max_results: Max of classification results.\n      num_threads: Number of CPU threads to run the model.\n      camera_id: The camera id to be passed to OpenCV.",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.classify",
        "documentation": {}
    },
    {
        "label": "VideoClassifierOptions",
        "kind": 6,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier",
        "peekOfCode": "class VideoClassifierOptions(NamedTuple):\n  \"\"\"A config to initialize an video classifier.\"\"\"\n  label_allow_list: List[str] = None\n  \"\"\"The optional allow list of labels.\"\"\"\n  label_deny_list: List[str] = None\n  \"\"\"The optional deny list of labels.\"\"\"\n  max_results: int = 5\n  \"\"\"The maximum number of top-scored classification results to return.\"\"\"\n  num_threads: int = 1\n  \"\"\"The number of CPU threads to be used.\"\"\"",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier",
        "documentation": {}
    },
    {
        "label": "Category",
        "kind": 6,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier",
        "peekOfCode": "class Category(NamedTuple):\n  \"\"\"A result of a video classification.\"\"\"\n  label: str\n  score: float\nclass VideoClassifier(object):\n  \"\"\"A wrapper class for a TFLite video classification model.\"\"\"\n  _MODEL_INPUT_SIGNATURE_NAME = 'image'\n  _MODEL_OUTPUT_SIGNATURE_NAME = 'logits'\n  _MODEL_INPUT_MEAN = 0\n  _MODEL_INPUT_STD = 255",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier",
        "documentation": {}
    },
    {
        "label": "VideoClassifier",
        "kind": 6,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier",
        "peekOfCode": "class VideoClassifier(object):\n  \"\"\"A wrapper class for a TFLite video classification model.\"\"\"\n  _MODEL_INPUT_SIGNATURE_NAME = 'image'\n  _MODEL_OUTPUT_SIGNATURE_NAME = 'logits'\n  _MODEL_INPUT_MEAN = 0\n  _MODEL_INPUT_STD = 255\n  def __init__(\n      self,\n      model_path: str,\n      label_file: str,",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier",
        "documentation": {}
    },
    {
        "label": "VideoClassifierTest",
        "kind": 6,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "peekOfCode": "class VideoClassifierTest(unittest.TestCase):\n  def setUp(self):\n    \"\"\"Initialize the shared variables.\"\"\"\n    super().setUp()\n    # Load frames from the test video.\n    cap = cv2.VideoCapture(_VIDEO_FILE)\n    frames = []\n    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n      _, frame = cap.read()\n      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "documentation": {}
    },
    {
        "label": "_MODEL_FILE",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "peekOfCode": "_MODEL_FILE = 'movinet_a0_int8.tflite'\n_LABEL_FILE = 'kinetics600_label_map.txt'\n_GROUND_TRUTH_LABEL = 'carving ice'\n_GROUND_TRUTH_MIN_SCORE = 0.5\n_VIDEO_FILE = 'test_data/carving_ice.mp4'\n_ALLOW_LIST = ['carving ice', 'sawing wood']\n_DENY_LIST = ['chiseling stone']\n_SCORE_THRESHOLD = 0.2\n_MAX_RESULTS = 3\n_ACCEPTABLE_ERROR_RANGE = 0.01",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "documentation": {}
    },
    {
        "label": "_LABEL_FILE",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "peekOfCode": "_LABEL_FILE = 'kinetics600_label_map.txt'\n_GROUND_TRUTH_LABEL = 'carving ice'\n_GROUND_TRUTH_MIN_SCORE = 0.5\n_VIDEO_FILE = 'test_data/carving_ice.mp4'\n_ALLOW_LIST = ['carving ice', 'sawing wood']\n_DENY_LIST = ['chiseling stone']\n_SCORE_THRESHOLD = 0.2\n_MAX_RESULTS = 3\n_ACCEPTABLE_ERROR_RANGE = 0.01\nclass VideoClassifierTest(unittest.TestCase):",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "documentation": {}
    },
    {
        "label": "_GROUND_TRUTH_LABEL",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "peekOfCode": "_GROUND_TRUTH_LABEL = 'carving ice'\n_GROUND_TRUTH_MIN_SCORE = 0.5\n_VIDEO_FILE = 'test_data/carving_ice.mp4'\n_ALLOW_LIST = ['carving ice', 'sawing wood']\n_DENY_LIST = ['chiseling stone']\n_SCORE_THRESHOLD = 0.2\n_MAX_RESULTS = 3\n_ACCEPTABLE_ERROR_RANGE = 0.01\nclass VideoClassifierTest(unittest.TestCase):\n  def setUp(self):",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "documentation": {}
    },
    {
        "label": "_GROUND_TRUTH_MIN_SCORE",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "peekOfCode": "_GROUND_TRUTH_MIN_SCORE = 0.5\n_VIDEO_FILE = 'test_data/carving_ice.mp4'\n_ALLOW_LIST = ['carving ice', 'sawing wood']\n_DENY_LIST = ['chiseling stone']\n_SCORE_THRESHOLD = 0.2\n_MAX_RESULTS = 3\n_ACCEPTABLE_ERROR_RANGE = 0.01\nclass VideoClassifierTest(unittest.TestCase):\n  def setUp(self):\n    \"\"\"Initialize the shared variables.\"\"\"",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "documentation": {}
    },
    {
        "label": "_VIDEO_FILE",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "peekOfCode": "_VIDEO_FILE = 'test_data/carving_ice.mp4'\n_ALLOW_LIST = ['carving ice', 'sawing wood']\n_DENY_LIST = ['chiseling stone']\n_SCORE_THRESHOLD = 0.2\n_MAX_RESULTS = 3\n_ACCEPTABLE_ERROR_RANGE = 0.01\nclass VideoClassifierTest(unittest.TestCase):\n  def setUp(self):\n    \"\"\"Initialize the shared variables.\"\"\"\n    super().setUp()",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "documentation": {}
    },
    {
        "label": "_ALLOW_LIST",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "peekOfCode": "_ALLOW_LIST = ['carving ice', 'sawing wood']\n_DENY_LIST = ['chiseling stone']\n_SCORE_THRESHOLD = 0.2\n_MAX_RESULTS = 3\n_ACCEPTABLE_ERROR_RANGE = 0.01\nclass VideoClassifierTest(unittest.TestCase):\n  def setUp(self):\n    \"\"\"Initialize the shared variables.\"\"\"\n    super().setUp()\n    # Load frames from the test video.",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "documentation": {}
    },
    {
        "label": "_DENY_LIST",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "peekOfCode": "_DENY_LIST = ['chiseling stone']\n_SCORE_THRESHOLD = 0.2\n_MAX_RESULTS = 3\n_ACCEPTABLE_ERROR_RANGE = 0.01\nclass VideoClassifierTest(unittest.TestCase):\n  def setUp(self):\n    \"\"\"Initialize the shared variables.\"\"\"\n    super().setUp()\n    # Load frames from the test video.\n    cap = cv2.VideoCapture(_VIDEO_FILE)",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "documentation": {}
    },
    {
        "label": "_SCORE_THRESHOLD",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "peekOfCode": "_SCORE_THRESHOLD = 0.2\n_MAX_RESULTS = 3\n_ACCEPTABLE_ERROR_RANGE = 0.01\nclass VideoClassifierTest(unittest.TestCase):\n  def setUp(self):\n    \"\"\"Initialize the shared variables.\"\"\"\n    super().setUp()\n    # Load frames from the test video.\n    cap = cv2.VideoCapture(_VIDEO_FILE)\n    frames = []",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "documentation": {}
    },
    {
        "label": "_MAX_RESULTS",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "peekOfCode": "_MAX_RESULTS = 3\n_ACCEPTABLE_ERROR_RANGE = 0.01\nclass VideoClassifierTest(unittest.TestCase):\n  def setUp(self):\n    \"\"\"Initialize the shared variables.\"\"\"\n    super().setUp()\n    # Load frames from the test video.\n    cap = cv2.VideoCapture(_VIDEO_FILE)\n    frames = []\n    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "documentation": {}
    },
    {
        "label": "_ACCEPTABLE_ERROR_RANGE",
        "kind": 5,
        "importPath": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "description": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "peekOfCode": "_ACCEPTABLE_ERROR_RANGE = 0.01\nclass VideoClassifierTest(unittest.TestCase):\n  def setUp(self):\n    \"\"\"Initialize the shared variables.\"\"\"\n    super().setUp()\n    # Load frames from the test video.\n    cap = cv2.VideoCapture(_VIDEO_FILE)\n    frames = []\n    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n      _, frame = cap.read()",
        "detail": "examples.lite.examples.video_classification.raspberry_pi.video_classifier_test",
        "documentation": {}
    },
    {
        "label": "OrderedDumper",
        "kind": 6,
        "importPath": "examples.lite.tools.build_model_maker_api_docs",
        "description": "examples.lite.tools.build_model_maker_api_docs",
        "peekOfCode": "class OrderedDumper(yaml.dumper.Dumper):\n  pass\ndef _dict_representer(dumper, data):\n  \"\"\"Force yaml to output dictionaries in order, not alphabetically.\"\"\"\n  return dumper.represent_dict(data.items())\nOrderedDumper.add_representer(dict, _dict_representer)\nflags.DEFINE_string('output_dir', '/tmp/mm_api/',\n                    'The path to output the files to')\nflags.DEFINE_string(\n    'code_url_prefix',",
        "detail": "examples.lite.tools.build_model_maker_api_docs",
        "documentation": {}
    },
    {
        "label": "DeprecatedAPIFilter",
        "kind": 6,
        "importPath": "examples.lite.tools.build_model_maker_api_docs",
        "description": "examples.lite.tools.build_model_maker_api_docs",
        "peekOfCode": "class DeprecatedAPIFilter(object):\n  \"\"\"Filter deprecated APIs.\"\"\"\n  def __init__(self):\n    \"\"\"Constructor.\"\"\"\n    self._deprecated = [\n        'tflite_model_maker.configs',\n        'tflite_model_maker.ExportFormat',\n        'tflite_model_maker.ImageClassifierDataLoader',\n    ]\n  def _is_deprecated(self, path: Sequence[str], child_name: str) -> bool:",
        "detail": "examples.lite.tools.build_model_maker_api_docs",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.lite.tools.build_model_maker_api_docs",
        "description": "examples.lite.tools.build_model_maker_api_docs",
        "peekOfCode": "def main(_):\n  doc_generator = generate_lib.DocGenerator(\n      root_title='TensorFlow Lite Model Maker',\n      py_modules=[('tflite_model_maker', tflite_model_maker)],\n      base_dir=[\n          pathlib.Path(tflite_model_maker.__file__).parent,\n          pathlib.Path(tensorflow_examples.__file__).parent,\n      ],\n      code_url_prefix=[\n          FLAGS.code_url_prefix,",
        "detail": "examples.lite.tools.build_model_maker_api_docs",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.lite.tools.build_model_maker_api_docs",
        "description": "examples.lite.tools.build_model_maker_api_docs",
        "peekOfCode": "FLAGS = flags.FLAGS\nclass DeprecatedAPIFilter(object):\n  \"\"\"Filter deprecated APIs.\"\"\"\n  def __init__(self):\n    \"\"\"Constructor.\"\"\"\n    self._deprecated = [\n        'tflite_model_maker.configs',\n        'tflite_model_maker.ExportFormat',\n        'tflite_model_maker.ImageClassifierDataLoader',\n    ]",
        "detail": "examples.lite.tools.build_model_maker_api_docs",
        "documentation": {}
    },
    {
        "label": "FormatDoc",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "description": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "peekOfCode": "class FormatDoc(object):\n  \"\"\"Decorator to format functionn doc with given parameters.\"\"\"\n  def __init__(self, *format_args, **format_kwargs):\n    self.args = format_args\n    self.kwargs = format_kwargs\n  def __call__(self, fn):\n    fn.__doc__ = fn.__doc__.format(*self.args, **self.kwargs)\n    return fn\nclass ModelMakerCLI(object):\n  \"\"\"Model Maker Command Line Interface.",
        "detail": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "documentation": {}
    },
    {
        "label": "ModelMakerCLI",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "description": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "peekOfCode": "class ModelMakerCLI(object):\n  \"\"\"Model Maker Command Line Interface.\n  Flags:\n    --tf: int, version of TF behavior. Valid: [1, 2], default: 2.\n  \"\"\"\n  def __init__(self, tf=2):\n    compat.setup_tf_behavior(tf)\n  @FormatDoc(MODELS=_IMAGE_MODELS)\n  def image_classification(self,\n                           data_dir,",
        "detail": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "description": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "peekOfCode": "def main():\n  fire.Fire(ModelMakerCLI)\nif __name__ == '__main__':\n  main()",
        "detail": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "documentation": {}
    },
    {
        "label": "_IMAGE_MODELS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "description": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "peekOfCode": "_IMAGE_MODELS = model_spec.IMAGE_CLASSIFICATION_MODELS\n_TEXT_MODELS = model_spec.TEXT_CLASSIFICATION_MODELS\n_QA_MODELS = model_spec.QUESTION_ANSWER_MODELS\nclass FormatDoc(object):\n  \"\"\"Decorator to format functionn doc with given parameters.\"\"\"\n  def __init__(self, *format_args, **format_kwargs):\n    self.args = format_args\n    self.kwargs = format_kwargs\n  def __call__(self, fn):\n    fn.__doc__ = fn.__doc__.format(*self.args, **self.kwargs)",
        "detail": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "documentation": {}
    },
    {
        "label": "_TEXT_MODELS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "description": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "peekOfCode": "_TEXT_MODELS = model_spec.TEXT_CLASSIFICATION_MODELS\n_QA_MODELS = model_spec.QUESTION_ANSWER_MODELS\nclass FormatDoc(object):\n  \"\"\"Decorator to format functionn doc with given parameters.\"\"\"\n  def __init__(self, *format_args, **format_kwargs):\n    self.args = format_args\n    self.kwargs = format_kwargs\n  def __call__(self, fn):\n    fn.__doc__ = fn.__doc__.format(*self.args, **self.kwargs)\n    return fn",
        "detail": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "documentation": {}
    },
    {
        "label": "_QA_MODELS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "description": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "peekOfCode": "_QA_MODELS = model_spec.QUESTION_ANSWER_MODELS\nclass FormatDoc(object):\n  \"\"\"Decorator to format functionn doc with given parameters.\"\"\"\n  def __init__(self, *format_args, **format_kwargs):\n    self.args = format_args\n    self.kwargs = format_kwargs\n  def __call__(self, fn):\n    fn.__doc__ = fn.__doc__.format(*self.args, **self.kwargs)\n    return fn\nclass ModelMakerCLI(object):",
        "detail": "examples.tensorflow_examples.lite.model_maker.cli.cli",
        "documentation": {}
    },
    {
        "label": "CLITest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "description": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "peekOfCode": "class CLITest(parameterized.TestCase):\n  @parameterized.parameters(\n      (['--tf=1'], 1),\n      (['--tf=2'], 2),\n      ([], 2),  # No extra flag, default\n  )\n  def test_init(self, tf_opt, expected_tf):\n    sys.argv = [BIN, 'image_classification', 'data', 'export'] + tf_opt\n    with patch_image() as run, patch_setup() as setup:\n      cli.main()",
        "detail": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "documentation": {}
    },
    {
        "label": "patch_image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "description": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "peekOfCode": "def patch_image():\n  \"\"\"Patch image classification demo run.\"\"\"\n  return patch.object(image_classification_demo, 'run')\ndef patch_text():\n  \"\"\"Patch text classification demo run.\"\"\"\n  return patch.object(text_classification_demo, 'run')\ndef patch_qa():\n  \"\"\"Patch question answer demo run.\"\"\"\n  return patch.object(question_answer_demo, 'run')\ndef patch_setup():",
        "detail": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "documentation": {}
    },
    {
        "label": "patch_text",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "description": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "peekOfCode": "def patch_text():\n  \"\"\"Patch text classification demo run.\"\"\"\n  return patch.object(text_classification_demo, 'run')\ndef patch_qa():\n  \"\"\"Patch question answer demo run.\"\"\"\n  return patch.object(question_answer_demo, 'run')\ndef patch_setup():\n  \"\"\"Patch image classification demo run.\"\"\"\n  return patch.object(compat, 'setup_tf_behavior')\nclass CLITest(parameterized.TestCase):",
        "detail": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "documentation": {}
    },
    {
        "label": "patch_qa",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "description": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "peekOfCode": "def patch_qa():\n  \"\"\"Patch question answer demo run.\"\"\"\n  return patch.object(question_answer_demo, 'run')\ndef patch_setup():\n  \"\"\"Patch image classification demo run.\"\"\"\n  return patch.object(compat, 'setup_tf_behavior')\nclass CLITest(parameterized.TestCase):\n  @parameterized.parameters(\n      (['--tf=1'], 1),\n      (['--tf=2'], 2),",
        "detail": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "documentation": {}
    },
    {
        "label": "patch_setup",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "description": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "peekOfCode": "def patch_setup():\n  \"\"\"Patch image classification demo run.\"\"\"\n  return patch.object(compat, 'setup_tf_behavior')\nclass CLITest(parameterized.TestCase):\n  @parameterized.parameters(\n      (['--tf=1'], 1),\n      (['--tf=2'], 2),\n      ([], 2),  # No extra flag, default\n  )\n  def test_init(self, tf_opt, expected_tf):",
        "detail": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "documentation": {}
    },
    {
        "label": "BIN",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "description": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "peekOfCode": "BIN = 'model_maker'  # Whatever binary name.\ndef patch_image():\n  \"\"\"Patch image classification demo run.\"\"\"\n  return patch.object(image_classification_demo, 'run')\ndef patch_text():\n  \"\"\"Patch text classification demo run.\"\"\"\n  return patch.object(text_classification_demo, 'run')\ndef patch_qa():\n  \"\"\"Patch question answer demo run.\"\"\"\n  return patch.object(question_answer_demo, 'run')",
        "detail": "examples.tensorflow_examples.lite.model_maker.cli.cli_test",
        "documentation": {}
    },
    {
        "label": "parse_arguments",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen",
        "peekOfCode": "def parse_arguments():\n  \"\"\"Parse arguments for API gen.\"\"\"\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '-o', '--output_dir', type=str, help='Base dir to output generated APIs.')\n  parser.add_argument(\n      '-v',\n      '--version',\n      type=str,\n      default='0.0.0dev',",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen",
        "documentation": {}
    },
    {
        "label": "load_golden",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen",
        "peekOfCode": "def load_golden(json_file: str) -> Dict[str, Sequence[str]]:\n  \"\"\"Loads Golden APIs.\"\"\"\n  content = _read_golden_text(json_file)\n  return json.loads(content)\ndef run(output_dir: str, base_package: str, version: str) -> None:\n  \"\"\"Runs main.\"\"\"\n  imports = load_golden('golden_api.json')\n  imports_doc = golden_api_doc.DOCS\n  deprecated_imports = deprecated_api.IMPORTS\n  api_util.write_packages(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen",
        "peekOfCode": "def run(output_dir: str, base_package: str, version: str) -> None:\n  \"\"\"Runs main.\"\"\"\n  imports = load_golden('golden_api.json')\n  imports_doc = golden_api_doc.DOCS\n  deprecated_imports = deprecated_api.IMPORTS\n  api_util.write_packages(\n      output_dir,\n      imports,\n      imports_doc,\n      base_package,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen",
        "peekOfCode": "def main() -> None:\n  args = parse_arguments()\n  run(args.output_dir, args.base_package, args.version)\nif __name__ == '__main__':\n  main()",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen",
        "documentation": {}
    },
    {
        "label": "ApiGenTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen_test",
        "peekOfCode": "class ApiGenTest(tf.test.TestCase):\n  def test_golden_api(self):\n    golden = api_gen.load_golden('golden_api.json')\n    imports = api_util.generate_imports()\n    imports_json = json.dumps(imports, indent=2, sort_keys=True)\n    golden_content = api_gen._read_golden_text('golden_api.json')\n    msg = ('Exported APIs do not match `golden_api.json`. Please check it.\\n\\n'\n           'Imports in json format: \\n{}\\n\\n\\n'\n           'Golden file content:\\n{}\\n\\n').format(imports_json, golden_content)\n    self.assertDictEqual(imports, golden, msg)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_gen_test",
        "documentation": {}
    },
    {
        "label": "Symbol",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "class Symbol:\n  \"\"\"Symbol to export.\"\"\"\n  exported_name: str  # Exported name of the symbol\n  exported_parts: List[str]  # Parts after splitting.\n  func: Optional[Callable]  # Function or class.\n  imported_module: str  # Imported module name.\n  imported_name: str  # Imported symbol name.\n  @classmethod\n  def from_callable(cls, exported_name: str, func: Callable) -> 'Symbol':\n    \"\"\"Creates a symbol from a callable (function or class).\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "mm_export",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "class mm_export:  # pylint: disable=invalid-name\n  \"\"\"Exports model maker APIs.\"\"\"\n  def __init__(self, name: str):\n    if name in NAME_TO_SYMBOL:\n      raise ValueError('API already exists: `{}`.'.format(name))\n    self._exported_name = name  # API name.\n  def __call__(self, func: Callable) -> Callable:\n    \"\"\"Exports function or class.\"\"\"\n    NAME_TO_SYMBOL[self._exported_name] = Symbol.from_callable(\n        self._exported_name, func)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "foo",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "def foo(...):\n  ...\n```\nIf a function is assigned to a variable, you can export it by calling\nmm_export explicitly. e.g.:\n```python\ndef get_foo(...):\n  ...\nmm_export('bar.foo')(get_foo)\n```",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "get_foo",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "def get_foo(...):\n  ...\nmm_export('bar.foo')(get_foo)\n```\nExporting a constant.\n```python\nFOO = 1\nmm_export('bar.FOO').export_constant(__name__, 'FOO')\n```\nwhere __name__ gets the current module name, and 'FOO' is the constant.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "split_name",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "def split_name(name: str) -> List[str]:\n  \"\"\"Splits name and returns a list of segments.\n  Args:\n    name: str.\n  Returns:\n    list of str: package segments\n  \"\"\"\n  parts = name.split('.')\n  return list(filter(lambda n: n, parts))\ndef as_package(names: List[str]) -> str:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "as_package",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "def as_package(names: List[str]) -> str:\n  \"\"\"Joins names as a package name.\"\"\"\n  return '.'.join(names)\ndef as_path(names: List[str]) -> str:\n  \"\"\"Joins names as a file path.\"\"\"\n  if names:\n    return os.path.join(*names)\n  else:\n    return ''\ndef _get_module_and_name(func: Callable) -> Tuple[str, str]:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "as_path",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "def as_path(names: List[str]) -> str:\n  \"\"\"Joins names as a file path.\"\"\"\n  if names:\n    return os.path.join(*names)\n  else:\n    return ''\ndef _get_module_and_name(func: Callable) -> Tuple[str, str]:\n  \"\"\"Gets module and name, or raise error if not a function.\"\"\"\n  if not inspect.isfunction(func) and not inspect.isclass(func):\n    raise ValueError('Expect a function or class, but got: {}'.format(func))",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "generate_imports",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "def generate_imports() -> Dict[str, Sequence[str]]:\n  \"\"\"Generates imports.\"\"\"\n  import_dict = collections.defaultdict(set)\n  for _, symbol in NAME_TO_SYMBOL.items():\n    package_name = symbol.get_package_name()\n    import_line = symbol.gen_import()\n    import_dict[package_name].add(import_line)\n    for k, line in symbol.gen_parents_import().items():\n      import_dict[k].update(line)\n  # Add prefix and sort import values.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "generate_package_doc",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "def generate_package_doc(package_name):\n  \"\"\"Generates package doc.\"\"\"\n  return '\"\"\"Generated API for package: {}.\"\"\"'.format(package_name)\ndef write_packages(\n    base_dir: str,\n    imports: Dict[str, Sequence[str]],\n    doc_dict: Dict[str, str],\n    base_package: str,\n    version: str,\n    deprecated_imports: Optional[Dict[str, Sequence[str]]] = None) -> None:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "write_packages",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "def write_packages(\n    base_dir: str,\n    imports: Dict[str, Sequence[str]],\n    doc_dict: Dict[str, str],\n    base_package: str,\n    version: str,\n    deprecated_imports: Optional[Dict[str, Sequence[str]]] = None) -> None:\n  \"\"\"Writes packages as init files.\n  Args:\n    base_dir: str, base directory to write packages.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "make_dirs_or_not",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "def make_dirs_or_not(dirpath: Union[PathOrStrType]):\n  \"\"\"Make dirs if not exists.\"\"\"\n  if not os.path.exists(dirpath):\n    os.makedirs(dirpath)\ndef write_python_file(filepath: PathOrStrType, package_doc: Optional[str],\n                      lines: Optional[Sequence[str]]):\n  \"\"\"Writes python file.\"\"\"\n  with open(filepath, 'w') as f:\n    f.write(LICENSE)\n    if package_doc:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "write_python_file",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "def write_python_file(filepath: PathOrStrType, package_doc: Optional[str],\n                      lines: Optional[Sequence[str]]):\n  \"\"\"Writes python file.\"\"\"\n  with open(filepath, 'w') as f:\n    f.write(LICENSE)\n    if package_doc:\n      f.write(package_doc + '\\n\\n')\n    if lines:\n      for line in lines:\n        f.write(line + '\\n')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "overwrite_version_in_package",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "def overwrite_version_in_package(base_dir: PathOrStrType, version: str):\n  \"\"\"Overwrites __version__ in package.\"\"\"\n  base_init = os.path.join(str(base_dir), '__init__.py')\n  with open(base_init, 'r+') as f:\n    content = f.read()\n    version_regex = re.compile(r'^__version__ = .+$', flags=re.MULTILINE)\n    new_content = version_regex.sub(_version_line(version), content)\n    f.seek(0)\n    f.write(new_content)\n    f.truncate()",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "FOO",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "FOO = 1\nmm_export('bar.FOO').export_constant(__name__, 'FOO')\n```\nwhere __name__ gets the current module name, and 'FOO' is the constant.\n\"\"\"\nimport collections\nfrom collections.abc import Callable  # pylint: disable=g-importing-member\nimport inspect\nimport os\nimport pathlib",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "NAME_TO_SYMBOL",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "NAME_TO_SYMBOL = {}\nLICENSE = \"\"\"# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "LICENSE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "LICENSE = \"\"\"# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "PACKAGE_PLACEHOLDER",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "PACKAGE_PLACEHOLDER = '$PKG'\nROOT_PACKAGE_KEY = ''\n@dataclasses.dataclass\nclass Symbol:\n  \"\"\"Symbol to export.\"\"\"\n  exported_name: str  # Exported name of the symbol\n  exported_parts: List[str]  # Parts after splitting.\n  func: Optional[Callable]  # Function or class.\n  imported_module: str  # Imported module name.\n  imported_name: str  # Imported symbol name.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "ROOT_PACKAGE_KEY",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "ROOT_PACKAGE_KEY = ''\n@dataclasses.dataclass\nclass Symbol:\n  \"\"\"Symbol to export.\"\"\"\n  exported_name: str  # Exported name of the symbol\n  exported_parts: List[str]  # Parts after splitting.\n  func: Optional[Callable]  # Function or class.\n  imported_module: str  # Imported module name.\n  imported_name: str  # Imported symbol name.\n  @classmethod",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "PathOrStrType",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "peekOfCode": "PathOrStrType = Union[pathlib.Path, str]\ndef make_dirs_or_not(dirpath: Union[PathOrStrType]):\n  \"\"\"Make dirs if not exists.\"\"\"\n  if not os.path.exists(dirpath):\n    os.makedirs(dirpath)\ndef write_python_file(filepath: PathOrStrType, package_doc: Optional[str],\n                      lines: Optional[Sequence[str]]):\n  \"\"\"Writes python file.\"\"\"\n  with open(filepath, 'w') as f:\n    f.write(LICENSE)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util",
        "documentation": {}
    },
    {
        "label": "MMExportTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util_test",
        "peekOfCode": "class MMExportTest(tf.test.TestCase):\n  def setUp(self):\n    super(MMExportTest, self).setUp()\n    api_util._reset_apis()\n  def test_call(self):\n    @api_util.mm_export('foo.a')\n    def a():\n      pass\n    self.assertLen(api_util.NAME_TO_SYMBOL, 1)\n    expected = api_util.Symbol('foo.a', ['foo', 'a'], a, '__main__', 'a')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util_test",
        "documentation": {}
    },
    {
        "label": "ApiUtilTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.api_util_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.api_util_test",
        "peekOfCode": "class ApiUtilTest(tf.test.TestCase):\n  def setUp(self):\n    super(ApiUtilTest, self).setUp()\n    api_util._reset_apis()\n  def test_get_module_and_name(self):\n    def a():\n      pass\n    module_and_name = api_util._get_module_and_name(a)\n    self.assertTupleEqual(module_and_name, ('__main__', 'a'))\n    expected = (",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.api_util_test",
        "documentation": {}
    },
    {
        "label": "IMPORTS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.deprecated_api",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.deprecated_api",
        "peekOfCode": "IMPORTS = {}\n# pylint: disable=line-too-long\nIMPORTS[''] = \"\"\"\n# Deprecated imports are kept for backward compatiblity and to be removed in\n# future versions. Please refer to public APIs for replacement:\n# https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker\n# pylint: disable=g-bad-import-order\nfrom tensorflow_examples.lite.model_maker.core.data_util.image_dataloader import ImageClassifierDataLoader\nfrom tensorflow_examples.lite.model_maker.core.export_format import ExportFormat\nfrom tensorflow_examples.lite.model_maker.core.task import configs",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.deprecated_api",
        "documentation": {}
    },
    {
        "label": "IMPORTS['']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.deprecated_api",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.deprecated_api",
        "peekOfCode": "IMPORTS[''] = \"\"\"\n# Deprecated imports are kept for backward compatiblity and to be removed in\n# future versions. Please refer to public APIs for replacement:\n# https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker\n# pylint: disable=g-bad-import-order\nfrom tensorflow_examples.lite.model_maker.core.data_util.image_dataloader import ImageClassifierDataLoader\nfrom tensorflow_examples.lite.model_maker.core.export_format import ExportFormat\nfrom tensorflow_examples.lite.model_maker.core.task import configs\n# pylint: enable=g-bad-import-order\n\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.deprecated_api",
        "documentation": {}
    },
    {
        "label": "DOCS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "DOCS = {}\n# pylint: disable=line-too-long\nDOCS[''] = \"\"\"\nPublic APIs for TFLite Model Maker, a transfer learning library to train custom TFLite models.\nYou can install the package with\n```bash\npip install tflite-model-maker\n```\nTypical usage of Model Maker is to create a model in a few lines of code, e.g.:\n```python",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "DOCS['']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "DOCS[''] = \"\"\"\nPublic APIs for TFLite Model Maker, a transfer learning library to train custom TFLite models.\nYou can install the package with\n```bash\npip install tflite-model-maker\n```\nTypical usage of Model Maker is to create a model in a few lines of code, e.g.:\n```python\n# Load input data specific to an on-device ML app.\ndata = DataLoader.from_folder('flower_photos/')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "data = DataLoader.from_folder('flower_photos/')\ntrain_data, test_data = data.split(0.9)\n# Customize the TensorFlow model.\nmodel = image_classifier.create(train_data)\n# Evaluate the model.\naccuracy = model.evaluate(test_data)\n# Export to Tensorflow Lite model and label file in `export_dir`.\nmodel.export(export_dir='/tmp/')\n```\nFor more details, please refer to our guide:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "model = image_classifier.create(train_data)\n# Evaluate the model.\naccuracy = model.evaluate(test_data)\n# Export to Tensorflow Lite model and label file in `export_dir`.\nmodel.export(export_dir='/tmp/')\n```\nFor more details, please refer to our guide:\nhttps://www.tensorflow.org/lite/guide/model_maker.\n\"\"\".lstrip()\nDOCS['audio_classifier'] = \"\"\"APIs to train an audio classification model.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "accuracy = model.evaluate(test_data)\n# Export to Tensorflow Lite model and label file in `export_dir`.\nmodel.export(export_dir='/tmp/')\n```\nFor more details, please refer to our guide:\nhttps://www.tensorflow.org/lite/guide/model_maker.\n\"\"\".lstrip()\nDOCS['audio_classifier'] = \"\"\"APIs to train an audio classification model.\nTutorial:\nhttps://colab.research.google.com/github/googlecodelabs/odml-pathways/blob/main/audio_classification/colab/model_maker_audio_colab.ipynb",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "DOCS['audio_classifier']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "DOCS['audio_classifier'] = \"\"\"APIs to train an audio classification model.\nTutorial:\nhttps://colab.research.google.com/github/googlecodelabs/odml-pathways/blob/main/audio_classification/colab/model_maker_audio_colab.ipynb\nDemo code:\nhttps://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/demo/audio_classification_demo.py\n\"\"\"\nDOCS['config'] = 'APIs for the config of TFLite Model Maker.'\nDOCS['image_classifier'] = \"\"\"APIs to train an image classification model.\nTask guide:\nhttps://www.tensorflow.org/lite/tutorials/model_maker_image_classification.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "DOCS['config']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "DOCS['config'] = 'APIs for the config of TFLite Model Maker.'\nDOCS['image_classifier'] = \"\"\"APIs to train an image classification model.\nTask guide:\nhttps://www.tensorflow.org/lite/tutorials/model_maker_image_classification.\n\"\"\"\nDOCS['model_spec'] = 'APIs for the model spec of TFLite Model Maker.'\nDOCS['object_detector'] = 'APIs to train an object detection model.'\nDOCS['question_answer'] = \"\"\"\nAPIs to train a model that can answer questions based on a predefined text.\nTask guide:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "DOCS['image_classifier']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "DOCS['image_classifier'] = \"\"\"APIs to train an image classification model.\nTask guide:\nhttps://www.tensorflow.org/lite/tutorials/model_maker_image_classification.\n\"\"\"\nDOCS['model_spec'] = 'APIs for the model spec of TFLite Model Maker.'\nDOCS['object_detector'] = 'APIs to train an object detection model.'\nDOCS['question_answer'] = \"\"\"\nAPIs to train a model that can answer questions based on a predefined text.\nTask guide:\nhttps://www.tensorflow.org/lite/tutorials/model_maker_question_answer.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "DOCS['model_spec']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "DOCS['model_spec'] = 'APIs for the model spec of TFLite Model Maker.'\nDOCS['object_detector'] = 'APIs to train an object detection model.'\nDOCS['question_answer'] = \"\"\"\nAPIs to train a model that can answer questions based on a predefined text.\nTask guide:\nhttps://www.tensorflow.org/lite/tutorials/model_maker_question_answer.\n\"\"\".lstrip()\nDOCS['recommendation'] = \"\"\"APIs to train an on-device recommendation model.\nDemo code:\nhttps://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/demo/recommendation_demo.py",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "DOCS['object_detector']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "DOCS['object_detector'] = 'APIs to train an object detection model.'\nDOCS['question_answer'] = \"\"\"\nAPIs to train a model that can answer questions based on a predefined text.\nTask guide:\nhttps://www.tensorflow.org/lite/tutorials/model_maker_question_answer.\n\"\"\".lstrip()\nDOCS['recommendation'] = \"\"\"APIs to train an on-device recommendation model.\nDemo code:\nhttps://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/demo/recommendation_demo.py\n\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "DOCS['question_answer']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "DOCS['question_answer'] = \"\"\"\nAPIs to train a model that can answer questions based on a predefined text.\nTask guide:\nhttps://www.tensorflow.org/lite/tutorials/model_maker_question_answer.\n\"\"\".lstrip()\nDOCS['recommendation'] = \"\"\"APIs to train an on-device recommendation model.\nDemo code:\nhttps://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/demo/recommendation_demo.py\n\"\"\"\nDOCS['recommendation.spec'] = \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "DOCS['recommendation']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "DOCS['recommendation'] = \"\"\"APIs to train an on-device recommendation model.\nDemo code:\nhttps://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/demo/recommendation_demo.py\n\"\"\"\nDOCS['recommendation.spec'] = \"\"\"\nAPIs for recommendation specifications.\nExample:\n```python\ninput_spec = recommendation.spec.InputSpec(\n    activity_feature_groups=[",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "DOCS['recommendation.spec']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "DOCS['recommendation.spec'] = \"\"\"\nAPIs for recommendation specifications.\nExample:\n```python\ninput_spec = recommendation.spec.InputSpec(\n    activity_feature_groups=[\n        # Group #1: defines how features are grouped in the first Group.\n        dict(\n            features=[\n                # First feature.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "input_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "input_spec = recommendation.spec.InputSpec(\n    activity_feature_groups=[\n        # Group #1: defines how features are grouped in the first Group.\n        dict(\n            features=[\n                # First feature.\n                dict(\n                    feature_name='context_movie_id',  # Feature name\n                    feature_type='INT',  # Feature type\n                    vocab_size=3953,     # ID size (number of IDs)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "model_hparams",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "model_hparams = recommendation.spec.ModelHParams(\n    hidden_layer_dims=[32, 32],  # Hidden layers dimension.\n    eval_top_k=[1, 5],           # Eval top 1 and top 5.\n    conv_num_filter_ratios=[2, 4],  # For CNN encoder, conv filter mutipler.\n    conv_kernel_size=16,            # For CNN encoder, base kernel size.\n    lstm_num_units=16,              # For LSTM/RNN, num units.\n    num_predictions=10,          # Number of output predictions. Select top 10.\n)\nspec = recommendation.ModelSpec(\n    input_spec=input_spec, model_hparams=model_hparams)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "spec = recommendation.ModelSpec(\n    input_spec=input_spec, model_hparams=model_hparams)\n# Or:\nspec = model_spec.get(\n    'recommendation', input_spec=input_spec, model_hparams=model_hparams)\n```\n\"\"\".lstrip()\nDOCS['searcher'] = \"\"\"APIs to create the searcher model.\nTask guide:\nhttps://www.tensorflow.org/lite/tutorials/model_maker_text_searcher.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "spec = model_spec.get(\n    'recommendation', input_spec=input_spec, model_hparams=model_hparams)\n```\n\"\"\".lstrip()\nDOCS['searcher'] = \"\"\"APIs to create the searcher model.\nTask guide:\nhttps://www.tensorflow.org/lite/tutorials/model_maker_text_searcher.\n\"\"\"\nDOCS['text_classifier'] = \"\"\"APIs to train a text classification model.\nTask guide:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "DOCS['searcher']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "DOCS['searcher'] = \"\"\"APIs to create the searcher model.\nTask guide:\nhttps://www.tensorflow.org/lite/tutorials/model_maker_text_searcher.\n\"\"\"\nDOCS['text_classifier'] = \"\"\"APIs to train a text classification model.\nTask guide:\nhttps://www.tensorflow.org/lite/tutorials/model_maker_text_classification.\n\"\"\"\n# pylint: enable=line-too-long",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "DOCS['text_classifier']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "description": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "peekOfCode": "DOCS['text_classifier'] = \"\"\"APIs to train a text classification model.\nTask guide:\nhttps://www.tensorflow.org/lite/tutorials/model_maker_text_classification.\n\"\"\"\n# pylint: enable=line-too-long",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.api.golden_api_doc",
        "documentation": {}
    },
    {
        "label": "ExamplesHelper",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader",
        "peekOfCode": "class ExamplesHelper(object):\n  \"\"\"Helper class for matching examples and labels.\"\"\"\n  @classmethod\n  def from_examples_folder(cls, path, examples_filter_fn):\n    \"\"\"Helper function for loading examples and parsing labels from example path.\n    This path contain a number of folders, each named by the category name. Each\n    folder contain a number of files. This helper class loads and parse the\n    tree structure.\n    Example folder:\n      /category1",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader",
        "peekOfCode": "class DataLoader(dataloader.ClassificationDataLoader):\n  \"\"\"DataLoader for audio tasks.\"\"\"\n  def __init__(self, dataset, size, index_to_label, spec, cache=False):\n    super(DataLoader, self).__init__(dataset, size, index_to_label)\n    self._spec = spec\n    self._cache = cache\n  def __len__(self):\n    \"\"\"Returns the number of audio files in the DataLoader.\n    Note that one audio file could be framed (mostly via a sliding window of\n    fixed size) into None or multiple audio clips during training and",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader",
        "documentation": {}
    },
    {
        "label": "error_import_librosa",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader",
        "peekOfCode": "error_import_librosa = None\ntry:\n  import librosa  # pylint: disable=g-import-not-at-top\n  ENABLE_RESAMPLE = True\nexcept (OSError, ImportError) as _error_import_librosa:  # pylint: disable=invalid-name\n  ENABLE_RESAMPLE = False\n  error_import_librosa = _error_import_librosa\nclass ExamplesHelper(object):\n  \"\"\"Helper class for matching examples and labels.\"\"\"\n  @classmethod",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader",
        "documentation": {}
    },
    {
        "label": "MockSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "peekOfCode": "class MockSpec(audio_spec.BaseSpec):\n  def __init__(self, *args, **kwargs):\n    super(MockSpec, self).__init__(*args, **kwargs)\n    self.expected_waveform_len = 44100\n  def create_model(self):\n    return None\n  def run_classifier(self, *args, **kwargs):\n    return None\n  @property\n  def target_sample_rate(self):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "documentation": {}
    },
    {
        "label": "Base",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "peekOfCode": "class Base(tf.test.TestCase):\n  def _get_folder_path(self, sub_folder_name):\n    folder_path = os.path.join(self.get_temp_dir(), sub_folder_name)\n    if os.path.exists(folder_path):\n      shutil.rmtree(folder_path)\n    tf.compat.v1.logging.info('Test path: %s', folder_path)\n    os.mkdir(folder_path)\n    return folder_path\n@unittest.skipIf(\n    version.parse(tf.__version__) < version.parse('2.5'),",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "documentation": {}
    },
    {
        "label": "LoadFromESC50Test",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "peekOfCode": "class LoadFromESC50Test(Base):\n  def test_from_esc50(self):\n    folder_path = self._get_folder_path('test_from_esc50')\n    headers = [\n        'filename', 'fold', 'target', 'category', 'esc10', 'src_file', 'take'\n    ]\n    rows = []\n    rows.append(['1-100032-A-0.wav', '1', '0', 'dog', 'True', '100032', 'A'])\n    rows.append([\n        '1-100210-B-36.wav', '2', '36', 'vacuum_cleaner', 'False', '100210', 'B'",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "documentation": {}
    },
    {
        "label": "ExamplesHelperTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "peekOfCode": "class ExamplesHelperTest(Base):\n  def test_examples_helper(self):\n    root = self._get_folder_path('test_examples_helper')\n    write_file(root, 'a/1.wav')\n    write_file(root, 'a/2.wav')\n    write_file(root, 'b/1.wav')\n    write_file(root, 'b/README')  # Ignored\n    write_file(root, 'a/b/c/d.wav')  # Ignored\n    write_file(root, 'AUTHORS.md')  # Ignored\n    write_file(root, 'temp.wav')  # Ignored",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "documentation": {}
    },
    {
        "label": "LoadFromFolderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "peekOfCode": "class LoadFromFolderTest(Base):\n  def test_spec(self):\n    folder_path = self._get_folder_path('test_spec')\n    write_sample(folder_path, 'unknown', '2s.wav', 44100, 2, value=1)\n    spec = audio_spec.YAMNetSpec()\n    audio_dataloader.DataLoader.from_folder(spec, folder_path)\n    spec = audio_spec.BrowserFFTSpec()\n    audio_dataloader.DataLoader.from_folder(spec, folder_path)\n  def test_no_audio_files_found(self):\n    folder_path = self._get_folder_path('test_no_audio_files_found')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "documentation": {}
    },
    {
        "label": "write_file",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "peekOfCode": "def write_file(root, filepath):\n  full_path = os.path.join(root, filepath)\n  os.makedirs(os.path.dirname(full_path), exist_ok=True)\n  with open(full_path, 'w') as f:\n    f.write('<content>')\ndef write_sample(root,\n                 category,\n                 file_name,\n                 sample_rate,\n                 duration_sec,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "documentation": {}
    },
    {
        "label": "write_sample",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "peekOfCode": "def write_sample(root,\n                 category,\n                 file_name,\n                 sample_rate,\n                 duration_sec,\n                 value,\n                 dtype=np.int16):\n  os.makedirs(os.path.join(root, category), exist_ok=True)\n  xs = value * np.ones(shape=(int(sample_rate * duration_sec),), dtype=dtype)\n  wavfile.write(os.path.join(root, category, file_name), sample_rate, xs)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "documentation": {}
    },
    {
        "label": "write_csv",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "peekOfCode": "def write_csv(root, folder, filename, headers, rows):\n  os.makedirs(os.path.join(root, folder), exist_ok=True)\n  with open(os.path.join(root, folder, filename), 'w') as f:\n    writer = csv.DictWriter(f, fieldnames=headers)\n    writer.writeheader()\n    for row in rows:\n      writer.writerow(dict(zip(headers, row)))\nclass MockSpec(audio_spec.BaseSpec):\n  def __init__(self, *args, **kwargs):\n    super(MockSpec, self).__init__(*args, **kwargs)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.audio_dataloader_test",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader",
        "peekOfCode": "class DataLoader(object):\n  \"\"\"This class provides generic utilities for loading customized domain data that will be used later in model retraining.\n  For different ML problems or tasks, such as image classification, text\n  classification etc., a subclass is provided to handle task-specific data\n  loading requirements.\n  \"\"\"\n  def __init__(self, dataset, size=None):\n    \"\"\"Init function for class `DataLoader`.\n    In most cases, one should use helper functions like `from_folder` to create\n    an instance of this class.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader",
        "documentation": {}
    },
    {
        "label": "ClassificationDataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader",
        "peekOfCode": "class ClassificationDataLoader(DataLoader):\n  \"\"\"DataLoader for classification models.\"\"\"\n  def __init__(self, dataset, size, index_to_label):\n    super(ClassificationDataLoader, self).__init__(dataset, size)\n    self.index_to_label = index_to_label\n  @property\n  def num_classes(self):\n    return len(self.index_to_label)\n  def split(self, fraction):\n    \"\"\"Splits dataset into two sub-datasets with the given fraction.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader",
        "documentation": {}
    },
    {
        "label": "shard",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader",
        "peekOfCode": "def shard(ds, input_pipeline_context):\n  # The dataset is always sharded by number of hosts.\n  # num_input_pipelines is the number of hosts rather than number of cores.\n  if (input_pipeline_context and\n      input_pipeline_context.num_input_pipelines > 1):\n    ds = ds.shard(input_pipeline_context.num_input_pipelines,\n                  input_pipeline_context.input_pipeline_id)\n  return ds\nclass DataLoader(object):\n  \"\"\"This class provides generic utilities for loading customized domain data that will be used later in model retraining.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader",
        "documentation": {}
    },
    {
        "label": "DataLoaderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader_test",
        "peekOfCode": "class DataLoaderTest(tf.test.TestCase):\n  def test_split(self):\n    ds = tf.data.Dataset.from_tensor_slices([[0, 1], [1, 1], [0, 0], [1, 0]])\n    data = dataloader.DataLoader(ds, 4)\n    train_data, test_data = data.split(0.5)\n    self.assertEqual(len(train_data), 2)\n    self.assertIsInstance(train_data, dataloader.DataLoader)\n    self.assertIsInstance(test_data, dataloader.DataLoader)\n    for i, elem in enumerate(train_data.gen_dataset()):\n      self.assertTrue((elem.numpy() == np.array([i, 1])).all())",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader_test",
        "documentation": {}
    },
    {
        "label": "ClassificationDataLoaderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader_test",
        "peekOfCode": "class ClassificationDataLoaderTest(tf.test.TestCase):\n  def test_split(self):\n    class MagicClassificationDataLoader(dataloader.ClassificationDataLoader):\n      def __init__(self, dataset, size, index_to_label, value):\n        super(MagicClassificationDataLoader,\n              self).__init__(dataset, size, index_to_label)\n        self.value = value\n      def split(self, fraction):\n        return self._split(fraction, self.index_to_label, self.value)\n    # Some dummy inputs.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.dataloader_test",
        "documentation": {}
    },
    {
        "label": "generate_elements",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.data_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.data_util",
        "peekOfCode": "def generate_elements(ds):\n  \"\"\"Generates elements from `tf.data.dataset`.\"\"\"\n  if compat.get_tf_behavior() == 2:\n    for element in ds.as_numpy_iterator():\n      yield element\n  else:\n    iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n    next_element = iterator.get_next()\n    with tf.compat.v1.Session() as sess:\n      try:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.data_util",
        "documentation": {}
    },
    {
        "label": "ImageClassifierDataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "peekOfCode": "class ImageClassifierDataLoader(dataloader.ClassificationDataLoader):\n  \"\"\"DataLoader for image classifier.\"\"\"\n  @classmethod\n  def from_folder(cls, filename, shuffle=True):\n    \"\"\"Image analysis for image classification load images with labels.\n    Assume the image data of the same label are in the same subdirectory.\n    Args:\n      filename: Name of the file.\n      shuffle: boolean, if shuffle, random shuffle data.\n    Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "documentation": {}
    },
    {
        "label": "load_image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "peekOfCode": "def load_image(path):\n  \"\"\"Loads image.\"\"\"\n  image_raw = tf.io.read_file(path)\n  image_tensor = tf.cond(\n      tf.image.is_jpeg(image_raw),\n      lambda: tf.image.decode_jpeg(image_raw, channels=3),\n      lambda: tf.image.decode_png(image_raw, channels=3))\n  return image_tensor\ndef create_data(name, data, info, label_names):\n  \"\"\"Creates an ImageClassifierDataLoader object from tfds data.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "documentation": {}
    },
    {
        "label": "create_data",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "peekOfCode": "def create_data(name, data, info, label_names):\n  \"\"\"Creates an ImageClassifierDataLoader object from tfds data.\"\"\"\n  if name not in data:\n    return None\n  data = data[name]\n  data = data.map(lambda a: (a['image'], a['label']))\n  size = info.splits[name].num_examples\n  return ImageClassifierDataLoader(data, size, label_names)\n@mm_export('image_classifier.DataLoader')\nclass ImageClassifierDataLoader(dataloader.ClassificationDataLoader):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_dataloader",
        "documentation": {}
    },
    {
        "label": "ImageDataLoaderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_dataloader_test",
        "peekOfCode": "class ImageDataLoaderTest(tf.test.TestCase):\n  def setUp(self):\n    super(ImageDataLoaderTest, self).setUp()\n    self.image_path = os.path.join(self.get_temp_dir(), 'random_image_dir')\n    if os.path.exists(self.image_path):\n      return\n    os.mkdir(self.image_path)\n    for class_name in ('daisy', 'tulips'):\n      class_subdir = os.path.join(self.image_path, class_name)\n      os.mkdir(class_subdir)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_dataloader_test",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_searcher_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_searcher_dataloader",
        "peekOfCode": "class DataLoader(searcher_dataloader.DataLoader):\n  \"\"\"DataLoader class for Image Searcher Task.\"\"\"\n  def __init__(\n      self,\n      embedder: image_embedder.ImageEmbedder,\n      metadata_type: _MetadataType = _MetadataType.FROM_FILE_NAME) -> None:\n    \"\"\"Initializes DataLoader for Image Searcher task.\n    Args:\n      embedder: Embedder to generate embedding from raw input image.\n      metadata_type: Type of MetadataLoader to load metadata for each input",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_searcher_dataloader",
        "documentation": {}
    },
    {
        "label": "_MetadataType",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_searcher_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_searcher_dataloader",
        "peekOfCode": "_MetadataType = metadata_loader.MetadataType\n_BaseOptions = base_options_module.BaseOptions\n@mm_export(\"searcher.ImageDataLoader\")\nclass DataLoader(searcher_dataloader.DataLoader):\n  \"\"\"DataLoader class for Image Searcher Task.\"\"\"\n  def __init__(\n      self,\n      embedder: image_embedder.ImageEmbedder,\n      metadata_type: _MetadataType = _MetadataType.FROM_FILE_NAME) -> None:\n    \"\"\"Initializes DataLoader for Image Searcher task.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_searcher_dataloader",
        "documentation": {}
    },
    {
        "label": "_BaseOptions",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_searcher_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_searcher_dataloader",
        "peekOfCode": "_BaseOptions = base_options_module.BaseOptions\n@mm_export(\"searcher.ImageDataLoader\")\nclass DataLoader(searcher_dataloader.DataLoader):\n  \"\"\"DataLoader class for Image Searcher Task.\"\"\"\n  def __init__(\n      self,\n      embedder: image_embedder.ImageEmbedder,\n      metadata_type: _MetadataType = _MetadataType.FROM_FILE_NAME) -> None:\n    \"\"\"Initializes DataLoader for Image Searcher task.\n    Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_searcher_dataloader",
        "documentation": {}
    },
    {
        "label": "ImageSearcherDataloaderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_searcher_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_searcher_dataloader_test",
        "peekOfCode": "class ImageSearcherDataloaderTest(parameterized.TestCase, tf.test.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.tflite_path = test_util.get_test_data_path(\n        \"mobilenet_v2_035_96_embedder_with_metadata.tflite\")\n    self.image_dir1 = test_util.get_test_data_path(\"food\")\n    self.image_dir2 = test_util.get_test_data_path(\"animals\")\n  @parameterized.parameters(\n      (False, 1.335398),\n      (True, 0.0494329),",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.image_searcher_dataloader_test",
        "documentation": {}
    },
    {
        "label": "MetadataType",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.metadata_loader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.metadata_loader",
        "peekOfCode": "class MetadataType(enum.Enum):\n  FROM_FILE_NAME = 1\n  FROM_DAT_FILE = 2\nclass MetadataLoader(object):\n  \"\"\"MetadataLoader class to load metadata for each input data.\"\"\"\n  def __init__(self, func: Callable[..., AnyStr]) -> None:\n    self._func = func\n  def load(self, path: str, **kwargs) -> AnyStr:\n    \"\"\"Loads metadata from input data path.\"\"\"\n    return self._func(path, **kwargs)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.metadata_loader",
        "documentation": {}
    },
    {
        "label": "MetadataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.metadata_loader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.metadata_loader",
        "peekOfCode": "class MetadataLoader(object):\n  \"\"\"MetadataLoader class to load metadata for each input data.\"\"\"\n  def __init__(self, func: Callable[..., AnyStr]) -> None:\n    self._func = func\n  def load(self, path: str, **kwargs) -> AnyStr:\n    \"\"\"Loads metadata from input data path.\"\"\"\n    return self._func(path, **kwargs)\n  @classmethod\n  def from_file_name(cls) -> \"MetadataLoader\":\n    \"\"\"Loads the file name as metadata.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.metadata_loader",
        "documentation": {}
    },
    {
        "label": "MetadataLoaderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.metadata_loader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.metadata_loader_test",
        "peekOfCode": "class MetadataLoaderTest(tf.test.TestCase):\n  def test_from_file_name(self):\n    loader = metadata_loader.MetadataLoader.from_file_name()\n    metadata = loader.load(\"/tmp/searcher_dataset/burger.jpg\")\n    self.assertEqual(metadata, \"burger\")\n  def test_from_dat_file(self):\n    loader = metadata_loader.MetadataLoader.from_dat_file()\n    expected_sparrow_metadata = \"{ 'type': 'sparrow', 'class': 'Aves'}\"\n    sparrow_metadata = loader.load(test_util.get_test_data_path(\"sparrow.png\"))\n    self.assertEqual(sparrow_metadata, expected_sparrow_metadata)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.metadata_loader_test",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader",
        "peekOfCode": "class DataLoader(dataloader.DataLoader):\n  \"\"\"DataLoader for object detector.\"\"\"\n  def __init__(self,\n               tfrecord_file_patten,\n               size,\n               label_map,\n               annotations_json_file=None):\n    \"\"\"Initialize DataLoader for object detector.\n    Args:\n      tfrecord_file_patten: Glob for tfrecord files. e.g. \"/tmp/coco*.tfrecord\".",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader",
        "documentation": {}
    },
    {
        "label": "DetectorDataLoader",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader",
        "peekOfCode": "DetectorDataLoader = TypeVar('DetectorDataLoader', bound='DataLoader')\n# Csv lines with the label map.\nCsvLines = Tuple[List[List[List[str]]], Dict[int, str]]\ndef _get_label_map(label_map):\n  \"\"\"Gets the label map dict.\"\"\"\n  if isinstance(label_map, list):\n    label_map_dict = {}\n    for i, label in enumerate(label_map):\n      # 0 is resevered for background.\n      label_map_dict[i + 1] = label",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader",
        "documentation": {}
    },
    {
        "label": "CsvLines",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader",
        "peekOfCode": "CsvLines = Tuple[List[List[List[str]]], Dict[int, str]]\ndef _get_label_map(label_map):\n  \"\"\"Gets the label map dict.\"\"\"\n  if isinstance(label_map, list):\n    label_map_dict = {}\n    for i, label in enumerate(label_map):\n      # 0 is resevered for background.\n      label_map_dict[i + 1] = label\n    label_map = label_map_dict\n  label_map = label_util.get_label_map(label_map)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader",
        "documentation": {}
    },
    {
        "label": "MockDetectorModelSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_test",
        "peekOfCode": "class MockDetectorModelSpec(object):\n  def __init__(self, model_name):\n    self.model_name = model_name\n    config = hparams_config.get_detection_config(model_name)\n    config.image_size = utils.parse_image_size(config.image_size)\n    config.update({'debug': False})\n    self.config = config\nclass ObjectDectectorDataLoaderTest(tf.test.TestCase):\n  def test_from_pascal_voc(self):\n    images_dir, annotations_dir, label_map = test_util.create_pascal_voc(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_test",
        "documentation": {}
    },
    {
        "label": "ObjectDectectorDataLoaderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_test",
        "peekOfCode": "class ObjectDectectorDataLoaderTest(tf.test.TestCase):\n  def test_from_pascal_voc(self):\n    images_dir, annotations_dir, label_map = test_util.create_pascal_voc(\n        self.get_temp_dir())\n    model_spec = MockDetectorModelSpec('efficientdet-lite0')\n    data = object_detector_dataloader.DataLoader.from_pascal_voc(\n        images_dir, annotations_dir, label_map)\n    self.assertIsInstance(data, object_detector_dataloader.DataLoader)\n    self.assertLen(data, 1)\n    self.assertEqual(data.label_map, label_map)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_test",
        "documentation": {}
    },
    {
        "label": "CacheFiles",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "class CacheFiles:\n  \"\"\"Cache files for object detection.\"\"\"\n  cache_prefix: str\n  tfrecord_files: Sequence[str]\n  meta_data_file: str\n  annotations_json_file: Optional[str]\ndef get_cache_files(cache_dir: Optional[str],\n                    cache_prefix_filename: str,\n                    num_shards: int = 10) -> CacheFiles:\n  \"\"\"Creates an object of CacheFiles class.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "CacheFilesWriter",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "class CacheFilesWriter(abc.ABC):\n  \"\"\"CacheFilesWriter class to write the cached files.\"\"\"\n  def __init__(self,\n               label_map: Dict[int, str],\n               images_dir: Optional[str],\n               num_shards: int = 10,\n               max_num_images: Optional[int] = None,\n               ignore_difficult_instances: bool = False) -> None:\n    \"\"\"Initializes CacheFilesWriter for object detector.\n    Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "PascalVocCacheFilesWriter",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "class PascalVocCacheFilesWriter(CacheFilesWriter):\n  \"\"\"CacheFilesWriter class to write the cached files for Pascal Voc data.\"\"\"\n  def _get_xml_dict(\n      self,\n      annotations_dir: str,\n      annotation_filenames: Optional[List[str]] = None) -> tf.train.Example:\n    \"\"\"Gets the tf example one by one from data with Pascal Voc format.\n    Args:\n      annotations_dir: Path to the annotations directory.\n      annotation_filenames: Collection of annotation filenames (strings) to be",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "CsvCacheFilesWriter",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "class CsvCacheFilesWriter(CacheFilesWriter):\n  \"\"\"DataLoader utilities to write the cached files for the csv file.\"\"\"\n  def _get_xml_dict(self, csv_lines: List[List[str]]) -> tf.train.Example:\n    \"\"\"Gets the tf example one by one from data with Pascal Voc format.\n    Args:\n      csv_lines: Lines in the csv files.\n    Yields:\n      tf.train.Example\n    \"\"\"\n    image_dict = {}",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "get_cache_prefix_filename_from_pascal",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "def get_cache_prefix_filename_from_pascal(images_dir: str,\n                                          annotations_dir: str,\n                                          annotation_filenames: Optional[\n                                              Collection[str]],\n                                          num_shards: int = 10) -> str:\n  \"\"\"Gets the prefix of cached files from PASCAL VOC data.\n  Args:\n    images_dir: Path to directory that store raw images.\n    annotations_dir: Path to the annotations directory.\n    annotation_filenames: Collection of annotation filenames (strings) to be",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "get_cache_prefix_filename_from_csv",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "def get_cache_prefix_filename_from_csv(csv_file: str, num_shards: int) -> str:\n  \"\"\"Gets the prefix of cached files from the csv file.\n  Args:\n    csv_file: Name of the csv file.\n    num_shards: Number of shards for output file.\n  Returns:\n    The prefix of cached files.\n  \"\"\"\n  hasher = hashlib.md5()\n  hasher.update(os.path.basename(csv_file).encode('utf-8'))",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "get_cache_files",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "def get_cache_files(cache_dir: Optional[str],\n                    cache_prefix_filename: str,\n                    num_shards: int = 10) -> CacheFiles:\n  \"\"\"Creates an object of CacheFiles class.\n  Args:\n    cache_dir: The cache directory to save TFRecord, metadata and json file.\n      When cache_dir is None, a temporary folder will be created and will not be\n      removed automatically after training which makes it can be used later.\n     cache_prefix_filename: The cache prefix filename.\n     num_shards: Number of shards for output file.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "is_cached",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "def is_cached(cache_files: CacheFiles) -> bool:\n  \"\"\"Checks whether cache files are already cached.\"\"\"\n  # annotations_json_file is optional, thus we don't check whether it is cached.\n  all_cached_files = list(\n      cache_files.tfrecord_files) + [cache_files.meta_data_file]\n  return all(tf.io.gfile.exists(path) for path in all_cached_files)\ndef is_all_cached(cache_files_collection: Collection[CacheFiles]) -> bool:\n  \"\"\"Checks whether a collection of cache files are all already cached.\"\"\"\n  return all(map(is_cached, cache_files_collection))\ndef get_cache_files_sequence(cache_dir: str, cache_prefix_filename: str,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "is_all_cached",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "def is_all_cached(cache_files_collection: Collection[CacheFiles]) -> bool:\n  \"\"\"Checks whether a collection of cache files are all already cached.\"\"\"\n  return all(map(is_cached, cache_files_collection))\ndef get_cache_files_sequence(cache_dir: str, cache_prefix_filename: str,\n                             set_prefixes: Collection[str],\n                             num_shards: int) -> Sequence[CacheFiles]:\n  \"\"\"Gets a sequence of cache files.\n  Args:\n    cache_dir: The cache directory to save TFRecord, metadata and json file.\n      When cache_dir is None, a temporary folder will be created and will not be",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "get_cache_files_sequence",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "def get_cache_files_sequence(cache_dir: str, cache_prefix_filename: str,\n                             set_prefixes: Collection[str],\n                             num_shards: int) -> Sequence[CacheFiles]:\n  \"\"\"Gets a sequence of cache files.\n  Args:\n    cache_dir: The cache directory to save TFRecord, metadata and json file.\n      When cache_dir is None, a temporary folder will be created and will not be\n      removed automatically after training which makes it can be used later.\n      cache_prefix_filename: The cache prefix filename.\n    set_prefixes: Set prefix names for training, validation and test data. e.g.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "JpegImagePlugin._getmp",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "JpegImagePlugin._getmp = lambda: None  # pylint: disable=protected-access\n# Suffix of the annotations json file name and the meta data file name.\nANN_JSON_FILE_SUFFIX = '_annotations.json'\nMETA_DATA_FILE_SUFFIX = '_meta_data.yaml'\ndef _get_cache_dir_or_create(cache_dir: Optional[str]) -> str:\n  \"\"\"Gets the cache directory or creates it if not exists.\"\"\"\n  # TODO(b/183683348): Unifies with other tasks as well.\n  # If `cache_dir` is None, a temporary folder will be created and will not be\n  # removed automatically after training which makes it can be used later.\n  if cache_dir is None:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "ANN_JSON_FILE_SUFFIX",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "ANN_JSON_FILE_SUFFIX = '_annotations.json'\nMETA_DATA_FILE_SUFFIX = '_meta_data.yaml'\ndef _get_cache_dir_or_create(cache_dir: Optional[str]) -> str:\n  \"\"\"Gets the cache directory or creates it if not exists.\"\"\"\n  # TODO(b/183683348): Unifies with other tasks as well.\n  # If `cache_dir` is None, a temporary folder will be created and will not be\n  # removed automatically after training which makes it can be used later.\n  if cache_dir is None:\n    cache_dir = tempfile.mkdtemp()\n  if not tf.io.gfile.exists(cache_dir):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "META_DATA_FILE_SUFFIX",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "peekOfCode": "META_DATA_FILE_SUFFIX = '_meta_data.yaml'\ndef _get_cache_dir_or_create(cache_dir: Optional[str]) -> str:\n  \"\"\"Gets the cache directory or creates it if not exists.\"\"\"\n  # TODO(b/183683348): Unifies with other tasks as well.\n  # If `cache_dir` is None, a temporary folder will be created and will not be\n  # removed automatically after training which makes it can be used later.\n  if cache_dir is None:\n    cache_dir = tempfile.mkdtemp()\n  if not tf.io.gfile.exists(cache_dir):\n    tf.io.gfile.makedirs(cache_dir)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util",
        "documentation": {}
    },
    {
        "label": "CacheFilesTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util_test",
        "peekOfCode": "class CacheFilesTest(tf.test.TestCase):\n  def test_get_cache_files(self):\n    cache_files = dataloader_util.get_cache_files(\n        cache_dir='/tmp/', cache_prefix_filename='train', num_shards=1)\n    self.assertEqual(cache_files.cache_prefix, '/tmp/train')\n    self.assertLen(cache_files.tfrecord_files, 1)\n    self.assertEqual(cache_files.tfrecord_files[0],\n                     '/tmp/train-00000-of-00001.tfrecord')\n    self.assertEqual(cache_files.meta_data_file, '/tmp/train_meta_data.yaml')\n    self.assertEqual(cache_files.annotations_json_file,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util_test",
        "documentation": {}
    },
    {
        "label": "CacheFilesWriterTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util_test",
        "peekOfCode": "class CacheFilesWriterTest(tf.test.TestCase):\n  def test_pascal_voc_cache_writer(self):\n    images_dir, annotations_dir, label_map = test_util.create_pascal_voc(\n        self.get_temp_dir())\n    cache_writer = dataloader_util.PascalVocCacheFilesWriter(\n        label_map, images_dir, num_shards=1)\n    cache_files = dataloader_util.get_cache_files(\n        cache_dir=self.get_temp_dir(), cache_prefix_filename='pascal')\n    cache_writer.write_files(cache_files, annotations_dir)\n    # Checks the TFRecord file.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader_util_test",
        "documentation": {}
    },
    {
        "label": "ModelHParams",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "peekOfCode": "ModelHParams = model_config.ModelConfig\nmm_export('recommendation.spec.ModelHParams').export_constant(\n    __name__, 'ModelHParams')\nInputSpec = input_config_pb2.InputConfig\nFeature = input_config_pb2.Feature\nFeatureGroup = input_config_pb2.FeatureGroup\nFeatureType = input_config_pb2.FeatureType\nEncoderType = input_config_pb2.EncoderType\nmm_export('recommendation.spec.InputSpec').export_constant(\n    __name__, 'InputSpec')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "documentation": {}
    },
    {
        "label": "InputSpec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "peekOfCode": "InputSpec = input_config_pb2.InputConfig\nFeature = input_config_pb2.Feature\nFeatureGroup = input_config_pb2.FeatureGroup\nFeatureType = input_config_pb2.FeatureType\nEncoderType = input_config_pb2.EncoderType\nmm_export('recommendation.spec.InputSpec').export_constant(\n    __name__, 'InputSpec')\nmm_export('recommendation.spec.Feature').export_constant(__name__, 'Feature')\nmm_export('recommendation.spec.FeatureGroup').export_constant(\n    __name__, 'FeatureGroup')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "documentation": {}
    },
    {
        "label": "Feature",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "peekOfCode": "Feature = input_config_pb2.Feature\nFeatureGroup = input_config_pb2.FeatureGroup\nFeatureType = input_config_pb2.FeatureType\nEncoderType = input_config_pb2.EncoderType\nmm_export('recommendation.spec.InputSpec').export_constant(\n    __name__, 'InputSpec')\nmm_export('recommendation.spec.Feature').export_constant(__name__, 'Feature')\nmm_export('recommendation.spec.FeatureGroup').export_constant(\n    __name__, 'FeatureGroup')\nEncoderType.__doc__ = 'EncoderType Enum (valid: BOW, CNN, LSTM).'",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "documentation": {}
    },
    {
        "label": "FeatureGroup",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "peekOfCode": "FeatureGroup = input_config_pb2.FeatureGroup\nFeatureType = input_config_pb2.FeatureType\nEncoderType = input_config_pb2.EncoderType\nmm_export('recommendation.spec.InputSpec').export_constant(\n    __name__, 'InputSpec')\nmm_export('recommendation.spec.Feature').export_constant(__name__, 'Feature')\nmm_export('recommendation.spec.FeatureGroup').export_constant(\n    __name__, 'FeatureGroup')\nEncoderType.__doc__ = 'EncoderType Enum (valid: BOW, CNN, LSTM).'\nmm_export('recommendation.spec.EncoderType').export_constant(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "documentation": {}
    },
    {
        "label": "FeatureType",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "peekOfCode": "FeatureType = input_config_pb2.FeatureType\nEncoderType = input_config_pb2.EncoderType\nmm_export('recommendation.spec.InputSpec').export_constant(\n    __name__, 'InputSpec')\nmm_export('recommendation.spec.Feature').export_constant(__name__, 'Feature')\nmm_export('recommendation.spec.FeatureGroup').export_constant(\n    __name__, 'FeatureGroup')\nEncoderType.__doc__ = 'EncoderType Enum (valid: BOW, CNN, LSTM).'\nmm_export('recommendation.spec.EncoderType').export_constant(\n    __name__, 'EncoderType')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "documentation": {}
    },
    {
        "label": "EncoderType",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "peekOfCode": "EncoderType = input_config_pb2.EncoderType\nmm_export('recommendation.spec.InputSpec').export_constant(\n    __name__, 'InputSpec')\nmm_export('recommendation.spec.Feature').export_constant(__name__, 'Feature')\nmm_export('recommendation.spec.FeatureGroup').export_constant(\n    __name__, 'FeatureGroup')\nEncoderType.__doc__ = 'EncoderType Enum (valid: BOW, CNN, LSTM).'\nmm_export('recommendation.spec.EncoderType').export_constant(\n    __name__, 'EncoderType')\nFeatureType.__doc__ = 'FeatureType Enum (valid: STRING, INT, FLOAT).'",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "documentation": {}
    },
    {
        "label": "EncoderType.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "peekOfCode": "EncoderType.__doc__ = 'EncoderType Enum (valid: BOW, CNN, LSTM).'\nmm_export('recommendation.spec.EncoderType').export_constant(\n    __name__, 'EncoderType')\nFeatureType.__doc__ = 'FeatureType Enum (valid: STRING, INT, FLOAT).'\nmm_export('recommendation.spec.FeatureType').export_constant(\n    __name__, 'FeatureType')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "documentation": {}
    },
    {
        "label": "FeatureType.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "peekOfCode": "FeatureType.__doc__ = 'FeatureType Enum (valid: STRING, INT, FLOAT).'\nmm_export('recommendation.spec.FeatureType').export_constant(\n    __name__, 'FeatureType')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_config",
        "documentation": {}
    },
    {
        "label": "RecommendationDataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_dataloader",
        "peekOfCode": "class RecommendationDataLoader(dataloader.DataLoader):\n  \"\"\"Recommendation data loader.\"\"\"\n  def __init__(self, dataset, size, vocab):\n    \"\"\"Init data loader.\n    Dataset is tf.data.Dataset of examples, containing:\n      for inputs:\n      - 'context': int64[], context ids as the input of variable length.\n      for outputs:\n      - 'label': int64[1], label id to predict.\n    where context is controlled by `max_context_length` in generating examples.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_dataloader",
        "documentation": {}
    },
    {
        "label": "RecommendationDataLoaderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_dataloader_test",
        "peekOfCode": "class RecommendationDataLoaderTest(tf.test.TestCase):\n  def setUp(self):\n    super().setUp()\n    _testutil.setup_fake_testdata(self)\n    self.input_spec = _testutil.get_input_spec()\n  def test_download_and_extract_data(self):\n    with _testutil.patch_download_and_extract_data(self.dataset_dir) as fn:\n      out_dir = _dl.RecommendationDataLoader.download_and_extract_movielens(\n          self.download_dir)\n      fn.called_once_with(self.download_dir)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_dataloader_test",
        "documentation": {}
    },
    {
        "label": "setup_fake_testdata",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "peekOfCode": "def setup_fake_testdata(obj):\n  \"\"\"Setup fake testdata folder.\n  This function creates new attrs:\n  - test_tempdir: temporary dir (optional), if not exists.\n  - download_dir: datasets dir for downloaded zip.\n  - dataset_dir: extracted dir for movielens data.\n  Args:\n    obj: object, usually TestCase instance's self or cls.\n  \"\"\"\n  if not hasattr(obj, 'test_tempdir'):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "documentation": {}
    },
    {
        "label": "patch_download_and_extract_data",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "peekOfCode": "def patch_download_and_extract_data(data_dir):\n  \"\"\"Patch download and extract data for testing.\n  The common usage is to generate data loader:\n  with patch_download_and_extract_data(movielens_dir):\n    train_loader = RecommendationDataLoader.from_movielens(\n        generated_dir, 'train', test_tempdir)\n  Args:\n    data_dir: str, path to data dir.\n  Returns:\n    mocked context.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "documentation": {}
    },
    {
        "label": "get_input_spec",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "peekOfCode": "def get_input_spec(encoder_type='cnn') -> recommendation_config.InputSpec:\n  \"\"\"Gets input spec (for test).\n  Input spec defines how the input features are extracted.\n  Args:\n    encoder_type: str. Case-insensitive {'CNN', 'LSTM', 'BOW'}.\n  Returns:\n    InputSpec.\n  \"\"\"\n  etype = encoder_type.upper()\n  if etype not in {'CNN', 'LSTM', 'BOW'}:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "documentation": {}
    },
    {
        "label": "get_model_hparams",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "peekOfCode": "def get_model_hparams() -> recommendation_config.ModelHParams:\n  \"\"\"Gets model hparams (for test).\n  ModelHParams defines the model architecture.\n  Returns:\n    ModelHParams.\n  \"\"\"\n  return recommendation_config.ModelHParams(\n      hidden_layer_dims=[32, 32],  # Hidden layers dimension.\n      eval_top_k=[1, 5],  # Eval top 1 and top 5.\n      conv_num_filter_ratios=[2, 4],  # For CNN encoder, conv filter mutipler.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "documentation": {}
    },
    {
        "label": "MOVIE_SIZE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "peekOfCode": "MOVIE_SIZE = 101\nRATING_SIZE = 5000\nUSER_SIZE = 50\nTRAIN_SIZE = 4455\nTEST_SIZE = 495\nVOCAB_SIZE = 101\nMAX_ITEM_ID = 999\ndef _generate_fake_data(data_dir):\n  \"\"\"Generates fake data to files.\n  It generates 3 files.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "documentation": {}
    },
    {
        "label": "RATING_SIZE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "peekOfCode": "RATING_SIZE = 5000\nUSER_SIZE = 50\nTRAIN_SIZE = 4455\nTEST_SIZE = 495\nVOCAB_SIZE = 101\nMAX_ITEM_ID = 999\ndef _generate_fake_data(data_dir):\n  \"\"\"Generates fake data to files.\n  It generates 3 files.\n  - movies.dat: movies data, with format per line:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "documentation": {}
    },
    {
        "label": "USER_SIZE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "peekOfCode": "USER_SIZE = 50\nTRAIN_SIZE = 4455\nTEST_SIZE = 495\nVOCAB_SIZE = 101\nMAX_ITEM_ID = 999\ndef _generate_fake_data(data_dir):\n  \"\"\"Generates fake data to files.\n  It generates 3 files.\n  - movies.dat: movies data, with format per line:\n               MovieID::Title::Genres.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "documentation": {}
    },
    {
        "label": "TRAIN_SIZE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "peekOfCode": "TRAIN_SIZE = 4455\nTEST_SIZE = 495\nVOCAB_SIZE = 101\nMAX_ITEM_ID = 999\ndef _generate_fake_data(data_dir):\n  \"\"\"Generates fake data to files.\n  It generates 3 files.\n  - movies.dat: movies data, with format per line:\n               MovieID::Title::Genres.\n  - users.dat: users data, with format per line:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "documentation": {}
    },
    {
        "label": "TEST_SIZE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "peekOfCode": "TEST_SIZE = 495\nVOCAB_SIZE = 101\nMAX_ITEM_ID = 999\ndef _generate_fake_data(data_dir):\n  \"\"\"Generates fake data to files.\n  It generates 3 files.\n  - movies.dat: movies data, with format per line:\n               MovieID::Title::Genres.\n  - users.dat: users data, with format per line:\n               UserID::Gender::Age::Occupation::Zip-code.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "documentation": {}
    },
    {
        "label": "VOCAB_SIZE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "peekOfCode": "VOCAB_SIZE = 101\nMAX_ITEM_ID = 999\ndef _generate_fake_data(data_dir):\n  \"\"\"Generates fake data to files.\n  It generates 3 files.\n  - movies.dat: movies data, with format per line:\n               MovieID::Title::Genres.\n  - users.dat: users data, with format per line:\n               UserID::Gender::Age::Occupation::Zip-code.\n  - ratings.dat: movie ratings by users, with format per line:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "documentation": {}
    },
    {
        "label": "MAX_ITEM_ID",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "peekOfCode": "MAX_ITEM_ID = 999\ndef _generate_fake_data(data_dir):\n  \"\"\"Generates fake data to files.\n  It generates 3 files.\n  - movies.dat: movies data, with format per line:\n               MovieID::Title::Genres.\n  - users.dat: users data, with format per line:\n               UserID::Gender::Age::Occupation::Zip-code.\n  - ratings.dat: movie ratings by users, with format per line:\n               UserID::MovieID::Rating::Timestamp",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.recommendation_testutil",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader",
        "peekOfCode": "class DataLoader(object):\n  \"\"\"Base DataLoader class for Searcher task.\"\"\"\n  def __init__(\n      self,\n      embedder_path: Optional[str] = None,\n      dataset: Optional[np.ndarray] = None,\n      metadata: Optional[List[AnyStr]] = None,\n  ) -> None:\n    \"\"\"Initializes DataLoader for Searcher task.\n    Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader",
        "documentation": {}
    },
    {
        "label": "SearcherDataloaderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "peekOfCode": "class SearcherDataloaderTest(tf.test.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.tflite_path = test_util.get_test_data_path(\n        \"mobilenet_v2_035_96_embedder_with_metadata.tflite\")\n  def test_concat_dataset(self):\n    dataset1 = np.random.rand(2, 1280)\n    dataset2 = np.random.rand(2, 1280)\n    dataset3 = np.random.rand(1, 1280)\n    metadata = [\"0\", \"1\", b\"\\x11\\x22\", b\"\\x33\\x44\", \"4\"]",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "documentation": {}
    },
    {
        "label": "_BaseOptions",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "peekOfCode": "_BaseOptions = base_options_pb2.BaseOptions\n_ImageEmbedder = image_embedder.ImageEmbedder\n_ImageEmbedderOptions = image_embedder.ImageEmbedderOptions\n_SearcherDataLoader = searcher_dataloader.DataLoader\nclass SearcherDataloaderTest(tf.test.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.tflite_path = test_util.get_test_data_path(\n        \"mobilenet_v2_035_96_embedder_with_metadata.tflite\")\n  def test_concat_dataset(self):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "documentation": {}
    },
    {
        "label": "_ImageEmbedder",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "peekOfCode": "_ImageEmbedder = image_embedder.ImageEmbedder\n_ImageEmbedderOptions = image_embedder.ImageEmbedderOptions\n_SearcherDataLoader = searcher_dataloader.DataLoader\nclass SearcherDataloaderTest(tf.test.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.tflite_path = test_util.get_test_data_path(\n        \"mobilenet_v2_035_96_embedder_with_metadata.tflite\")\n  def test_concat_dataset(self):\n    dataset1 = np.random.rand(2, 1280)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "documentation": {}
    },
    {
        "label": "_ImageEmbedderOptions",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "peekOfCode": "_ImageEmbedderOptions = image_embedder.ImageEmbedderOptions\n_SearcherDataLoader = searcher_dataloader.DataLoader\nclass SearcherDataloaderTest(tf.test.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.tflite_path = test_util.get_test_data_path(\n        \"mobilenet_v2_035_96_embedder_with_metadata.tflite\")\n  def test_concat_dataset(self):\n    dataset1 = np.random.rand(2, 1280)\n    dataset2 = np.random.rand(2, 1280)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "documentation": {}
    },
    {
        "label": "_SearcherDataLoader",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "peekOfCode": "_SearcherDataLoader = searcher_dataloader.DataLoader\nclass SearcherDataloaderTest(tf.test.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.tflite_path = test_util.get_test_data_path(\n        \"mobilenet_v2_035_96_embedder_with_metadata.tflite\")\n  def test_concat_dataset(self):\n    dataset1 = np.random.rand(2, 1280)\n    dataset2 = np.random.rand(2, 1280)\n    dataset3 = np.random.rand(1, 1280)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.searcher_dataloader_test",
        "documentation": {}
    },
    {
        "label": "TextClassifierDataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader",
        "peekOfCode": "class TextClassifierDataLoader(dataloader.ClassificationDataLoader):\n  \"\"\"DataLoader for text classifier.\"\"\"\n  @classmethod\n  def from_folder(cls,\n                  filename,\n                  model_spec='average_word_vec',\n                  is_training=True,\n                  class_labels=None,\n                  shuffle=True,\n                  cache_dir=None):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader",
        "documentation": {}
    },
    {
        "label": "QuestionAnswerDataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader",
        "peekOfCode": "class QuestionAnswerDataLoader(dataloader.DataLoader):\n  \"\"\"DataLoader for question answering.\"\"\"\n  def __init__(self, dataset, size, version_2_with_negative, examples, features,\n               squad_file):\n    super(QuestionAnswerDataLoader, self).__init__(dataset, size)\n    self.version_2_with_negative = version_2_with_negative\n    self.examples = examples\n    self.features = features\n    self.squad_file = squad_file\n  @classmethod",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader",
        "documentation": {}
    },
    {
        "label": "MockClassifierModelSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "peekOfCode": "class MockClassifierModelSpec(object):\n  need_gen_vocab = False\n  def __init__(self, seq_len=4, index_to_label=None):\n    self.seq_len = seq_len\n    self.index_to_label = index_to_label\n  def get_name_to_features(self):\n    \"\"\"Gets the dictionary describing the features.\"\"\"\n    name_to_features = {\n        'input_ids': tf.io.FixedLenFeature([self.seq_len], tf.int64),\n        'label_ids': tf.io.FixedLenFeature([], tf.int64),",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "documentation": {}
    },
    {
        "label": "MockQAModelSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "peekOfCode": "class MockQAModelSpec(object):\n  def __init__(self, vocab_dir):\n    self.seq_len = 384\n    self.predict_batch_size = 8\n    self.query_len = 64\n    self.doc_stride = 128\n    vocab_file = os.path.join(vocab_dir, 'vocab.txt')\n    vocab = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'good', 'bad']\n    with open(vocab_file, 'w') as f:\n      f.write('\\n'.join(vocab))",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "documentation": {}
    },
    {
        "label": "LoaderFunctionTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "peekOfCode": "class LoaderFunctionTest(tf.test.TestCase):\n  def setUp(self):\n    super(LoaderFunctionTest, self).setUp()\n    self.model_spec = MockClassifierModelSpec()\n  def test_load(self):\n    tfrecord_file = self._get_tfrecord_file()\n    meta_data_file = self._get_meta_data_file()\n    dataset, meta_data = text_dataloader._load(tfrecord_file, meta_data_file,\n                                               self.model_spec)\n    for i, (input_ids, label_ids) in enumerate(dataset):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "documentation": {}
    },
    {
        "label": "TextClassifierDataLoaderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "peekOfCode": "class TextClassifierDataLoaderTest(tf.test.TestCase):\n  TRAIN_LABELS_AND_TEXT = (('neutral', 'indifferent'),\n                           ('pos', 'extremely great'), ('neg', 'totally awful'))\n  TEST_LABELS_AND_TEXT = (('pos', 'super good'), ('neg', 'really bad'))\n  def _get_folder_path(self):\n    folder_path = os.path.join(self.get_temp_dir(), 'random_text_dir')\n    if os.path.exists(folder_path):\n      return\n    os.mkdir(folder_path)\n    for label, text in self.TEST_LABELS_AND_TEXT:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "documentation": {}
    },
    {
        "label": "QuestionAnswerDataLoaderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "peekOfCode": "class QuestionAnswerDataLoaderTest(tf.test.TestCase, parameterized.TestCase):\n  @parameterized.parameters(\n      ('train-v1.1.json', True, False, 1),\n      ('dev-v1.1.json', False, False, 8),\n      ('train-v2.0.json', True, True, 2),\n      ('dev-v2.0.json', False, True, 8),\n  )\n  def test_from_squad(self, test_file, is_training, version_2_with_negative,\n                      size):\n    path = test_util.get_test_data_path('squad_testdata')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_dataloader_test",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_searcher_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_searcher_dataloader",
        "peekOfCode": "class DataLoader(searcher_dataloader.DataLoader):\n  \"\"\"DataLoader class for Text Searcher.\"\"\"\n  def __init__(self, embedder: text_embedder.TextEmbedder) -> None:\n    \"\"\"Initializes DataLoader for Image Searcher task.\n    Args:\n      embedder: Embedder to generate embedding from raw input image.\n    \"\"\"\n    self._embedder = embedder\n    super().__init__(embedder_path=embedder.options.base_options.file_name)\n  @classmethod",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_searcher_dataloader",
        "documentation": {}
    },
    {
        "label": "_BaseOptions",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_searcher_dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_searcher_dataloader",
        "peekOfCode": "_BaseOptions = base_options_module.BaseOptions\n@mm_export(\"searcher.TextDataLoader\")\nclass DataLoader(searcher_dataloader.DataLoader):\n  \"\"\"DataLoader class for Text Searcher.\"\"\"\n  def __init__(self, embedder: text_embedder.TextEmbedder) -> None:\n    \"\"\"Initializes DataLoader for Image Searcher task.\n    Args:\n      embedder: Embedder to generate embedding from raw input image.\n    \"\"\"\n    self._embedder = embedder",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_searcher_dataloader",
        "documentation": {}
    },
    {
        "label": "TextSearcherDataloaderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_searcher_dataloader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_searcher_dataloader_test",
        "peekOfCode": "class TextSearcherDataloaderTest(parameterized.TestCase, tf.test.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.tflite_path = test_util.get_test_data_path(\n        \"regex_one_embedding_with_metadata.tflite\")\n    self.text_csv_file1 = test_util.get_test_data_path(\"movies.csv\")\n    self.text_csv_file2 = test_util.get_test_data_path(\"trips.csv\")\n  @parameterized.parameters(\n      (False, 0.0301549),\n      (True, 0.538059),",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.data_util.text_searcher_dataloader_test",
        "documentation": {}
    },
    {
        "label": "WarmUp",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.optimization.warmup",
        "description": "examples.tensorflow_examples.lite.model_maker.core.optimization.warmup",
        "peekOfCode": "class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n  \"\"\"Applies a warmup schedule on a given learning rate decay schedule.\"\"\"\n  def __init__(self,\n               initial_learning_rate,\n               decay_schedule_fn,\n               warmup_steps,\n               name=None):\n    super(WarmUp, self).__init__()\n    self.initial_learning_rate = initial_learning_rate\n    self.warmup_steps = warmup_steps",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.optimization.warmup",
        "documentation": {}
    },
    {
        "label": "QuestionAnswererInfo",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "peekOfCode": "class QuestionAnswererInfo(ModelSpecificInfo):\n  \"\"\"Holds information specificly tied to a Bert question answerer model.\"\"\"\n  def __init__(self,\n               name,\n               version,\n               description,\n               input_names,\n               output_names,\n               tokenizer_type,\n               vocab_file=None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "documentation": {}
    },
    {
        "label": "MetadataPopulatorForBertQuestionAndAnswer",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "peekOfCode": "class MetadataPopulatorForBertQuestionAndAnswer(MetadataPopulatorForBert):\n  \"\"\"Populates the metadata for a Bert QA model.\"\"\"\n  def _create_output_metadata(self):\n    \"\"\"Creates the output metadata for a Bert QA model.\"\"\"\n    # Creates outputs info.\n    end_meta = _metadata_fb.TensorMetadataT()\n    end_meta.name = \"end_logits\"\n    end_meta.description = (\n        \"logits over the sequence which indicates the\"\n        \" end position of the answer span with closed interval.\")",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "documentation": {}
    },
    {
        "label": "bert_qa_outputs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "peekOfCode": "def bert_qa_outputs(start_logits_name, end_logits_name):\n  \"\"\"Creates the output tensor names of a Bert question answerer model in order.\n  The names correspond to `Tensor.name` in the TFLite schema. It helps to\n  determine the tensor order when populating the metadata.\n  Args:\n    start_logits_name: name of the start logits tensor, which represents the\n      start position of the answer span.\n    end_logits_name: name of the end logits tensor, which represents the end\n      position of the answer span.\n  Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DESCRIPTION",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "peekOfCode": "DEFAULT_DESCRIPTION = (\n    \"Answers questions based on the content of a given \"\n    \"passage. To integrate the model into your app, try the \"\n    \"`BertQuestionAnswerer` API in the TensorFlow Lite Task \"\n    \"library. `BertQuestionAnswerer` takes a passage string and a \"\n    \"query string, and returns the answer strings. It encapsulates \"\n    \"the processing logic of inputs and outputs and runs the \"\n    \"inference with the best practice.\")\nDEFAULT_INPUT_NAMES = bert_qa_inputs(\n    ids_name=\"input_ids\",",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "documentation": {}
    },
    {
        "label": "DEFAULT_INPUT_NAMES",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "peekOfCode": "DEFAULT_INPUT_NAMES = bert_qa_inputs(\n    ids_name=\"input_ids\",\n    mask_name=\"input_mask\",\n    segment_ids_name=\"segment_ids\")\nDEFAULT_OUTPUT_NAMES = bert_qa_outputs(\n    start_logits_name=\"start_logits\", end_logits_name=\"end_logits\")\n_MODEL_INFO = {\n    \"mobilebert_float.tflite\":\n        QuestionAnswererInfo(\n            name=\"MobileBert Question and Answerer\",",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "documentation": {}
    },
    {
        "label": "DEFAULT_OUTPUT_NAMES",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "peekOfCode": "DEFAULT_OUTPUT_NAMES = bert_qa_outputs(\n    start_logits_name=\"start_logits\", end_logits_name=\"end_logits\")\n_MODEL_INFO = {\n    \"mobilebert_float.tflite\":\n        QuestionAnswererInfo(\n            name=\"MobileBert Question and Answerer\",\n            version=\"v1\",\n            description=DEFAULT_DESCRIPTION,\n            input_names=DEFAULT_INPUT_NAMES,\n            output_names=DEFAULT_OUTPUT_NAMES,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "documentation": {}
    },
    {
        "label": "_MODEL_INFO",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "peekOfCode": "_MODEL_INFO = {\n    \"mobilebert_float.tflite\":\n        QuestionAnswererInfo(\n            name=\"MobileBert Question and Answerer\",\n            version=\"v1\",\n            description=DEFAULT_DESCRIPTION,\n            input_names=DEFAULT_INPUT_NAMES,\n            output_names=DEFAULT_OUTPUT_NAMES,\n            tokenizer_type=Tokenizer.BERT_TOKENIZER,\n            vocab_file=\"vocab.txt\"),",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.question_answerer.metadata_writer_for_bert_question_answerer",
        "documentation": {}
    },
    {
        "label": "ClassifierSpecificInfo",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier.metadata_writer_for_bert_text_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier.metadata_writer_for_bert_text_classifier",
        "peekOfCode": "class ClassifierSpecificInfo(ModelSpecificInfo):\n  \"\"\"Holds information specificly tied to a Bert text classifier model.\"\"\"\n  def __init__(self,\n               name,\n               version,\n               description,\n               input_names,\n               tokenizer_type,\n               label_file,\n               vocab_file=None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier.metadata_writer_for_bert_text_classifier",
        "documentation": {}
    },
    {
        "label": "MetadataPopulatorForBertTextClassifier",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier.metadata_writer_for_bert_text_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier.metadata_writer_for_bert_text_classifier",
        "peekOfCode": "class MetadataPopulatorForBertTextClassifier(MetadataPopulatorForBert):\n  \"\"\"Populates the metadata for a Bert text classifier model.\"\"\"\n  def __init__(self, model_file, export_directory, model_info):\n    self.model_info = model_info\n    model_dir_name = os.path.dirname(model_file)\n    file_paths = []\n    if model_info.vocab_file is not None:\n      file_paths.append(os.path.join(model_dir_name, model_info.vocab_file))\n    if model_info.sp_model is not None:\n      file_paths.append(os.path.join(model_dir_name, model_info.sp_model))",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier.metadata_writer_for_bert_text_classifier",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DESCRIPTION",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier.metadata_writer_for_bert_text_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier.metadata_writer_for_bert_text_classifier",
        "peekOfCode": "DEFAULT_DESCRIPTION = (\n    \"Classifies the input string based on the known catergories. To integrate \"\n    \"the model into your app, try the `BertNLClassifier` API in the TensorFlow \"\n    \"Lite Task library. `BertNLClassifier` takes an input string, and returns \"\n    \"the classified label with probability. It encapsulates the processing \"\n    \"logic of inputs and outputs and runs the inference with the best \"\n    \"practice.\")\n_MODEL_INFO = {\n    \"sst2_mobilebert_quant.tflite\":\n        ClassifierSpecificInfo(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier.metadata_writer_for_bert_text_classifier",
        "documentation": {}
    },
    {
        "label": "_MODEL_INFO",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier.metadata_writer_for_bert_text_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier.metadata_writer_for_bert_text_classifier",
        "peekOfCode": "_MODEL_INFO = {\n    \"sst2_mobilebert_quant.tflite\":\n        ClassifierSpecificInfo(\n            name=\"MobileBert text classifier\",\n            version=\"v1\",\n            description=DEFAULT_DESCRIPTION,\n            input_names=bert_qa_inputs(\n                ids_name=\"serving_default_input_word_ids:0\",\n                mask_name=\"serving_default_input_mask:0\",\n                segment_ids_name=\"serving_default_input_type_ids:0\"),",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.text_classifier.metadata_writer_for_bert_text_classifier",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "peekOfCode": "class Tokenizer(enum.Enum):\n  BERT_TOKENIZER = \"BERT_TOKENIZER\"\n  SENTENCE_PIECE = \"SENTENCE_PIECE\"\ndef bert_qa_inputs(ids_name, mask_name, segment_ids_name):\n  \"\"\"Creates the input tensor names of a Bert model in order.\n  The names correspond to `Tensor.name` in the TFLite schema. It helps to\n  determine the tensor order when populating the metadata.\n  Args:\n    ids_name: name of the ids tensor, which represents the tokenized ids of\n      input text as concatenated query and passage.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "documentation": {}
    },
    {
        "label": "ModelSpecificInfo",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "peekOfCode": "class ModelSpecificInfo(object):\n  \"\"\"Holds information that is specificly tied to a Bert model.\"\"\"\n  def __init__(self,\n               name,\n               version,\n               description,\n               input_names,\n               tokenizer_type,\n               vocab_file=None,\n               sp_model=None):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "documentation": {}
    },
    {
        "label": "MetadataPopulatorForBert",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "peekOfCode": "class MetadataPopulatorForBert(metadata_writer.MetadataWriter):\n  \"\"\"Populates the metadata for a Bert model.\"\"\"\n  def __init__(self, model_file, export_directory, model_info):\n    self.model_info = model_info\n    model_dir_name = os.path.dirname(model_file)\n    file_paths = []\n    if model_info.vocab_file is not None:\n      file_paths.append(os.path.join(model_dir_name, model_info.vocab_file))\n    if model_info.sp_model is not None:\n      file_paths.append(os.path.join(model_dir_name, model_info.sp_model))",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "documentation": {}
    },
    {
        "label": "bert_qa_inputs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "peekOfCode": "def bert_qa_inputs(ids_name, mask_name, segment_ids_name):\n  \"\"\"Creates the input tensor names of a Bert model in order.\n  The names correspond to `Tensor.name` in the TFLite schema. It helps to\n  determine the tensor order when populating the metadata.\n  Args:\n    ids_name: name of the ids tensor, which represents the tokenized ids of\n      input text as concatenated query and passage.\n    mask_name: name of the mask tensor, which represents the mask with 1 for\n      real tokens and 0 for padding tokens.\n    segment_ids_name: name of the segment ids tensor, where 0 is for query and 1",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.bert.metadata_writer_for_bert",
        "documentation": {}
    },
    {
        "label": "ModelSpecificInfo",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "peekOfCode": "class ModelSpecificInfo(object):\n  \"\"\"Holds information that is specificly tied to a text classifier.\"\"\"\n  def __init__(self,\n               name,\n               version,\n               description,\n               delim_regex_pattern=DEFAULT_DELIM_REGEX_PATTERN,\n               label_file=DEFAULT_LABEL_FILE,\n               vocab_file=DEFAULT_VOCAB_FILE):\n    self.name = name",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "documentation": {}
    },
    {
        "label": "MetadataPopulatorForTextClassifier",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "peekOfCode": "class MetadataPopulatorForTextClassifier(metadata_writer.MetadataWriter):\n  \"\"\"Populates the metadata for a text classifier.\"\"\"\n  def __init__(self, model_file, export_directory, model_info, label_file_path,\n               vocab_file_path):\n    self.model_info = model_info\n    super(MetadataPopulatorForTextClassifier,\n          self).__init__(model_file, export_directory,\n                         [label_file_path, vocab_file_path])\n  def _create_metadata(self):\n    \"\"\"Creates the metadata for a text classifier.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "documentation": {}
    },
    {
        "label": "DEFAULT_LABEL_FILE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "peekOfCode": "DEFAULT_LABEL_FILE = \"labels.txt\"\nDEFAULT_VOCAB_FILE = \"vocab.txt\"\n# Split pattern used in the tokenizer.\nDEFAULT_DELIM_REGEX_PATTERN = r\"[^\\w\\']+\"\nclass ModelSpecificInfo(object):\n  \"\"\"Holds information that is specificly tied to a text classifier.\"\"\"\n  def __init__(self,\n               name,\n               version,\n               description,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "documentation": {}
    },
    {
        "label": "DEFAULT_VOCAB_FILE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "peekOfCode": "DEFAULT_VOCAB_FILE = \"vocab.txt\"\n# Split pattern used in the tokenizer.\nDEFAULT_DELIM_REGEX_PATTERN = r\"[^\\w\\']+\"\nclass ModelSpecificInfo(object):\n  \"\"\"Holds information that is specificly tied to a text classifier.\"\"\"\n  def __init__(self,\n               name,\n               version,\n               description,\n               delim_regex_pattern=DEFAULT_DELIM_REGEX_PATTERN,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DELIM_REGEX_PATTERN",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "peekOfCode": "DEFAULT_DELIM_REGEX_PATTERN = r\"[^\\w\\']+\"\nclass ModelSpecificInfo(object):\n  \"\"\"Holds information that is specificly tied to a text classifier.\"\"\"\n  def __init__(self,\n               name,\n               version,\n               description,\n               delim_regex_pattern=DEFAULT_DELIM_REGEX_PATTERN,\n               label_file=DEFAULT_LABEL_FILE,\n               vocab_file=DEFAULT_VOCAB_FILE):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "documentation": {}
    },
    {
        "label": "_MODEL_INFO",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "peekOfCode": "_MODEL_INFO = {\n    # AverageWordVector model trained on IMDB movie reviews dataset.\n    \"text_classification.tflite\":\n        ModelSpecificInfo(\n            name=\"Sentiment Analyzer (AverageWordVecModelSpec)\",\n            description=\"Detect if the input text's sentiment is positive or \"\n            \"negative. The model was trained on the IMDB Movie Reviews dataset \"\n            \"so it is more accurate when input text is a movie review.\",\n            version=\"v1\"),\n}",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.text_classifier.metadata_writer_for_text_classifier",
        "documentation": {}
    },
    {
        "label": "MetadataWriter",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.metadata_writer",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.metadata_writer",
        "peekOfCode": "class MetadataWriter(abc.ABC):\n  \"\"\"Writes the metadata and associated file into a TFLite model.\"\"\"\n  def __init__(self, model_file, export_directory, associated_files):\n    \"\"\"Constructs the MetadataWriter.\n    Args:\n      model_file: the path to the model to be populated.\n      export_directory: path to the directory where the model and json file will\n        be exported to.\n      associated_files: path to the associated files to be populated.\n    \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writers.metadata_writer",
        "documentation": {}
    },
    {
        "label": "MetadataWriter",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "peekOfCode": "class MetadataWriter:\n  \"\"\"Helper class to populate Audio Metadata, to be used in `with` statement.\n  Simple usage for model with two classification heads.\n  with MetadataWriter(tflite_path) as writer:\n    writer.add_input(sample_rate=16000, channels=1)\n    writer.add_output(name='animal_sound', labels=['dog', 'cat'])\n    writer.add_output(name='speech_command', labels=['yes', 'no'])\n    writer.save(tflite_path, json_filepath)\n  `add_output` can also take an ordered dict for multiple locales, example:\n  writer.add_output(name='animal_sound', labels=collections.OrderedDict([",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "documentation": {}
    },
    {
        "label": "BaseSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "peekOfCode": "class BaseSpec(abc.ABC):\n  \"\"\"Base model spec for audio classification.\"\"\"\n  def __init__(self, model_dir=None, strategy=None):\n    _ensure_tf25(_get_tf_version())\n    self.model_dir = model_dir\n    if not model_dir:\n      self.model_dir = tempfile.mkdtemp()\n    tf.compat.v1.logging.info('Checkpoints are stored in %s', self.model_dir)\n    self.strategy = strategy or tf.distribute.get_strategy()\n  @abc.abstractproperty",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "documentation": {}
    },
    {
        "label": "BrowserFFTSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "peekOfCode": "class BrowserFFTSpec(BaseSpec):\n  \"\"\"Model good at detecting speech commands, using Browser FFT spectrum.\"\"\"\n  EXPECTED_WAVEFORM_LENGTH = 44032\n  # Information used to populate TFLite metadata.\n  _MODEL_NAME = 'AudioClassifier'\n  _MODEL_DESCRIPTION = ('Identify the most prominent type in the audio clip '\n                        'from a known set of categories.')\n  _MODEL_VERSION = 'v1'\n  _MODEL_AUTHOR = 'TensorFlow Lite Model Maker'\n  _MODEL_LICENSES = ('Apache License. Version 2.0 '",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "documentation": {}
    },
    {
        "label": "YAMNetSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "peekOfCode": "class YAMNetSpec(BaseSpec):\n  \"\"\"Model good at detecting environmental sounds, using YAMNet embedding.\"\"\"\n  EXPECTED_WAVEFORM_LENGTH = 15600  # effectively 0.975s\n  EMBEDDING_SIZE = 1024\n  # Information used to populate TFLite metadata.\n  _MODEL_NAME = 'yamnet/classification'\n  _MODEL_DESCRIPTION = 'Recognizes sound events'\n  _MODEL_VERSION = 'v1'\n  _MODEL_AUTHOR = 'TensorFlow Lite Model Maker'\n  _MODEL_LICENSES = ('Apache License. Version 2.0 '",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "documentation": {}
    },
    {
        "label": "TFJS_MODEL_ROOT",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "peekOfCode": "TFJS_MODEL_ROOT = 'https://storage.googleapis.com/tfjs-models/tfjs'\ndef _load_browser_fft_preprocess_model():\n  \"\"\"Load a model replicating WebAudio's AnalyzerNode.getFloatFrequencyData.\"\"\"\n  model_name = 'sc_preproc_model'\n  file_extension = '.tar.gz'\n  filename = model_name + file_extension\n  # Load the preprocessing model, which transforms audio waveform into\n  # spectrograms (2D image-like representation of sound).\n  # This model replicates WebAudio's AnalyzerNode.getFloatFrequencyData\n  # (https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getFloatFrequencyData).",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec",
        "documentation": {}
    },
    {
        "label": "BaseSpecTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec_test",
        "peekOfCode": "class BaseSpecTest(tf.test.TestCase):\n  def testEnsureVersion(self):\n    valid_versions = ['2.5.0', '2.5.0rc1', '2.6']\n    invalid_versions = [\n        '2.4.1',\n    ]\n    specs = [audio_spec.YAMNetSpec, audio_spec.BrowserFFTSpec]\n    tmp_version_fn = audio_spec._get_tf_version\n    for spec in specs:\n      for valid_version in valid_versions:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec_test",
        "documentation": {}
    },
    {
        "label": "BaseTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec_test",
        "peekOfCode": "class BaseTest(tf.test.TestCase):\n  def _train_and_export(self,\n                        spec,\n                        num_classes,\n                        filename,\n                        expected_model_size,\n                        quantization_config=None,\n                        training=True):\n    dataset = _gen_dataset(\n        spec, total_samples=10, num_classes=num_classes, batch_size=2, seed=100)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec_test",
        "documentation": {}
    },
    {
        "label": "YAMNetSpecTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec_test",
        "peekOfCode": "class YAMNetSpecTest(BaseTest):\n  def _test_preprocess(self, input_shape, input_count, output_shape,\n                       output_count):\n    spec = audio_spec.YAMNetSpec()\n    wav_ds = tf.data.Dataset.from_tensor_slices([tf.ones(input_shape)] *\n                                                input_count)\n    label_ds = tf.data.Dataset.range(input_count).map(\n        lambda x: tf.cast(x, tf.int32))\n    ds = tf.data.Dataset.zip((wav_ds, label_ds))\n    ds = spec.preprocess_ds(ds)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec_test",
        "documentation": {}
    },
    {
        "label": "BrowserFFTSpecTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec_test",
        "peekOfCode": "class BrowserFFTSpecTest(BaseTest):\n  @classmethod\n  def setUpClass(cls):\n    super(BrowserFFTSpecTest, cls).setUpClass()\n    cls._spec = audio_spec.BrowserFFTSpec()\n  def test_model_initialization(self):\n    model = self._spec.create_model(10)\n    self.assertEqual(self._spec._preprocess_model.input_shape,\n                     (None, self._spec.EXPECTED_WAVEFORM_LENGTH))\n    self.assertEqual(self._spec._preprocess_model.output_shape,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.audio_spec_test",
        "documentation": {}
    },
    {
        "label": "ImageModelSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "class ImageModelSpec(object):\n  \"\"\"A specification of image model.\"\"\"\n  mean_rgb = [0.0]\n  stddev_rgb = [255.0]\n  def __init__(self,\n               uri,\n               compat_tf_versions=None,\n               input_image_shape=None,\n               name=''):\n    \"\"\"Initializes a new instance of the `ImageModelSpec` class.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "mobilenet_v2_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "mobilenet_v2_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4',\n    compat_tf_versions=2,\n    name='mobilenet_v2')\nmobilenet_v2_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates MobileNet v2 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)\nmm_export('image_classifier.MobileNetV2Spec').export_constant(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "mobilenet_v2_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "mobilenet_v2_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates MobileNet v2 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)\nmm_export('image_classifier.MobileNetV2Spec').export_constant(\n    __name__, 'mobilenet_v2_spec')\nresnet_50_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4',\n    compat_tf_versions=2,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "resnet_50_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "resnet_50_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4',\n    compat_tf_versions=2,\n    name='resnet_50')\nresnet_50_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates ResNet 50 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)\nmm_export('image_classifier.Resnet50Spec').export_constant(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "resnet_50_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "resnet_50_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates ResNet 50 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)\nmm_export('image_classifier.Resnet50Spec').export_constant(\n    __name__, 'resnet_50_spec')\nefficientnet_lite0_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2',\n    compat_tf_versions=[1, 2],",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite0_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "efficientnet_lite0_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2',\n    compat_tf_versions=[1, 2],\n    name='efficientnet_lite0')\nefficientnet_lite0_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates EfficientNet-Lite0 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)\nmm_export('image_classifier.EfficientNetLite0Spec').export_constant(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite0_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "efficientnet_lite0_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates EfficientNet-Lite0 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)\nmm_export('image_classifier.EfficientNetLite0Spec').export_constant(\n    __name__, 'efficientnet_lite0_spec')\nefficientnet_lite1_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/tensorflow/efficientnet/lite1/feature-vector/2',\n    compat_tf_versions=[1, 2],",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite1_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "efficientnet_lite1_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/tensorflow/efficientnet/lite1/feature-vector/2',\n    compat_tf_versions=[1, 2],\n    input_image_shape=[240, 240],\n    name='efficientnet_lite1')\nefficientnet_lite1_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates EfficientNet-Lite1 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite1_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "efficientnet_lite1_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates EfficientNet-Lite1 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)\nmm_export('image_classifier.EfficientNetLite1Spec').export_constant(\n    __name__, 'efficientnet_lite1_spec')\nefficientnet_lite2_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/tensorflow/efficientnet/lite2/feature-vector/2',\n    compat_tf_versions=[1, 2],",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite2_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "efficientnet_lite2_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/tensorflow/efficientnet/lite2/feature-vector/2',\n    compat_tf_versions=[1, 2],\n    input_image_shape=[260, 260],\n    name='efficientnet_lite2')\nefficientnet_lite2_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates EfficientNet-Lite2 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite2_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "efficientnet_lite2_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates EfficientNet-Lite2 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)\nmm_export('image_classifier.EfficientNetLite2Spec').export_constant(\n    __name__, 'efficientnet_lite2_spec')\nefficientnet_lite3_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/tensorflow/efficientnet/lite3/feature-vector/2',\n    compat_tf_versions=[1, 2],",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite3_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "efficientnet_lite3_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/tensorflow/efficientnet/lite3/feature-vector/2',\n    compat_tf_versions=[1, 2],\n    input_image_shape=[280, 280],\n    name='efficientnet_lite3')\nefficientnet_lite3_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates EfficientNet-Lite3 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite3_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "efficientnet_lite3_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates EfficientNet-Lite3 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)\nmm_export('image_classifier.EfficientNetLite3Spec').export_constant(\n    __name__, 'efficientnet_lite3_spec')\nefficientnet_lite4_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/tensorflow/efficientnet/lite4/feature-vector/2',\n    compat_tf_versions=[1, 2],",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite4_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "efficientnet_lite4_spec = functools.partial(\n    ImageModelSpec,\n    uri='https://tfhub.dev/tensorflow/efficientnet/lite4/feature-vector/2',\n    compat_tf_versions=[1, 2],\n    input_image_shape=[300, 300],\n    name='efficientnet_lite4')\nefficientnet_lite4_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates EfficientNet-Lite4 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite4_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "peekOfCode": "efficientnet_lite4_spec.__doc__ = util.wrap_doc(\n    ImageModelSpec,\n    'Creates EfficientNet-Lite4 model spec. See also: `tflite_model_maker.image_classifier.ModelSpec`.'\n)\nmm_export('image_classifier.EfficientNetLite4Spec').export_constant(\n    __name__, 'efficientnet_lite4_spec')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.image_spec",
        "documentation": {}
    },
    {
        "label": "ModelSpecTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.model_spec_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.model_spec_test",
        "peekOfCode": "class ModelSpecTest(tf.test.TestCase, parameterized.TestCase):\n  def test_get(self):\n    spec = ms.get('mobilenet_v2')\n    self.assertIsInstance(spec, image_spec.ImageModelSpec)\n    spec = ms.get('average_word_vec')\n    self.assertIsInstance(spec, text_spec.AverageWordVecModelSpec)\n    spec = ms.get(image_spec.mobilenet_v2_spec)\n    self.assertIsInstance(spec, image_spec.ImageModelSpec)\n  @parameterized.parameters(MODELS)\n  def test_get_not_none(self, model):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.model_spec_test",
        "documentation": {}
    },
    {
        "label": "MODELS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.model_spec_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.model_spec_test",
        "peekOfCode": "MODELS = (\n    ms.IMAGE_CLASSIFICATION_MODELS + ms.TEXT_CLASSIFICATION_MODELS +\n    ms.QUESTION_ANSWER_MODELS)\nclass ModelSpecTest(tf.test.TestCase, parameterized.TestCase):\n  def test_get(self):\n    spec = ms.get('mobilenet_v2')\n    self.assertIsInstance(spec, image_spec.ImageModelSpec)\n    spec = ms.get('average_word_vec')\n    self.assertIsInstance(spec, text_spec.AverageWordVecModelSpec)\n    spec = ms.get(image_spec.mobilenet_v2_spec)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.model_spec_test",
        "documentation": {}
    },
    {
        "label": "ExportModel",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "class ExportModel(efficientdet_keras.EfficientDetModel):\n  \"\"\"Model to be exported as SavedModel/TFLite format.\"\"\"\n  def __init__(self,\n               model: tf.keras.Model,\n               config: hparams_config.Config,\n               pre_mode: Optional[str] = 'infer',\n               post_mode: Optional[str] = 'global',\n               name: Optional[str] = ''):\n    \"\"\"Initilizes an instance with the keras model and pre/post_mode paramaters.\n    Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "EfficientDetModelSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "class EfficientDetModelSpec(object):\n  \"\"\"A specification of the EfficientDet model.\"\"\"\n  compat_tf_versions = compat.get_compat_tf_versions(2)\n  def __init__(self,\n               model_name: str,\n               uri: str,\n               hparams: str = '',\n               model_dir: Optional[str] = None,\n               epochs: int = 50,\n               batch_size: int = 64,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "_NUM_CALIBRATION_STEPS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "_NUM_CALIBRATION_STEPS = 500\ndef _get_ordered_label_map(\n    label_map: Optional[Dict[int, str]]) -> Optional[Dict[int, str]]:\n  \"\"\"Gets label_map as an OrderedDict instance with ids sorted.\"\"\"\n  if not label_map:\n    return label_map\n  ordered_label_map = collections.OrderedDict()\n  for idx in sorted(label_map.keys()):\n    ordered_label_map[idx] = label_map[idx]\n  return ordered_label_map",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "efficientdet_lite0_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "efficientdet_lite0_spec = functools.partial(\n    EfficientDetModelSpec,\n    model_name='efficientdet-lite0',\n    uri='https://tfhub.dev/tensorflow/efficientdet/lite0/feature-vector/1',\n)\nefficientdet_lite0_spec.__doc__ = util.wrap_doc(\n    EfficientDetModelSpec,\n    'Creates EfficientDet-Lite0 model spec. See also: `tflite_model_maker.object_detector.EfficientDetSpec`.'\n)\nmm_export('object_detector.EfficientDetLite0Spec').export_constant(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "efficientdet_lite0_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "efficientdet_lite0_spec.__doc__ = util.wrap_doc(\n    EfficientDetModelSpec,\n    'Creates EfficientDet-Lite0 model spec. See also: `tflite_model_maker.object_detector.EfficientDetSpec`.'\n)\nmm_export('object_detector.EfficientDetLite0Spec').export_constant(\n    __name__, 'efficientdet_lite0_spec')\nefficientdet_lite1_spec = functools.partial(\n    EfficientDetModelSpec,\n    model_name='efficientdet-lite1',\n    uri='https://tfhub.dev/tensorflow/efficientdet/lite1/feature-vector/1',",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "efficientdet_lite1_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "efficientdet_lite1_spec = functools.partial(\n    EfficientDetModelSpec,\n    model_name='efficientdet-lite1',\n    uri='https://tfhub.dev/tensorflow/efficientdet/lite1/feature-vector/1',\n)\nefficientdet_lite1_spec.__doc__ = util.wrap_doc(\n    EfficientDetModelSpec,\n    'Creates EfficientDet-Lite1 model spec. See also: `tflite_model_maker.object_detector.EfficientDetSpec`.'\n)\nmm_export('object_detector.EfficientDetLite1Spec').export_constant(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "efficientdet_lite1_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "efficientdet_lite1_spec.__doc__ = util.wrap_doc(\n    EfficientDetModelSpec,\n    'Creates EfficientDet-Lite1 model spec. See also: `tflite_model_maker.object_detector.EfficientDetSpec`.'\n)\nmm_export('object_detector.EfficientDetLite1Spec').export_constant(\n    __name__, 'efficientdet_lite1_spec')\nefficientdet_lite2_spec = functools.partial(\n    EfficientDetModelSpec,\n    model_name='efficientdet-lite2',\n    uri='https://tfhub.dev/tensorflow/efficientdet/lite2/feature-vector/1',",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "efficientdet_lite2_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "efficientdet_lite2_spec = functools.partial(\n    EfficientDetModelSpec,\n    model_name='efficientdet-lite2',\n    uri='https://tfhub.dev/tensorflow/efficientdet/lite2/feature-vector/1',\n)\nefficientdet_lite2_spec.__doc__ = util.wrap_doc(\n    EfficientDetModelSpec,\n    'Creates EfficientDet-Lite2 model spec. See also: `tflite_model_maker.object_detector.EfficientDetSpec`.'\n)\nmm_export('object_detector.EfficientDetLite2Spec').export_constant(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "efficientdet_lite2_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "efficientdet_lite2_spec.__doc__ = util.wrap_doc(\n    EfficientDetModelSpec,\n    'Creates EfficientDet-Lite2 model spec. See also: `tflite_model_maker.object_detector.EfficientDetSpec`.'\n)\nmm_export('object_detector.EfficientDetLite2Spec').export_constant(\n    __name__, 'efficientdet_lite2_spec')\nefficientdet_lite3_spec = functools.partial(\n    EfficientDetModelSpec,\n    model_name='efficientdet-lite3',\n    uri='https://tfhub.dev/tensorflow/efficientdet/lite3/feature-vector/1',",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "efficientdet_lite3_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "efficientdet_lite3_spec = functools.partial(\n    EfficientDetModelSpec,\n    model_name='efficientdet-lite3',\n    uri='https://tfhub.dev/tensorflow/efficientdet/lite3/feature-vector/1',\n)\nefficientdet_lite3_spec.__doc__ = util.wrap_doc(\n    EfficientDetModelSpec,\n    'Creates EfficientDet-Lite3 model spec. See also: `tflite_model_maker.object_detector.EfficientDetSpec`.'\n)\nmm_export('object_detector.EfficientDetLite3Spec').export_constant(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "efficientdet_lite3_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "efficientdet_lite3_spec.__doc__ = util.wrap_doc(\n    EfficientDetModelSpec,\n    'Creates EfficientDet-Lite3 model spec. See also: `tflite_model_maker.object_detector.EfficientDetSpec`.'\n)\nmm_export('object_detector.EfficientDetLite3Spec').export_constant(\n    __name__, 'efficientdet_lite3_spec')\nefficientdet_lite4_spec = functools.partial(\n    EfficientDetModelSpec,\n    model_name='efficientdet-lite4',\n    uri='https://tfhub.dev/tensorflow/efficientdet/lite4/feature-vector/2',",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "efficientdet_lite4_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "efficientdet_lite4_spec = functools.partial(\n    EfficientDetModelSpec,\n    model_name='efficientdet-lite4',\n    uri='https://tfhub.dev/tensorflow/efficientdet/lite4/feature-vector/2',\n)\nefficientdet_lite4_spec.__doc__ = util.wrap_doc(\n    EfficientDetModelSpec,\n    'Creates EfficientDet-Lite4 model spec. See also: `tflite_model_maker.object_detector.EfficientDetSpec`.'\n)\nmm_export('object_detector.EfficientDetLite4Spec').export_constant(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "efficientdet_lite4_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "peekOfCode": "efficientdet_lite4_spec.__doc__ = util.wrap_doc(\n    EfficientDetModelSpec,\n    'Creates EfficientDet-Lite4 model spec. See also: `tflite_model_maker.object_detector.EfficientDetSpec`.'\n)\nmm_export('object_detector.EfficientDetLite4Spec').export_constant(\n    __name__, 'efficientdet_lite4_spec')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec",
        "documentation": {}
    },
    {
        "label": "EfficientDetModelSpecTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec_test",
        "peekOfCode": "class EfficientDetModelSpecTest(tf.test.TestCase):\n  @classmethod\n  def setUpClass(cls):\n    super(EfficientDetModelSpecTest, cls).setUpClass()\n    hub_path = test_util.get_test_data_path('fake_effdet_lite0_hub')\n    cls._spec = object_detector_spec.EfficientDetModelSpec(\n        model_name='efficientdet-lite0', uri=hub_path, hparams=dict(map_freq=1))\n    with cls._spec.ds_strategy.scope():\n      cls.model = cls._spec.create_model()\n  def test_export_saved_model(self):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.object_detector_spec_test",
        "documentation": {}
    },
    {
        "label": "RecommendationSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.recommendation_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.recommendation_spec",
        "peekOfCode": "class RecommendationSpec(object):\n  \"\"\"Recommendation model spec.\"\"\"\n  compat_tf_versions = compat.get_compat_tf_versions(2)\n  def __init__(self, input_spec: recommendation_config.InputSpec,\n               model_hparams: recommendation_config.ModelHParams):\n    \"\"\"Initialize spec.\n    Args:\n      input_spec: InputSpec, specify data format for input and embedding.\n      model_hparams: ModelHParams, specify hparams for model achitecture.\n    \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.recommendation_spec",
        "documentation": {}
    },
    {
        "label": "RecommendationSpecTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.recommendation_spec_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.recommendation_spec_test",
        "peekOfCode": "class RecommendationSpecTest(tf.test.TestCase, parameterized.TestCase):\n  @parameterized.parameters(\n      ('bow'),\n      ('cnn'),\n      ('lstm'),\n  )\n  def test_create_recommendation_model(self, encoder_type):\n    input_spec = _testutil.get_input_spec(encoder_type)\n    model_hparams = _testutil.get_model_hparams()\n    spec = recommendation_spec.RecommendationSpec(input_spec, model_hparams)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.recommendation_spec_test",
        "documentation": {}
    },
    {
        "label": "AverageWordVecModelSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "class AverageWordVecModelSpec(object):\n  \"\"\"A specification of averaging word vector model.\"\"\"\n  PAD = '<PAD>'  # Index: 0\n  START = '<START>'  # Index: 1\n  UNKNOWN = '<UNKNOWN>'  # Index: 2\n  compat_tf_versions = compat.get_compat_tf_versions(2)\n  need_gen_vocab = True\n  convert_from_saved_model_tf2 = False\n  def __init__(self,\n               num_words=10000,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "BertModelSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "class BertModelSpec(object):\n  \"\"\"A specification of BERT model.\"\"\"\n  compat_tf_versions = compat.get_compat_tf_versions(2)\n  need_gen_vocab = False\n  convert_from_saved_model_tf2 = True  # Convert to TFLite from saved_model.\n  def __init__(\n      self,\n      uri='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1',\n      model_dir=None,\n      seq_len=128,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "BertClassifierModelSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "class BertClassifierModelSpec(BertModelSpec):\n  \"\"\"A specification of BERT model for text classification.\"\"\"\n  def __init__(\n      self,\n      uri='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1',\n      model_dir=None,\n      seq_len=128,\n      dropout_rate=0.1,\n      initializer_range=0.02,\n      learning_rate=3e-5,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "BertQAModelSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "class BertQAModelSpec(BertModelSpec):\n  \"\"\"A specification of BERT model for question answering.\"\"\"\n  def __init__(\n      self,\n      uri='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1',\n      model_dir=None,\n      seq_len=384,\n      query_len=64,\n      doc_stride=128,\n      dropout_rate=0.1,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "create_classifier_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "def create_classifier_model(bert_config,\n                            num_labels,\n                            max_seq_length,\n                            initializer=None,\n                            hub_module_url=None,\n                            hub_module_trainable=True,\n                            is_tf2=True):\n  \"\"\"BERT classifier model in functional API style.\n  Construct a Keras model for predicting `num_labels` outputs from an input with\n  maximum sequence length `max_seq_length`.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "dump_to_files",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "def dump_to_files(all_predictions, all_nbest_json, scores_diff_json,\n                  version_2_with_negative, output_dir):\n  \"\"\"Save output to json files for question answering.\"\"\"\n  output_prediction_file = os.path.join(output_dir, 'predictions.json')\n  output_nbest_file = os.path.join(output_dir, 'nbest_predictions.json')\n  output_null_log_odds_file = os.path.join(output_dir, 'null_odds.json')\n  tf.compat.v1.logging.info('Writing predictions to: %s',\n                            (output_prediction_file))\n  tf.compat.v1.logging.info('Writing nbest to: %s', (output_nbest_file))\n  squad_lib.write_to_json_files(all_predictions, output_prediction_file)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "create_qa_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "def create_qa_model(bert_config,\n                    max_seq_length,\n                    initializer=None,\n                    hub_module_url=None,\n                    hub_module_trainable=True,\n                    is_tf2=True):\n  \"\"\"Returns BERT qa model along with core BERT model to import weights.\n  Args:\n    bert_config: BertConfig, the config defines the core Bert model.\n    max_seq_length: integer, the maximum input sequence length.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "create_qa_model_from_squad",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "def create_qa_model_from_squad(max_seq_length,\n                               hub_module_url,\n                               hub_module_trainable=True,\n                               is_tf2=False):\n  \"\"\"Creates QA model the initialized from the model retrained on Squad dataset.\n  Args:\n    max_seq_length: integer, the maximum input sequence length.\n    hub_module_url: TF-Hub path/url to Bert module that's retrained on Squad\n      dataset.\n    hub_module_trainable: True to finetune layers in the hub module.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "mobilebert_classifier_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "mobilebert_classifier_spec = functools.partial(\n    BertClassifierModelSpec,\n    uri='https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/1',\n    is_tf2=False,\n    distribution_strategy='off',\n    name='MobileBert',\n    default_batch_size=48,\n)\nmobilebert_classifier_spec.__doc__ = util.wrap_doc(\n    BertClassifierModelSpec,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "mobilebert_classifier_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "mobilebert_classifier_spec.__doc__ = util.wrap_doc(\n    BertClassifierModelSpec,\n    'Creates MobileBert model spec for the text classification task. See also: `tflite_model_maker.text_classifier.BertClassifierSpec`.'\n)\nmm_export('text_classifier.MobileBertClassifierSpec').export_constant(\n    __name__, 'mobilebert_classifier_spec')\nmobilebert_qa_spec = functools.partial(\n    BertQAModelSpec,\n    uri='https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/1',\n    is_tf2=False,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "mobilebert_qa_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "mobilebert_qa_spec = functools.partial(\n    BertQAModelSpec,\n    uri='https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/1',\n    is_tf2=False,\n    distribution_strategy='off',\n    learning_rate=4e-05,\n    name='MobileBert',\n    default_batch_size=32,\n)\nmobilebert_qa_spec.__doc__ = util.wrap_doc(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "mobilebert_qa_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "mobilebert_qa_spec.__doc__ = util.wrap_doc(\n    BertQAModelSpec,\n    'Creates MobileBert model spec for the question answer task. See also: `tflite_model_maker.question_answer.BertQaSpec`.'\n)\nmm_export('question_answer.MobileBertQaSpec').export_constant(\n    __name__, 'mobilebert_qa_spec')\nmobilebert_qa_squad_spec = functools.partial(\n    BertQAModelSpec,\n    uri='https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/squadv1/1',\n    is_tf2=False,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "mobilebert_qa_squad_spec",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "mobilebert_qa_squad_spec = functools.partial(\n    BertQAModelSpec,\n    uri='https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/squadv1/1',\n    is_tf2=False,\n    distribution_strategy='off',\n    learning_rate=4e-05,\n    name='MobileBert',\n    init_from_squad_model=True,\n    default_batch_size=32,\n)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "mobilebert_qa_squad_spec.__doc__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "peekOfCode": "mobilebert_qa_squad_spec.__doc__ = util.wrap_doc(\n    BertQAModelSpec,\n    'Creates MobileBert model spec that\\'s already retrained on SQuAD1.1 for '\n    'the question answer task. See also: `tflite_model_maker.question_answer.BertQaSpec`.'\n)\nmm_export('question_answer.MobileBertQaSquadSpec').export_constant(\n    __name__, 'mobilebert_qa_squad_spec')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec",
        "documentation": {}
    },
    {
        "label": "AverageWordVecModelSpecTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec_test",
        "peekOfCode": "class AverageWordVecModelSpecTest(tf.test.TestCase):\n  def setUp(self):\n    super(AverageWordVecModelSpecTest, self).setUp()\n    self.model_spec = text_spec.AverageWordVecModelSpec(seq_len=5)\n    self.vocab = collections.OrderedDict(\n        (('<PAD>', 0), ('<START>', 1), ('<UNKNOWN>', 2), ('good', 3), ('bad',\n                                                                       4)))\n    self.model_spec.vocab = self.vocab\n  def test_tokenize(self):\n    model_spec = text_spec.AverageWordVecModelSpec()",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec_test",
        "documentation": {}
    },
    {
        "label": "BertClassifierModelSpecTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec_test",
        "peekOfCode": "class BertClassifierModelSpecTest(tf.test.TestCase, parameterized.TestCase):\n  @parameterized.parameters(\n      ('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', True),\n      ('https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1', False),\n  )\n  def test_bert(self, uri, is_tf2):\n    model_spec = text_spec.BertClassifierModelSpec(\n        uri, is_tf2=is_tf2, distribution_strategy='off', seq_len=3)\n    self._test_convert_examples_to_features(model_spec)\n    self._test_run_classifier(model_spec)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec_test",
        "documentation": {}
    },
    {
        "label": "dict_with_default",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.util",
        "peekOfCode": "def dict_with_default(default_dict, **updates):\n  default_dict.update(updates)\n  return default_dict\ndef create_int_feature(values):\n  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n  return feature\ndef get_num_gpus(num_gpus):\n  try:\n    tot_num_gpus = len(tf.config.experimental.list_physical_devices('GPU'))\n  except (tf.errors.NotFoundError, tf.errors.InternalError):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.util",
        "documentation": {}
    },
    {
        "label": "create_int_feature",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.util",
        "peekOfCode": "def create_int_feature(values):\n  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n  return feature\ndef get_num_gpus(num_gpus):\n  try:\n    tot_num_gpus = len(tf.config.experimental.list_physical_devices('GPU'))\n  except (tf.errors.NotFoundError, tf.errors.InternalError):\n    tot_num_gpus = max(0, num_gpus)\n  if num_gpus > tot_num_gpus or num_gpus == -1:\n    num_gpus = tot_num_gpus",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.util",
        "documentation": {}
    },
    {
        "label": "get_num_gpus",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.util",
        "peekOfCode": "def get_num_gpus(num_gpus):\n  try:\n    tot_num_gpus = len(tf.config.experimental.list_physical_devices('GPU'))\n  except (tf.errors.NotFoundError, tf.errors.InternalError):\n    tot_num_gpus = max(0, num_gpus)\n  if num_gpus > tot_num_gpus or num_gpus == -1:\n    num_gpus = tot_num_gpus\n  return num_gpus\ndef wrap_doc(func_or_class, short_desciption):\n  \"\"\"Wrap doc string of function or class, and replace short description.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.util",
        "documentation": {}
    },
    {
        "label": "wrap_doc",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.util",
        "peekOfCode": "def wrap_doc(func_or_class, short_desciption):\n  \"\"\"Wrap doc string of function or class, and replace short description.\"\"\"\n  if inspect.isfunction(func_or_class):\n    doc = func_or_class.__doc__\n  elif inspect.isclass(func_or_class):\n    doc = func_or_class.__init__.__doc__\n  else:\n    raise ValueError('Only support function or classtion, but got: {}.'.format(\n        func_or_class))\n  if not doc:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_spec.util",
        "documentation": {}
    },
    {
        "label": "AudioClassifier",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier",
        "peekOfCode": "class AudioClassifier(classification_model.ClassificationModel):\n  \"\"\"Audio classifier for training/inference and exporing.\"\"\"\n  # TODO(b/171848856): Add TFJS export.\n  DEFAULT_EXPORT_FORMAT = (ExportFormat.TFLITE)\n  ALLOWED_EXPORT_FORMAT = (ExportFormat.LABEL, ExportFormat.TFLITE,\n                           ExportFormat.SAVED_MODEL)\n  def train(self, train_data, validation_data, epochs, batch_size):\n    # TODO(b/171449557): Upstream this to the parent class.\n    if len(train_data) < batch_size:\n      raise ValueError('The size of the train_data (%d) couldn\\'t be smaller '",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier",
        "documentation": {}
    },
    {
        "label": "create",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier",
        "peekOfCode": "create = AudioClassifier.create\nmm_export('audio_classifier.create').export_constant(__name__, 'create')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier",
        "documentation": {}
    },
    {
        "label": "BrowserFFTWithoutPreprocessing",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier_test",
        "peekOfCode": "class BrowserFFTWithoutPreprocessing(audio_spec.BrowserFFTSpec):\n  def preprocess_ds(self, ds, is_training=False, cache_fn=None):\n    _ = is_training\n    @tf.function\n    def _crop(wav, label):\n      wav = wav[:self.EXPECTED_WAVEFORM_LENGTH]\n      return wav, label\n    ds = ds.map(_crop)\n    if cache_fn:\n      ds = cache_fn(ds)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier_test",
        "documentation": {}
    },
    {
        "label": "YAMNetWithoutPreprcessing",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier_test",
        "peekOfCode": "class YAMNetWithoutPreprcessing(audio_spec.YAMNetSpec):\n  def preprocess_ds(self, ds, is_training=False, cache_fn=None):\n    @tf.function\n    def _crop(wav, label):\n      wav = wav[:audio_spec.YAMNetSpec.EXPECTED_WAVEFORM_LENGTH]\n      return wav, label\n    ds = ds.map(_crop)\n    return ds\ndef write_sample(root,\n                 category,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier_test",
        "documentation": {}
    },
    {
        "label": "AudioClassifierTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier_test",
        "peekOfCode": "class AudioClassifierTest(tf.test.TestCase):\n  def testBrowserFFT(self):\n    self._test_spec(audio_spec.BrowserFFTSpec(),\n                    BrowserFFTWithoutPreprocessing())\n  def testYAMNet(self):\n    self._test_spec(audio_spec.YAMNetSpec(), YAMNetWithoutPreprcessing())\n  def testConfusionMatrix(self):\n    spec = audio_spec.BrowserFFTSpec()\n    temp_folder = self.get_temp_dir()\n    cat1 = write_sample(temp_folder, 'cat', '1.wav', 44100, duration_sec=1)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier_test",
        "documentation": {}
    },
    {
        "label": "write_sample",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier_test",
        "peekOfCode": "def write_sample(root,\n                 category,\n                 file_name,\n                 sample_rate,\n                 duration_sec,\n                 dtype=np.int16):\n  os.makedirs(os.path.join(root, category), exist_ok=True)\n  xs = np.random.rand(int(sample_rate * duration_sec),) * (1 << 15)\n  xs = xs.astype(dtype)\n  full_path = os.path.join(root, category, file_name)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.audio_classifier_test",
        "documentation": {}
    },
    {
        "label": "ClassificationModel",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.classification_model",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.classification_model",
        "peekOfCode": "class ClassificationModel(custom_model.CustomModel):\n  \"\"\"\"The abstract base class that represents a Tensorflow classification model.\"\"\"\n  DEFAULT_EXPORT_FORMAT = (ExportFormat.TFLITE, ExportFormat.LABEL)\n  ALLOWED_EXPORT_FORMAT = (ExportFormat.TFLITE, ExportFormat.LABEL,\n                           ExportFormat.SAVED_MODEL, ExportFormat.TFJS)\n  def __init__(self, model_spec, index_to_label, shuffle, train_whole_model):\n    \"\"\"Initialize a instance with data, deploy mode and other related parameters.\n    Args:\n      model_spec: Specification for the model.\n      index_to_label: A list that map from index to label class name.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.classification_model",
        "documentation": {}
    },
    {
        "label": "MockClassificationModel",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.classification_model_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.classification_model_test",
        "peekOfCode": "class MockClassificationModel(classification_model.ClassificationModel):\n  def train(self, train_data, validation_data=None, **kwargs):\n    pass\n  def export(self, **kwargs):\n    pass\n  def evaluate(self, data, **kwargs):\n    pass\nclass ClassificationModelTest(tf.test.TestCase):\n  def setUp(self):\n    super(ClassificationModelTest, self).setUp()",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.classification_model_test",
        "documentation": {}
    },
    {
        "label": "ClassificationModelTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.classification_model_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.classification_model_test",
        "peekOfCode": "class ClassificationModelTest(tf.test.TestCase):\n  def setUp(self):\n    super(ClassificationModelTest, self).setUp()\n    self.num_classes = 2\n    self.model = MockClassificationModel(\n        model_spec=None,\n        index_to_label=['pos', 'neg'],\n        train_whole_model=False,\n        shuffle=False)\n  def test_predict_top_k(self):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.classification_model_test",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.configs",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.configs",
        "peekOfCode": "class QuantizationConfig(object):\n  \"\"\"Configuration for post-training quantization.\n  Refer to\n  https://www.tensorflow.org/lite/performance/post_training_quantization\n  for different post-training quantization options.\n  \"\"\"\n  def __init__(\n      self,\n      optimizations=None,\n      representative_data=None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.configs",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUANTIZATION_STEPS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.configs",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.configs",
        "peekOfCode": "DEFAULT_QUANTIZATION_STEPS = 500\ndef _get_representative_dataset_gen(dataset, num_steps):\n  \"\"\"Gets the function that generates representative dataset for quantized.\"\"\"\n  def representative_dataset_gen():\n    \"\"\"Generates representative dataset for quantized.\"\"\"\n    if compat.get_tf_behavior() == 2:\n      for image, _ in dataset.take(num_steps):\n        yield [image]\n    else:\n      iterator = tf.compat.v1.data.make_initializable_iterator(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.configs",
        "documentation": {}
    },
    {
        "label": "QuantizationConfigType",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.configs",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.configs",
        "peekOfCode": "QuantizationConfigType = Optional[Union[str, QuantizationConfig]]",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.configs",
        "documentation": {}
    },
    {
        "label": "CustomModel",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.custom_model",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.custom_model",
        "peekOfCode": "class CustomModel(abc.ABC):\n  \"\"\"\"The abstract base class that represents a Tensorflow classification model.\"\"\"\n  DEFAULT_EXPORT_FORMAT = (ExportFormat.TFLITE)\n  ALLOWED_EXPORT_FORMAT = (ExportFormat.TFLITE, ExportFormat.SAVED_MODEL,\n                           ExportFormat.TFJS)\n  def __init__(self, model_spec, shuffle):\n    \"\"\"Initialize a instance with data, deploy mode and other related parameters.\n    Args:\n      model_spec: Specification for the model.\n      shuffle: Whether the training data should be shuffled.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.custom_model",
        "documentation": {}
    },
    {
        "label": "MockCustomModel",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.custom_model_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.custom_model_test",
        "peekOfCode": "class MockCustomModel(custom_model.CustomModel):\n  DEFAULT_EXPORT_FORMAT = (ExportFormat.TFLITE, ExportFormat.LABEL)\n  ALLOWED_EXPORT_FORMAT = (ExportFormat.TFLITE, ExportFormat.LABEL,\n                           ExportFormat.SAVED_MODEL, ExportFormat.TFJS)\n  def _export_labels(self, label_filepath):\n    with open(label_filepath, 'w') as f:\n      f.write('0\\n')\n  def train(self, train_data, validation_data=None, **kwargs):\n    pass\n  def evaluate(self, data, **kwargs):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.custom_model_test",
        "documentation": {}
    },
    {
        "label": "CustomModelTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.custom_model_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.custom_model_test",
        "peekOfCode": "class CustomModelTest(tf.test.TestCase):\n  def setUp(self):\n    super(CustomModelTest, self).setUp()\n    self.model = MockCustomModel(\n        model_spec=None,\n        shuffle=False)\n    self.model.model = test_util.build_model(input_shape=[4], num_classes=2)\n  def _check_nonempty_dir(self, dirpath):\n    self.assertTrue(os.path.isdir(dirpath))\n    self.assertNotEmpty(os.listdir(dirpath))",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.custom_model_test",
        "documentation": {}
    },
    {
        "label": "HubKerasLayerV1V2",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.hub_loader",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.hub_loader",
        "peekOfCode": "class HubKerasLayerV1V2(hub.KerasLayer):\n  \"\"\"Class to loads TF v1 and TF v1 hub modules that could be fine-tuned.\n  Since TF v1 modules couldn't be retrained in hub.KerasLayer. This class\n  provides a workaround for retraining the whole tf1 model in tf2. In\n  particular, it extract self._func._self_unconditional_checkpoint_dependencies\n  into trainable variable in tf1.\n  Doesn't update moving-mean/moving-variance for BatchNormalization during\n  fine-tuning.\n  \"\"\"\n  def _setup_layer(self, trainable=False, **kwargs):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.hub_loader",
        "documentation": {}
    },
    {
        "label": "HubKerasLayerV1V2Test",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.hub_loader_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.hub_loader_test",
        "peekOfCode": "class HubKerasLayerV1V2Test(tf.test.TestCase, parameterized.TestCase):\n  @parameterized.parameters(\n      (\"hub_module_v1_mini\", True),\n      (\"saved_model_v2_mini\", True),\n      (\"hub_module_v1_mini\", False),\n      (\"saved_model_v2_mini\", False),\n  )\n  def test_load_with_defaults(self, module_name, trainable):\n    inputs, expected_outputs = 10., 11.  # Test modules perform increment op.\n    path = test_util.get_test_data_path(module_name)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.hub_loader_test",
        "documentation": {}
    },
    {
        "label": "ImageClassifier",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier",
        "peekOfCode": "class ImageClassifier(classification_model.ClassificationModel):\n  \"\"\"ImageClassifier class for inference and exporting to tflite.\"\"\"\n  def __init__(self,\n               model_spec,\n               index_to_label,\n               shuffle=True,\n               hparams=hub_lib.get_default_hparams(),\n               use_augmentation=False,\n               representative_data=None):\n    \"\"\"Init function for ImageClassifier class.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier",
        "documentation": {}
    },
    {
        "label": "get_hub_lib_hparams",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier",
        "peekOfCode": "def get_hub_lib_hparams(**kwargs):\n  \"\"\"Gets the hyperparameters for the tensorflow hub's library.\"\"\"\n  hparams = hub_lib.get_default_hparams()\n  return train_image_classifier_lib.add_params(hparams, **kwargs)\ndef _get_model_info(model_spec,\n                    num_classes,\n                    quantization_config=None,\n                    version='v1'):\n  \"\"\"Gets the specific info for the image model.\"\"\"\n  if not isinstance(model_spec, image_spec.ImageModelSpec):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier",
        "documentation": {}
    },
    {
        "label": "create",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier",
        "peekOfCode": "create = ImageClassifier.create\nmm_export('image_classifier.create').export_constant(__name__, 'create')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier",
        "documentation": {}
    },
    {
        "label": "ImageClassifierTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier_test",
        "peekOfCode": "class ImageClassifierTest(tf.test.TestCase):\n  IMAGE_SIZE = 24\n  IMAGES_PER_CLASS = 2\n  CMY_NAMES_AND_RGB_VALUES = (('cyan', (0, 255, 255)),\n                              ('magenta', (255, 0, 255)), ('yellow', (255, 255,\n                                                                      0)))\n  def _gen(self):\n    for i, (_, rgb) in enumerate(self.CMY_NAMES_AND_RGB_VALUES):\n      for _ in range(self.IMAGES_PER_CLASS):\n        yield (_fill_image(rgb, self.IMAGE_SIZE), i)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier_test",
        "documentation": {}
    },
    {
        "label": "ImageClassifierV1Test",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier_v1_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier_v1_test",
        "peekOfCode": "class ImageClassifierV1Test(image_classifier_test.ImageClassifierTest):\n  \"\"\"Share image tests of the base class, but in tf v1 behavior.\"\"\"\nif __name__ == '__main__':\n  compat.setup_tf_behavior(tf_version=1)\n  tf.test.main()",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.image_classifier_v1_test",
        "documentation": {}
    },
    {
        "label": "Preprocessor",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "peekOfCode": "class Preprocessor(object):\n  \"\"\"Preprocessing for image classification.\"\"\"\n  def __init__(self,\n               input_shape,\n               num_classes,\n               mean_rgb,\n               stddev_rgb,\n               use_augmentation=False):\n    self.input_shape = input_shape\n    self.num_classes = num_classes",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "documentation": {}
    },
    {
        "label": "distorted_bounding_box_crop",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "peekOfCode": "def distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100):\n  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n  Args:\n    image: 4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "documentation": {}
    },
    {
        "label": "preprocess_for_train",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "peekOfCode": "def preprocess_for_train(image,\n                         image_size=IMAGE_SIZE,\n                         resize_method=tf.image.ResizeMethod.BILINEAR):\n  \"\"\"Preprocesses the given image for evaluation.\n  Args:\n    image: 4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\n      of shape [height, width, channels].\n    image_size: image size.\n    resize_method: resize method. If none, use bicubic.\n  Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "documentation": {}
    },
    {
        "label": "preprocess_for_eval",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "peekOfCode": "def preprocess_for_eval(image,\n                        image_size=IMAGE_SIZE,\n                        resize_method=tf.image.ResizeMethod.BILINEAR):\n  \"\"\"Preprocesses the given image for evaluation.\n  Args:\n    image: 4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\n      of shape [height, width, channels].\n    image_size: image size.\n    resize_method: if None, use bicubic.\n  Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "documentation": {}
    },
    {
        "label": "IMAGE_SIZE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "peekOfCode": "IMAGE_SIZE = 224\nCROP_PADDING = 32\nclass Preprocessor(object):\n  \"\"\"Preprocessing for image classification.\"\"\"\n  def __init__(self,\n               input_shape,\n               num_classes,\n               mean_rgb,\n               stddev_rgb,\n               use_augmentation=False):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "documentation": {}
    },
    {
        "label": "CROP_PADDING",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "peekOfCode": "CROP_PADDING = 32\nclass Preprocessor(object):\n  \"\"\"Preprocessing for image classification.\"\"\"\n  def __init__(self,\n               input_shape,\n               num_classes,\n               mean_rgb,\n               stddev_rgb,\n               use_augmentation=False):\n    self.input_shape = input_shape",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing",
        "documentation": {}
    },
    {
        "label": "PreprocessorTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing_test",
        "peekOfCode": "class PreprocessorTest(tf.test.TestCase):\n  def test_without_augmentation(self):\n    preprocessor = image_preprocessing.Preprocessor([2, 2],\n                                                    2,\n                                                    mean_rgb=[0.0],\n                                                    stddev_rgb=[255.0],\n                                                    use_augmentation=False)\n    actual_image = np.array([[[0., 0.00392157, 0.00784314],\n                              [0.14117648, 0.14509805, 0.14901961]],\n                             [[0.37647063, 0.3803922, 0.38431376],",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.image_preprocessing_test",
        "documentation": {}
    },
    {
        "label": "ModelSpecificInfo",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "peekOfCode": "class ModelSpecificInfo(object):\n  \"\"\"Holds information that is specificly tied to an image classifier.\"\"\"\n  def __init__(self, name, version, image_width, image_height, image_min,\n               image_max, mean, std, num_classes, author):\n    self.name = name\n    self.version = version\n    self.image_width = image_width\n    self.image_height = image_height\n    self.image_min = image_min\n    self.image_max = image_max",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "documentation": {}
    },
    {
        "label": "MetadataPopulatorForImageClassifier",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "peekOfCode": "class MetadataPopulatorForImageClassifier(object):\n  \"\"\"Populates the metadata for an image classifier.\"\"\"\n  def __init__(self, model_file, model_info, label_file_path):\n    self.model_file = model_file\n    self.model_info = model_info\n    self.label_file_path = label_file_path\n    self.metadata_buf = None\n  def populate(self):\n    \"\"\"Creates metadata and then populates it for an image classifier.\"\"\"\n    self._create_metadata()",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "peekOfCode": "def define_flags():\n  flags.DEFINE_string(\"model_file\", None,\n                      \"Path and file name to the TFLite model file.\")\n  flags.DEFINE_string(\"label_file\", None, \"Path to the label file.\")\n  flags.DEFINE_string(\"export_directory\", None,\n                      \"Path to save the TFLite model files with metadata.\")\n  flags.mark_flag_as_required(\"model_file\")\n  flags.mark_flag_as_required(\"label_file\")\n  flags.mark_flag_as_required(\"export_directory\")\nclass ModelSpecificInfo(object):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "peekOfCode": "def main(_):\n  model_file = FLAGS.model_file\n  model_basename = os.path.basename(model_file)\n  if model_basename not in _MODEL_INFO:\n    raise ValueError(\n        \"The model info for, {0}, is not defined yet.\".format(model_basename))\n  export_model_path = os.path.join(FLAGS.export_directory, model_basename)\n  # Copies model_file to export_path.\n  tf.io.gfile.copy(model_file, export_model_path, overwrite=False)\n  # Generate the metadata objects and put them in the model file",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef define_flags():\n  flags.DEFINE_string(\"model_file\", None,\n                      \"Path and file name to the TFLite model file.\")\n  flags.DEFINE_string(\"label_file\", None, \"Path to the label file.\")\n  flags.DEFINE_string(\"export_directory\", None,\n                      \"Path to save the TFLite model files with metadata.\")\n  flags.mark_flag_as_required(\"model_file\")\n  flags.mark_flag_as_required(\"label_file\")\n  flags.mark_flag_as_required(\"export_directory\")",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "documentation": {}
    },
    {
        "label": "_MODEL_INFO",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "peekOfCode": "_MODEL_INFO = {\n    \"mobilenet_v1_0.75_160_quantized.tflite\":\n        ModelSpecificInfo(\n            name=\"MobileNetV1 image classifier\",\n            version=\"v1\",\n            image_width=160,\n            image_height=160,\n            image_min=0,\n            image_max=255,\n            mean=[127.5],",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.metadata_writer_for_image_classifier",
        "documentation": {}
    },
    {
        "label": "DummyContextManager",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "peekOfCode": "class DummyContextManager(object):\n  def __enter__(self):\n    pass\n  def __exit__(self, *args):\n    pass\ndef export_labels(filepath, index_to_label):\n  with tf.io.gfile.GFile(filepath, 'w') as f:\n    f.write('\\n'.join(index_to_label))\ndef export_saved_model(model,\n                       filepath,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "documentation": {}
    },
    {
        "label": "LiteRunner",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "peekOfCode": "class LiteRunner(object):\n  \"\"\"Runs inference with the TFLite model.\"\"\"\n  def __init__(self,\n               tflite_filepath,\n               reorder_input_details_fn=None,\n               reorder_output_details_fn=None):\n    \"\"\"Initializes Lite runner with tflite model file.\n    Args:\n      tflite_filepath: File path to the TFLite model.\n      reorder_input_details_fn: Function to reorder the input details to map the",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "documentation": {}
    },
    {
        "label": "set_batch_size",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "peekOfCode": "def set_batch_size(model, batch_size):\n  \"\"\"Sets batch size for the model.\"\"\"\n  for model_input in model.inputs:\n    new_shape = [batch_size] + model_input.shape[1:]\n    model_input.set_shape(new_shape)\ndef get_steps_per_epoch(steps_per_epoch=None, batch_size=None, train_data=None):\n  \"\"\"Gets the estimated training steps per epoch.\n  1. If `steps_per_epoch` is set, returns `steps_per_epoch` directly.\n  2. Else if we can get the length of training data successfully, returns\n     `train_data_length // batch_size`.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "documentation": {}
    },
    {
        "label": "get_steps_per_epoch",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "peekOfCode": "def get_steps_per_epoch(steps_per_epoch=None, batch_size=None, train_data=None):\n  \"\"\"Gets the estimated training steps per epoch.\n  1. If `steps_per_epoch` is set, returns `steps_per_epoch` directly.\n  2. Else if we can get the length of training data successfully, returns\n     `train_data_length // batch_size`.\n  3. Else if it fails to get the length of training data, return None.\n  Args:\n    steps_per_epoch: int, training steps per epoch.\n    batch_size: int, batch size.\n    train_data: training data.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "documentation": {}
    },
    {
        "label": "export_labels",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "peekOfCode": "def export_labels(filepath, index_to_label):\n  with tf.io.gfile.GFile(filepath, 'w') as f:\n    f.write('\\n'.join(index_to_label))\ndef export_saved_model(model,\n                       filepath,\n                       overwrite=True,\n                       include_optimizer=True,\n                       save_format=None,\n                       signatures=None,\n                       options=None):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "documentation": {}
    },
    {
        "label": "export_saved_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "peekOfCode": "def export_saved_model(model,\n                       filepath,\n                       overwrite=True,\n                       include_optimizer=True,\n                       save_format=None,\n                       signatures=None,\n                       options=None):\n  \"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.\n  Args:\n    model: Instance of a Keras model.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "documentation": {}
    },
    {
        "label": "export_tflite",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "peekOfCode": "def export_tflite(model,\n                  tflite_filepath,\n                  quantization_config=None,\n                  convert_from_saved_model_tf2=False,\n                  preprocess=None,\n                  supported_ops=(tf.lite.OpsSet.TFLITE_BUILTINS,)):\n  \"\"\"Converts the retrained model to tflite format and saves it.\n  Args:\n    model: model to be converted to tflite.\n    tflite_filepath: File path to save tflite model.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "documentation": {}
    },
    {
        "label": "get_lite_runner",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "peekOfCode": "def get_lite_runner(tflite_filepath, model_spec=None):\n  \"\"\"Gets `LiteRunner` from file path to TFLite model and `model_spec`.\"\"\"\n  # Gets the functions to handle the input & output indexes if exists.\n  reorder_input_details_fn = None\n  if hasattr(model_spec, 'reorder_input_details'):\n    reorder_input_details_fn = model_spec.reorder_input_details\n  reorder_output_details_fn = None\n  if hasattr(model_spec, 'reorder_output_details'):\n    reorder_output_details_fn = model_spec.reorder_output_details\n  lite_runner = LiteRunner(tflite_filepath, reorder_input_details_fn,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "documentation": {}
    },
    {
        "label": "export_tfjs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "peekOfCode": "def export_tfjs(keras_or_saved_model,\n                output_dir,\n                tflite_filepath=None,\n                **kwargs):\n  \"\"\"Exports saved model to tfjs.\n  https://www.tensorflow.org/js/guide/conversion?hl=en\n  Args:\n    keras_or_saved_model: Keras or saved model.\n    output_dir: Output TF.js model dir.\n    tflite_filepath: str, file path to existing tflite model. If set, the",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "documentation": {}
    },
    {
        "label": "load_tfjs_keras_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "peekOfCode": "def load_tfjs_keras_model(model_path):\n  \"\"\"Loads tfjs keras model from path.\"\"\"\n  return tfjs_converter.keras_tfjs_loader.load_keras_model(\n      model_path, load_weights=True)\ndef extract_tflite_metadata_json(tflite_filepath):\n  \"\"\"Extracts metadata from tflite model filepath.\n  Args:\n    tflite_filepath: str, path to tflite model file.\n  Returns:\n    str: tflite metadata json string.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "documentation": {}
    },
    {
        "label": "extract_tflite_metadata_json",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "peekOfCode": "def extract_tflite_metadata_json(tflite_filepath):\n  \"\"\"Extracts metadata from tflite model filepath.\n  Args:\n    tflite_filepath: str, path to tflite model file.\n  Returns:\n    str: tflite metadata json string.\n  \"\"\"\n  displayer = _metadata.MetadataDisplayer.with_model_file(tflite_filepath)\n  return displayer.get_metadata_json()",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "documentation": {}
    },
    {
        "label": "ESTIMITED_STEPS_PER_EPOCH",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "peekOfCode": "ESTIMITED_STEPS_PER_EPOCH = 1000\ndef set_batch_size(model, batch_size):\n  \"\"\"Sets batch size for the model.\"\"\"\n  for model_input in model.inputs:\n    new_shape = [batch_size] + model_input.shape[1:]\n    model_input.set_shape(new_shape)\ndef get_steps_per_epoch(steps_per_epoch=None, batch_size=None, train_data=None):\n  \"\"\"Gets the estimated training steps per epoch.\n  1. If `steps_per_epoch` is set, returns `steps_per_epoch` directly.\n  2. Else if we can get the length of training data successfully, returns",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util",
        "documentation": {}
    },
    {
        "label": "ModelUtilTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util_test",
        "peekOfCode": "class ModelUtilTest(tf.test.TestCase):\n  @test_util.test_in_tf_1and2\n  def test_export_tflite(self):\n    input_dim = 4\n    model = test_util.build_model(input_shape=[input_dim], num_classes=2)\n    tflite_file = os.path.join(self.get_temp_dir(), 'model.tflite')\n    model_util.export_tflite(model, tflite_file)\n    self._test_tflite(model, tflite_file, input_dim)\n  @test_util.test_in_tf_1and2\n  def test_export_tflite_quantized(self):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util_test",
        "documentation": {}
    },
    {
        "label": "ModelUtilTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.model_util_v1_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.model_util_v1_test",
        "peekOfCode": "class ModelUtilTest(model_util_test.ModelUtilTest):\n  \"\"\"Share text tests of the base class, but in tf v1 behavior.\"\"\"\nif __name__ == '__main__':\n  compat.setup_tf_behavior(tf_version=1)\n  tf.test.main()",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.model_util_v1_test",
        "documentation": {}
    },
    {
        "label": "ObjectDetector",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.object_detector",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.object_detector",
        "peekOfCode": "class ObjectDetector(custom_model.CustomModel):\n  \"\"\"ObjectDetector class for inference and exporting to tflite.\"\"\"\n  ALLOWED_EXPORT_FORMAT = (ExportFormat.TFLITE, ExportFormat.SAVED_MODEL,\n                           ExportFormat.LABEL)\n  def __init__(\n      self,\n      model_spec: object_detector_spec.EfficientDetModelSpec,\n      label_map: Dict[int, str],\n      representative_data: Optional[\n          object_detector_dataloader.DataLoader] = None",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.object_detector",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.object_detector",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.object_detector",
        "peekOfCode": "T = TypeVar('T', bound='ObjectDetector')\n@mm_export('object_detector.ObjectDetector')\nclass ObjectDetector(custom_model.CustomModel):\n  \"\"\"ObjectDetector class for inference and exporting to tflite.\"\"\"\n  ALLOWED_EXPORT_FORMAT = (ExportFormat.TFLITE, ExportFormat.SAVED_MODEL,\n                           ExportFormat.LABEL)\n  def __init__(\n      self,\n      model_spec: object_detector_spec.EfficientDetModelSpec,\n      label_map: Dict[int, str],",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.object_detector",
        "documentation": {}
    },
    {
        "label": "create",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.object_detector",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.object_detector",
        "peekOfCode": "create = ObjectDetector.create\nmm_export('object_detector.create').export_constant(__name__, 'create')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.object_detector",
        "documentation": {}
    },
    {
        "label": "ObjectDetectorTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.object_detector_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.object_detector_test",
        "peekOfCode": "class ObjectDetectorTest(tf.test.TestCase):\n  def testEfficientDetLite0(self):\n    # Gets model specification.\n    spec = model_spec.get('efficientdet_lite0')\n    # Prepare data.\n    images_dir, annotations_dir, label_map = test_util.create_pascal_voc(\n        self.get_temp_dir())\n    data = object_detector_dataloader.DataLoader.from_pascal_voc(\n        images_dir, annotations_dir, label_map)\n    # Train the model.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.object_detector_test",
        "documentation": {}
    },
    {
        "label": "QuestionAnswer",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.question_answer",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.question_answer",
        "peekOfCode": "class QuestionAnswer(custom_model.CustomModel):\n  \"\"\"QuestionAnswer class for inference and exporting to tflite.\"\"\"\n  DEFAULT_EXPORT_FORMAT = (ExportFormat.TFLITE, ExportFormat.VOCAB)\n  ALLOWED_EXPORT_FORMAT = (ExportFormat.TFLITE, ExportFormat.VOCAB,\n                           ExportFormat.SAVED_MODEL)\n  def train(self,\n            train_data,\n            epochs=None,\n            batch_size=None,\n            steps_per_epoch=None):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.question_answer",
        "documentation": {}
    },
    {
        "label": "create",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.question_answer",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.question_answer",
        "peekOfCode": "create = QuestionAnswer.create\nmm_export('question_answer.create').export_constant(__name__, 'create')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.question_answer",
        "documentation": {}
    },
    {
        "label": "QuestionAnswerTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.question_answer_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.question_answer_test",
        "peekOfCode": "class QuestionAnswerTest(tf.test.TestCase, parameterized.TestCase):\n  @test_util.test_in_tf_1\n  def test_bert_model_v1_incompatible(self):\n    with self.assertRaisesRegex(ValueError, 'Incompatible versions'):\n      text_spec.BertQAModelSpec(trainable=False)\n  @test_util.test_in_tf_2\n  def test_bert_model(self):\n    # Only test squad1.1 since it takes too long time for this.\n    version = '1.1'\n    model_spec = text_spec.BertQAModelSpec(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.question_answer_test",
        "documentation": {}
    },
    {
        "label": "QuestionAnswerV1Test",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.question_answer_v1_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.question_answer_v1_test",
        "peekOfCode": "class QuestionAnswerV1Test(question_answer_test.QuestionAnswerTest):\n  \"\"\"Share text tests of the base class, but in tf v1 behavior.\"\"\"\nif __name__ == '__main__':\n  compat.setup_tf_behavior(tf_version=1)\n  tf.test.main()",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.question_answer_v1_test",
        "documentation": {}
    },
    {
        "label": "Recommendation",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.recommendation",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.recommendation",
        "peekOfCode": "class Recommendation(custom_model.CustomModel):\n  \"\"\"Recommendation task class.\"\"\"\n  DEFAULT_EXPORT_FORMAT = (ExportFormat.TFLITE,)\n  ALLOWED_EXPORT_FORMAT = (ExportFormat.LABEL, ExportFormat.TFLITE,\n                           ExportFormat.SAVED_MODEL)\n  # ID = 0 means a placeholder to OOV. Used for padding.\n  OOV_ID = 0\n  def __init__(self,\n               model_spec,\n               model_dir,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.recommendation",
        "documentation": {}
    },
    {
        "label": "create",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.recommendation",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.recommendation",
        "peekOfCode": "create = Recommendation.create\nmm_export('recommendation.create').export_constant(__name__, 'create')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.recommendation",
        "documentation": {}
    },
    {
        "label": "RecommendationTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.recommendation_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.recommendation_test",
        "peekOfCode": "class RecommendationTest(parameterized.TestCase, tf.test.TestCase):\n  def setUp(self):\n    super().setUp()\n    _testutil.setup_fake_testdata(self)\n    self.input_spec = _testutil.get_input_spec()\n    self.model_hparams = _testutil.get_model_hparams()\n    self.train_loader = _dl.RecommendationDataLoader.from_movielens(\n        self.dataset_dir, 'train', self.input_spec)\n    self.test_loader = _dl.RecommendationDataLoader.from_movielens(\n        self.dataset_dir, 'test', self.input_spec)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.recommendation_test",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "peekOfCode": "class ExportFormat(enum.Enum):\n  TFLITE = \"TFLITE\"\n  SCANN_INDEX_FILE = \"SCANN_INDEX_FILE\"\n@mm_export(\"searcher.Tree\")\n@dataclasses.dataclass\nclass Tree:\n  \"\"\"K-Means partitioning tree configuration.\n  In ScaNN, we use single layer K-Means tree to partition the database (index)\n  as a way to reduce search space.\n  Attributes:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "documentation": {}
    },
    {
        "label": "Tree",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "peekOfCode": "class Tree:\n  \"\"\"K-Means partitioning tree configuration.\n  In ScaNN, we use single layer K-Means tree to partition the database (index)\n  as a way to reduce search space.\n  Attributes:\n    num_leaves: How many leaves (partitions) to have on the K-Means tree. In\n      general, a good starting point would be the square root of the database\n      size.\n    num_leaves_to_search: During inference ScaNN will compare the query vector\n      against all the partition centroids and select the closest",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "documentation": {}
    },
    {
        "label": "ScoreAH",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "peekOfCode": "class ScoreAH:\n  \"\"\"Product Quantization (PQ) based in-partition scoring configuration.\n  In ScaNN we use PQ to compress the database embeddings, but not the query\n  embedding. We called it Asymmetric Hashing. See\n  https://research.google/pubs/pub41694/\n  Attributes:\n    dimensions_per_block: How many dimensions in each PQ block. If the embedding\n      vector dimensionality is a multiple of this value, there will be\n      `number_of_dimensions / dimensions_per_block` PQ blocks. Otherwise, the\n      last block will be the remainder. For example, if a vector has 12",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "documentation": {}
    },
    {
        "label": "ScoreBruteForce",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "peekOfCode": "class ScoreBruteForce:\n  \"\"\"Bruce force in-partition scoring configuration.\n  There'll be no compression or quantization applied to the database\n  embeddings or query embeddings.\n  \"\"\"\n@mm_export(\"searcher.ScaNNOptions\")\n@dataclasses.dataclass\nclass ScaNNOptions:\n  \"\"\"Options to build ScaNN.\n  ScaNN",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "documentation": {}
    },
    {
        "label": "ScaNNOptions",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "peekOfCode": "class ScaNNOptions:\n  \"\"\"Options to build ScaNN.\n  ScaNN\n  (https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) is\n  a highly efficient and scalable vector nearest neighbor retrieval\n  library from Google Research. We use ScaNN to build the on-device search\n  index, and do on-device retrieval with a simplified implementation.\n  TODO(b/231134703) Add a link to the README\n  Attributes:\n    distance_measure: How to compute the distance. Allowed values are",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "documentation": {}
    },
    {
        "label": "Searcher",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "peekOfCode": "class Searcher(object):\n  \"\"\"Creates the similarity search model with ScaNN.\"\"\"\n  def __init__(self,\n               serialized_scann_path: str,\n               metadata: List[AnyStr],\n               embedder_path: Optional[str] = None) -> None:\n    \"\"\"Initializes the Searcher object.\n    Args:\n      serialized_scann_path: Path to the dir that contains the ScaNN's\n        artifacts.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "documentation": {}
    },
    {
        "label": "_USE_RESPONSE_ENCODING_INDEX",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "peekOfCode": "_USE_RESPONSE_ENCODING_INDEX = 1\n@mm_export(\"searcher.ExportFormat\")\n@enum.unique\nclass ExportFormat(enum.Enum):\n  TFLITE = \"TFLITE\"\n  SCANN_INDEX_FILE = \"SCANN_INDEX_FILE\"\n@mm_export(\"searcher.Tree\")\n@dataclasses.dataclass\nclass Tree:\n  \"\"\"K-Means partitioning tree configuration.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.searcher",
        "documentation": {}
    },
    {
        "label": "SearcherTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.searcher_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.searcher_test",
        "peekOfCode": "class SearcherTest(tf.test.TestCase, parameterized.TestCase):\n  def test_searcher_with_image_embedder(self):\n    tflite_path = test_util.get_test_data_path(\n        \"mobilenet_v2_035_96_embedder_with_metadata.tflite\")\n    image_folder = test_util.get_test_data_path(\"animals\")\n    # Creates the image searcher data loader.\n    data_loader = image_searcher_dataloader.DataLoader.create(tflite_path)\n    data_loader.load_from_folder(image_folder)\n    # Expands the data by 50x.\n    repeated_times = 500",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.searcher_test",
        "documentation": {}
    },
    {
        "label": "TextClassifier",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.text_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.text_classifier",
        "peekOfCode": "class TextClassifier(classification_model.ClassificationModel):\n  \"\"\"TextClassifier class for inference and exporting to tflite.\"\"\"\n  DEFAULT_EXPORT_FORMAT = (ExportFormat.TFLITE, ExportFormat.LABEL,\n                           ExportFormat.VOCAB)\n  ALLOWED_EXPORT_FORMAT = (ExportFormat.TFLITE, ExportFormat.LABEL,\n                           ExportFormat.VOCAB, ExportFormat.SAVED_MODEL,\n                           ExportFormat.TFJS)\n  def __init__(self,\n               model_spec,\n               index_to_label,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.text_classifier",
        "documentation": {}
    },
    {
        "label": "create",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.text_classifier",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.text_classifier",
        "peekOfCode": "create = TextClassifier.create\nmm_export('text_classifier.create').export_constant(__name__, 'create')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.text_classifier",
        "documentation": {}
    },
    {
        "label": "TextClassifierTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.text_classifier_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.text_classifier_test",
        "peekOfCode": "class TextClassifierTest(tf.test.TestCase):\n  TEST_LABELS_AND_TEXT = (('pos', 'super good'), ('neg', 'really bad.'))\n  def _gen_text_dir(self, text_per_class=1):\n    text_dir = os.path.join(self.get_temp_dir(), 'random_text_dir')\n    if os.path.exists(text_dir):\n      return text_dir\n    os.mkdir(text_dir)\n    for class_name, text in self.TEST_LABELS_AND_TEXT:\n      class_subdir = os.path.join(text_dir, class_name)\n      os.mkdir(class_subdir)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.text_classifier_test",
        "documentation": {}
    },
    {
        "label": "TextClassifierV1Test",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.text_classifier_v1_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.text_classifier_v1_test",
        "peekOfCode": "class TextClassifierV1Test(text_classifier_test.TextClassifierTest):\n  \"\"\"Share text tests of the base class, but in tf v1 behavior.\"\"\"\nif __name__ == '__main__':\n  compat.setup_tf_behavior(tf_version=1)\n  tf.test.main()",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.text_classifier_v1_test",
        "documentation": {}
    },
    {
        "label": "HParams",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "peekOfCode": "class HParams(\n    collections.namedtuple(\n        \"HParams\", hub_lib.HParams._fields + (\"warmup_steps\", \"model_dir\"))):\n  \"\"\"The hyperparameters for make_image_classifier.\n  train_epochs: Training will do this many iterations over the dataset.\n  do_fine_tuning: If true, the Hub module is trained together with the\n    classification layer on top.\n  batch_size: Each training step samples a batch of this many images.\n  learning_rate: Base learning rate when train batch size is 256. Linear to the\n    batch size.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "documentation": {}
    },
    {
        "label": "add_params",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "peekOfCode": "def add_params(hparams, **kwargs):\n  param_dict = {k: v for k, v in kwargs.items() if v is not None}\n  return hparams._replace(**param_dict)\nclass HParams(\n    collections.namedtuple(\n        \"HParams\", hub_lib.HParams._fields + (\"warmup_steps\", \"model_dir\"))):\n  \"\"\"The hyperparameters for make_image_classifier.\n  train_epochs: Training will do this many iterations over the dataset.\n  do_fine_tuning: If true, the Hub module is trained together with the\n    classification layer on top.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "documentation": {}
    },
    {
        "label": "get_default_hparams",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "peekOfCode": "def get_default_hparams():\n  \"\"\"Returns a fresh HParams object initialized to default values.\"\"\"\n  default_hub_hparams = hub_lib.get_default_hparams()\n  as_dict = default_hub_hparams._asdict()\n  as_dict.update(\n      train_epochs=10,\n      do_fine_tuning=False,\n      batch_size=64,\n      learning_rate=0.004,\n      dropout_rate=0.2,",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "documentation": {}
    },
    {
        "label": "create_optimizer",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "peekOfCode": "def create_optimizer(init_lr, num_decay_steps, num_warmup_steps):\n  \"\"\"Creates an optimizer with learning rate schedule.\"\"\"\n  # Leverages cosine decay of the learning rate.\n  learning_rate_fn = tf.keras.experimental.CosineDecay(\n      initial_learning_rate=init_lr, decay_steps=num_decay_steps, alpha=0.0)\n  if num_warmup_steps:\n    learning_rate_fn = warmup.WarmUp(\n        initial_learning_rate=init_lr,\n        decay_schedule_fn=learning_rate_fn,\n        warmup_steps=num_warmup_steps)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "documentation": {}
    },
    {
        "label": "get_default_callbacks",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "peekOfCode": "def get_default_callbacks(model_dir):\n  \"\"\"Gets default callbacks.\"\"\"\n  summary_dir = os.path.join(model_dir, \"summaries\")\n  summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)\n  # Save checkpoint every 20 epochs.\n  checkpoint_path = os.path.join(model_dir, \"checkpoint\")\n  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n      checkpoint_path, save_weights_only=True, period=20)\n  return [summary_callback, checkpoint_callback]\ndef hub_train_model(model, hparams, train_ds, validation_ds, steps_per_epoch):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "documentation": {}
    },
    {
        "label": "hub_train_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "peekOfCode": "def hub_train_model(model, hparams, train_ds, validation_ds, steps_per_epoch):\n  \"\"\"Trains model with the given data and hyperparameters.\n  If using a DistributionStrategy, call this under its `.scope()`.\n  Args:\n    model: The tf.keras.Model from _build_model().\n    hparams: A namedtuple of hyperparameters. This function expects\n      .train_epochs: a Python integer with the number of passes over the\n        training dataset;\n      .learning_rate: a Python float forwarded to the optimizer;\n      .momentum: a Python float forwarded to the optimizer;",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "peekOfCode": "def train_model(model, hparams, train_ds, validation_ds, steps_per_epoch):\n  \"\"\"Trains model with the given data and hyperparameters.\n  Args:\n    model: The tf.keras.Model from _build_model().\n    hparams: A namedtuple of hyperparameters. This function expects\n      .train_epochs: a Python integer with the number of passes over the\n        training dataset;\n      .learning_rate: a Python float forwarded to the optimizer; Base learning\n        rate when train batch size is 256. Linear to the batch size;\n      .batch_size: a Python integer, the number of examples returned by each",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DECAY_SAMPLES",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "peekOfCode": "DEFAULT_DECAY_SAMPLES = 10000 * 256\nDEFAULT_WARMUP_EPOCHS = 2\ndef add_params(hparams, **kwargs):\n  param_dict = {k: v for k, v in kwargs.items() if v is not None}\n  return hparams._replace(**param_dict)\nclass HParams(\n    collections.namedtuple(\n        \"HParams\", hub_lib.HParams._fields + (\"warmup_steps\", \"model_dir\"))):\n  \"\"\"The hyperparameters for make_image_classifier.\n  train_epochs: Training will do this many iterations over the dataset.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "documentation": {}
    },
    {
        "label": "DEFAULT_WARMUP_EPOCHS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "peekOfCode": "DEFAULT_WARMUP_EPOCHS = 2\ndef add_params(hparams, **kwargs):\n  param_dict = {k: v for k, v in kwargs.items() if v is not None}\n  return hparams._replace(**param_dict)\nclass HParams(\n    collections.namedtuple(\n        \"HParams\", hub_lib.HParams._fields + (\"warmup_steps\", \"model_dir\"))):\n  \"\"\"The hyperparameters for make_image_classifier.\n  train_epochs: Training will do this many iterations over the dataset.\n  do_fine_tuning: If true, the Hub module is trained together with the",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.task.train_image_classifier_lib",
        "documentation": {}
    },
    {
        "label": "OndeviceScannBuilder",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.ondevice_scann_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.ondevice_scann_builder",
        "peekOfCode": "class OndeviceScannBuilder(scann_builder.ScannBuilder):\n  \"\"\"ScannBuilder for on-device applications.\"\"\"\n  def create_config(self):\n    \"\"\"Creates the config.\"\"\"\n    config = super().create_config()\n    config_proto = scann_pb2.ScannConfig()\n    text_format.Parse(config, config_proto)\n    # We don't support residual quantization on device so we need to disable\n    # use_residual_quantization.\n    if config_proto.hash.asymmetric_hash.use_residual_quantization:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.ondevice_scann_builder",
        "documentation": {}
    },
    {
        "label": "builder",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.ondevice_scann_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.ondevice_scann_builder",
        "peekOfCode": "def builder(db, num_neighbors, distance_measure):\n  \"\"\"pybind analogue of builder() in scann_ops.py for the on-device use case.\"\"\"\n  def builder_lambda(db, config, training_threads, **kwargs):\n    return scann_ops_pybind.create_searcher(db, config, training_threads,\n                                           **kwargs)\n  return OndeviceScannBuilder(\n      db, num_neighbors, distance_measure).set_builder_lambda(builder_lambda)\nclass OndeviceScannBuilder(scann_builder.ScannBuilder):\n  \"\"\"ScannBuilder for on-device applications.\"\"\"\n  def create_config(self):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.ondevice_scann_builder",
        "documentation": {}
    },
    {
        "label": "OndeviceScannBuilderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.ondevice_scann_builder_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.ondevice_scann_builder_test",
        "peekOfCode": "class OndeviceScannBuilderTest(tf.test.TestCase):\n  def test_create_from_config_bruteforce(self):\n    dataset = np.random.random(size=(1000, 1024))\n    builder = ondevice_scann_builder.builder(\n        dataset, num_neighbors=10, distance_measure=\"dot_product\")\n    builder.score_brute_force()\n    config = builder.create_config()\n    config_proto = scann_pb2.ScannConfig()\n    text_format.Parse(config, config_proto)\n    expected_config_proto = \"\"\"num_neighbors: 10",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.ondevice_scann_builder_test",
        "documentation": {}
    },
    {
        "label": "OnDeviceArtifacts",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "peekOfCode": "class OnDeviceArtifacts(NamedTuple):\n  ondevice_config: serialized_searcher_pb2.ScannOnDeviceConfig\n  hashed_dataset: Optional[np.ndarray]\n  float_dataset: Optional[np.ndarray]\n  partition_assignments: Optional[np.ndarray]\ndef get_distance_measure(\n    distance_measure_str: str) -> serialized_searcher_pb2.DistanceMeasure:\n  \"\"\"Maps a distance measure string to a DistanceMeasure proto.\"\"\"\n  return _DISTANCE_MAP.get(distance_measure_str,\n                           serialized_searcher_pb2.UNSPECIFIED)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "documentation": {}
    },
    {
        "label": "get_distance_measure",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "peekOfCode": "def get_distance_measure(\n    distance_measure_str: str) -> serialized_searcher_pb2.DistanceMeasure:\n  \"\"\"Maps a distance measure string to a DistanceMeasure proto.\"\"\"\n  return _DISTANCE_MAP.get(distance_measure_str,\n                           serialized_searcher_pb2.UNSPECIFIED)\ndef get_indexer(\n    on_device_distance: serialized_searcher_pb2.DistanceMeasure,\n    lookup_type: hash_pb2.AsymmetricHasherConfig.LookupType,\n    ah_codebook: centers_pb2.CentersForAllSubspaces\n) -> serialized_searcher_pb2.IndexerProto:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "documentation": {}
    },
    {
        "label": "get_indexer",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "peekOfCode": "def get_indexer(\n    on_device_distance: serialized_searcher_pb2.DistanceMeasure,\n    lookup_type: hash_pb2.AsymmetricHasherConfig.LookupType,\n    ah_codebook: centers_pb2.CentersForAllSubspaces\n) -> serialized_searcher_pb2.IndexerProto:\n  \"\"\"Helper util that builds an indexer for `convert_artifacts_to_leveldb`.\n  Args:\n    on_device_distance: DistanceMeasure used in NN search.\n    lookup_type: Type of lookup table used for asymmetric distance NN search.\n    ah_codebook: ScaNN AsymmetricHashing hasher.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "documentation": {}
    },
    {
        "label": "get_partitioner",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "peekOfCode": "def get_partitioner(\n    on_device_distance: serialized_searcher_pb2.DistanceMeasure,\n    search_fraction: float,\n    partition_centroids: partitioner_pb2.SerializedPartitioner\n) -> serialized_searcher_pb2.PartitionerProto:\n  \"\"\"Helper util that builds a partitioner for `convert_artifacts_to_leveldb`.\n  Args:\n    on_device_distance: DistanceMeasure used in NN search.\n    search_fraction: Fraction of partitions searched.\n    partition_centroids: ScaNN partitioner.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "documentation": {}
    },
    {
        "label": "convert_serialized_to_on_device",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "peekOfCode": "def convert_serialized_to_on_device(serialized_path: str) -> OnDeviceArtifacts:\n  \"\"\"Converts ScaNN's serialized artifacts to on-device format.\n  Args:\n    serialized_path: Path to the dir that contains the ScaNN's artifacts.\n  Returns:\n    A named tuple containing the on-device ScannOnDeviceConfig as well as the\n    database (maybe compressed) and partition assignment (if partitioning is\n    enabled) in numpy format.\n  \"\"\"\n  config_path = os.path.join(serialized_path, 'scann_config.pb')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "documentation": {}
    },
    {
        "label": "convert_artifacts_to_leveldb",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "peekOfCode": "def convert_artifacts_to_leveldb(output_file_path: str,\n                                 metadata: List[AnyStr],\n                                 userinfo: AnyStr,\n                                 artifacts: OnDeviceArtifacts,\n                                 compression: bool = True) -> None:\n  \"\"\"Converts artifacts to the index file.\n  Raises exception if the input is invalid or failed to create the file.\n  Args:\n    output_file_path: Path to the levelDB index file.\n    metadata: The metadata for each of the embeddings in the database. Passed in",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "documentation": {}
    },
    {
        "label": "_DISTANCE_MAP",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "peekOfCode": "_DISTANCE_MAP = {\n    'SquaredL2Distance': serialized_searcher_pb2.SQUARED_L2_DISTANCE,\n    'DotProductDistance': serialized_searcher_pb2.DOT_PRODUCT,\n}\n_LOOKUP_TYPE_MAP = {\n    hash_pb2.AsymmetricHasherConfig.FLOAT:\n        serialized_searcher_pb2.AsymmetricHashingProto.FLOAT,\n    hash_pb2.AsymmetricHasherConfig.INT8:\n        serialized_searcher_pb2.AsymmetricHashingProto.INT8,\n    hash_pb2.AsymmetricHasherConfig.INT16:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "documentation": {}
    },
    {
        "label": "_LOOKUP_TYPE_MAP",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "peekOfCode": "_LOOKUP_TYPE_MAP = {\n    hash_pb2.AsymmetricHasherConfig.FLOAT:\n        serialized_searcher_pb2.AsymmetricHashingProto.FLOAT,\n    hash_pb2.AsymmetricHasherConfig.INT8:\n        serialized_searcher_pb2.AsymmetricHashingProto.INT8,\n    hash_pb2.AsymmetricHasherConfig.INT16:\n        serialized_searcher_pb2.AsymmetricHashingProto.INT16,\n    hash_pb2.AsymmetricHasherConfig.INT8_LUT16:\n        serialized_searcher_pb2.AsymmetricHashingProto.INT8_LUT16,\n}",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter",
        "documentation": {}
    },
    {
        "label": "ScannConverterTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter_test",
        "peekOfCode": "class ScannConverterTest(tf.test.TestCase, parameterized.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.artifact_dir = os.path.join(FLAGS.test_tmpdir, 'serialized_searcher')\n    os.mkdir(self.artifact_dir)\n  def tearDown(self):\n    super().tearDown()\n    shutil.rmtree(self.artifact_dir)\n  @parameterized.named_parameters(\n      dict(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter_test",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter_test",
        "peekOfCode": "FLAGS = flags.FLAGS\nDIMENSIONS = 20\nNUM_NEIGHBORS = 10\nclass ScannConverterTest(tf.test.TestCase, parameterized.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.artifact_dir = os.path.join(FLAGS.test_tmpdir, 'serialized_searcher')\n    os.mkdir(self.artifact_dir)\n  def tearDown(self):\n    super().tearDown()",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter_test",
        "documentation": {}
    },
    {
        "label": "DIMENSIONS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter_test",
        "peekOfCode": "DIMENSIONS = 20\nNUM_NEIGHBORS = 10\nclass ScannConverterTest(tf.test.TestCase, parameterized.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.artifact_dir = os.path.join(FLAGS.test_tmpdir, 'serialized_searcher')\n    os.mkdir(self.artifact_dir)\n  def tearDown(self):\n    super().tearDown()\n    shutil.rmtree(self.artifact_dir)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter_test",
        "documentation": {}
    },
    {
        "label": "NUM_NEIGHBORS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter_test",
        "description": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter_test",
        "peekOfCode": "NUM_NEIGHBORS = 10\nclass ScannConverterTest(tf.test.TestCase, parameterized.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.artifact_dir = os.path.join(FLAGS.test_tmpdir, 'serialized_searcher')\n    os.mkdir(self.artifact_dir)\n  def tearDown(self):\n    super().tearDown()\n    shutil.rmtree(self.artifact_dir)\n  @parameterized.named_parameters(",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.utils.scann_converter_test",
        "documentation": {}
    },
    {
        "label": "setup_tf_behavior",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "description": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "peekOfCode": "def setup_tf_behavior(tf_version=_DEFAULT_TF_BEHAVIOR):\n  \"\"\"Setup tf behavior. It must be used before the main().\"\"\"\n  global _tf_behavior_version\n  if tf_version not in [1, 2]:\n    raise ValueError(\n        'tf_version should be in [1, 2], but got {}'.format(tf_version))\n  if tf_version == 1:\n    tf.compat.v1.logging.warn(\n        'Using v1 behavior. Please note that it is mainly to run legacy models,'\n        'however v2 is more preferrable if they are supported.')",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "documentation": {}
    },
    {
        "label": "get_tf_behavior",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "description": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "peekOfCode": "def get_tf_behavior():\n  \"\"\"Gets version for tf behavior.\n  Returns:\n    int, 1 or 2 indicating the behavior version.\n  \"\"\"\n  return _tf_behavior_version\ndef get_compat_tf_versions(compat_tf_versions=None):\n  \"\"\"Gets compatible tf versions (default: [2]).\n  Args:\n    compat_tf_versions: int, int list or None, indicates compatible versions.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "documentation": {}
    },
    {
        "label": "get_compat_tf_versions",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "description": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "peekOfCode": "def get_compat_tf_versions(compat_tf_versions=None):\n  \"\"\"Gets compatible tf versions (default: [2]).\n  Args:\n    compat_tf_versions: int, int list or None, indicates compatible versions.\n  Returns:\n    A list of compatible tf versions.\n  \"\"\"\n  if compat_tf_versions is None:\n    compat_tf_versions = [2]\n  if not isinstance(compat_tf_versions, list):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "documentation": {}
    },
    {
        "label": "_DEFAULT_TF_BEHAVIOR",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "description": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "peekOfCode": "_DEFAULT_TF_BEHAVIOR = 2\n# Get version of tf behavior in use (valid 1 or 2).\n_tf_behavior_version = _DEFAULT_TF_BEHAVIOR\ndef setup_tf_behavior(tf_version=_DEFAULT_TF_BEHAVIOR):\n  \"\"\"Setup tf behavior. It must be used before the main().\"\"\"\n  global _tf_behavior_version\n  if tf_version not in [1, 2]:\n    raise ValueError(\n        'tf_version should be in [1, 2], but got {}'.format(tf_version))\n  if tf_version == 1:",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "documentation": {}
    },
    {
        "label": "_tf_behavior_version",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "description": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "peekOfCode": "_tf_behavior_version = _DEFAULT_TF_BEHAVIOR\ndef setup_tf_behavior(tf_version=_DEFAULT_TF_BEHAVIOR):\n  \"\"\"Setup tf behavior. It must be used before the main().\"\"\"\n  global _tf_behavior_version\n  if tf_version not in [1, 2]:\n    raise ValueError(\n        'tf_version should be in [1, 2], but got {}'.format(tf_version))\n  if tf_version == 1:\n    tf.compat.v1.logging.warn(\n        'Using v1 behavior. Please note that it is mainly to run legacy models,'",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.compat",
        "documentation": {}
    },
    {
        "label": "ExportFormat",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.export_format",
        "description": "examples.tensorflow_examples.lite.model_maker.core.export_format",
        "peekOfCode": "class ExportFormat(enum.Enum):\n  TFLITE = \"TFLITE\"\n  SAVED_MODEL = \"SAVED_MODEL\"\n  LABEL = \"LABEL\"\n  VOCAB = \"VOCAB\"\n  TFJS = \"TFJS\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.export_format",
        "documentation": {}
    },
    {
        "label": "load_json_file",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.file_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.file_util",
        "peekOfCode": "def load_json_file(json_file):\n  \"\"\"Loads json data from file.\"\"\"\n  with tf.io.gfile.GFile(json_file, 'r') as reader:\n    return json.load(reader)\ndef write_json_file(json_file, data):\n  \"\"\"Writes json data into file.\"\"\"\n  with tf.io.gfile.GFile(json_file, 'w') as f:\n    json.dump(data, f)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.file_util",
        "documentation": {}
    },
    {
        "label": "write_json_file",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.file_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.file_util",
        "peekOfCode": "def write_json_file(json_file, data):\n  \"\"\"Writes json data into file.\"\"\"\n  with tf.io.gfile.GFile(json_file, 'w') as f:\n    json.dump(data, f)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.file_util",
        "documentation": {}
    },
    {
        "label": "test_srcdir",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "peekOfCode": "def test_srcdir():\n  \"\"\"Returns the path where to look for test data files.\"\"\"\n  if \"test_srcdir\" in flags.FLAGS:\n    return flags.FLAGS[\"test_srcdir\"].value\n  elif \"TEST_SRCDIR\" in os.environ:\n    return os.environ[\"TEST_SRCDIR\"]\n  else:\n    raise RuntimeError(\"Missing TEST_SRCDIR environment.\")\ndef get_test_data_path(file_or_dirname):\n  \"\"\"Return full test data path.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "documentation": {}
    },
    {
        "label": "get_test_data_path",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "peekOfCode": "def get_test_data_path(file_or_dirname):\n  \"\"\"Return full test data path.\"\"\"\n  for (directory, subdirs, files) in tf.io.gfile.walk(test_srcdir()):\n    for f in subdirs + files:\n      if f.endswith(file_or_dirname):\n        return os.path.join(directory, f)\n  raise ValueError(\"No %s in test directory\" % file_or_dirname)\ndef get_cache_dir(temp_dir, filename):\n  \"\"\"Gets `cache_dir` for `tf.keras.utils.get_file` function.\"\"\"\n  # Copies SST-2.zip in testdata folder to a temp folder since",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "documentation": {}
    },
    {
        "label": "get_cache_dir",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "peekOfCode": "def get_cache_dir(temp_dir, filename):\n  \"\"\"Gets `cache_dir` for `tf.keras.utils.get_file` function.\"\"\"\n  # Copies SST-2.zip in testdata folder to a temp folder since\n  # `tf.keras.utils.get_file` needed writability of the path.\n  try:\n    src_path = get_test_data_path(filename)\n    dest_path = os.path.join(temp_dir, \"datasets\")\n    if not tf.io.gfile.exists(dest_path):\n      tf.io.gfile.mkdir(dest_path)\n    shutil.copy2(src_path, dest_path)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "documentation": {}
    },
    {
        "label": "test_in_tf_1",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "peekOfCode": "def test_in_tf_1(fn):\n  \"\"\"Decorator to test in tf 1 behaviors.\"\"\"\n  @functools.wraps(fn)\n  def decorator(*args, **kwargs):\n    if compat.get_tf_behavior() != 1:\n      tf.compat.v1.logging.info(\"Skip function {} for test_in_tf_1\".format(\n          fn.__name__))\n      return\n    fn(*args, **kwargs)\n  return decorator",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "documentation": {}
    },
    {
        "label": "test_in_tf_2",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "peekOfCode": "def test_in_tf_2(fn):\n  \"\"\"Decorator to test in tf 2 behaviors.\"\"\"\n  @functools.wraps(fn)\n  def decorator(*args, **kwargs):\n    if compat.get_tf_behavior() != 2:\n      tf.compat.v1.logging.info(\"Skip function {} for test_in_tf_2\".format(\n          fn.__name__))\n      return\n    fn(*args, **kwargs)\n  return decorator",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "documentation": {}
    },
    {
        "label": "test_in_tf_1and2",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "peekOfCode": "def test_in_tf_1and2(fn):\n  \"\"\"Decorator to test in tf 1 and 2 behaviors.\"\"\"\n  @functools.wraps(fn)\n  def decorator(*args, **kwargs):\n    if compat.get_tf_behavior() not in [1, 2]:\n      tf.compat.v1.logging.info(\"Skip function {} for test_in_tf_1and2\".format(\n          fn.__name__))\n      return\n    fn(*args, **kwargs)\n  return decorator",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "documentation": {}
    },
    {
        "label": "build_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "peekOfCode": "def build_model(input_shape, num_classes):\n  \"\"\"Builds a simple model for test.\"\"\"\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  if len(input_shape) == 3:  # Image inputs.\n    outputs = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(outputs)\n  elif len(input_shape) == 1:  # Text inputs.\n    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(inputs)\n  else:\n    raise ValueError(\"Model inputs should be 2D tensor or 4D tensor.\")",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "peekOfCode": "def get_dataloader(data_size, input_shape, num_classes, max_input_value=1000):\n  \"\"\"Gets a simple `DataLoader` object for test.\"\"\"\n  features = tf.random.uniform(\n      shape=[data_size] + input_shape,\n      minval=0,\n      maxval=max_input_value,\n      dtype=tf.float32)\n  labels = tf.random.uniform(\n      shape=[data_size], minval=0, maxval=num_classes, dtype=tf.int32)\n  ds = tf.data.Dataset.from_tensor_slices((features, labels))",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "documentation": {}
    },
    {
        "label": "create_pascal_voc",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "peekOfCode": "def create_pascal_voc(temp_dir=None):\n  \"\"\"Creates test data with PASCAL VOC format.\"\"\"\n  if temp_dir is None or not tf.io.gfile.exists(temp_dir):\n    temp_dir = tempfile.mkdtemp()\n  # Saves the image into images_dir.\n  image_file_name = \"2012_12.jpg\"\n  image_data = np.random.rand(256, 256, 3)\n  images_dir = os.path.join(temp_dir, \"images\")\n  os.mkdir(images_dir)\n  save_path = os.path.join(images_dir, image_file_name)",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "documentation": {}
    },
    {
        "label": "is_same_output",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "peekOfCode": "def is_same_output(tflite_file,\n                   keras_model,\n                   input_tensors,\n                   model_spec=None,\n                   atol=1e-04):\n  \"\"\"Whether the output of TFLite model is the same as keras model.\"\"\"\n  # Gets output from lite model.\n  lite_runner = model_util.get_lite_runner(tflite_file, model_spec)\n  lite_output = lite_runner.run(input_tensors)\n  # Gets output from keras model.",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "description": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef test_srcdir():\n  \"\"\"Returns the path where to look for test data files.\"\"\"\n  if \"test_srcdir\" in flags.FLAGS:\n    return flags.FLAGS[\"test_srcdir\"].value\n  elif \"TEST_SRCDIR\" in os.environ:\n    return os.environ[\"TEST_SRCDIR\"]\n  else:\n    raise RuntimeError(\"Missing TEST_SRCDIR environment.\")\ndef get_test_data_path(file_or_dirname):",
        "detail": "examples.tensorflow_examples.lite.model_maker.core.test_util",
        "documentation": {}
    },
    {
        "label": "download_bird_dataset",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "peekOfCode": "def download_bird_dataset(**kwargs):\n  \"\"\"Downloads the bird dataset, and returns path to the directory.\"\"\"\n  return _download_dataset(\n      'birds_dataset.zip',\n      'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/birds_dataset.zip',\n      'small_birds_dataset', **kwargs)\ndef download_speech_commands_dataset(**kwargs):\n  \"\"\"Downloads demo dataset, and returns directory path.\"\"\"\n  return _download_dataset(\n      'mini_speech_commands.zip',",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "documentation": {}
    },
    {
        "label": "download_speech_commands_dataset",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "peekOfCode": "def download_speech_commands_dataset(**kwargs):\n  \"\"\"Downloads demo dataset, and returns directory path.\"\"\"\n  return _download_dataset(\n      'mini_speech_commands.zip',\n      'https://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip',\n      'mini_speech_commands', **kwargs)\ndef download_esc50_dataset(**kwargs):\n  \"\"\"Downloads ESC50 dataset, and returns directory path.\"\"\"\n  return _download_dataset(\n      'esc-50.zip', 'https://github.com/karoldvl/ESC-50/archive/master.zip',",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "documentation": {}
    },
    {
        "label": "download_esc50_dataset",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "peekOfCode": "def download_esc50_dataset(**kwargs):\n  \"\"\"Downloads ESC50 dataset, and returns directory path.\"\"\"\n  return _download_dataset(\n      'esc-50.zip', 'https://github.com/karoldvl/ESC-50/archive/master.zip',\n      'ESC-50-master', **kwargs)\ndef run(spec,\n        data_dir,\n        dataset_type,\n        export_dir,\n        epochs=5,",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "peekOfCode": "def run(spec,\n        data_dir,\n        dataset_type,\n        export_dir,\n        epochs=5,\n        batch_size=32,\n        **kwargs):\n  \"\"\"Runs demo.\"\"\"\n  spec = model_spec.get(spec)\n  if dataset_type == 'esc50':",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "peekOfCode": "def main(_):\n  logging.set_verbosity(logging.INFO)\n  export_dir = os.path.expanduser(FLAGS.export_dir)\n  if not FLAGS.data_dir:\n    if FLAGS.dataset == 'esc50':\n      data_dir = download_esc50_dataset()\n    elif FLAGS.dataset == 'bird':\n      data_dir = download_bird_dataset()\n    elif FLAGS.dataset == 'mini_speech_command':\n      data_dir = download_speech_commands_dataset()",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef _define_flags():\n  \"\"\"Define CLI flags and their default values.\"\"\"\n  flags.DEFINE_string('export_dir', None,\n                      'The directory to save exported files.')\n  flags.DEFINE_string('data_dir', None, 'The directory to load dataset from.')\n  flags.DEFINE_string('spec', 'audio_browser_fft',\n                      'Name of the model spec to use.')\n  flags.DEFINE_string(\n      'dataset', 'mini_speech_command',",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo",
        "documentation": {}
    },
    {
        "label": "AudioClassificationDemoTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo_test",
        "peekOfCode": "class AudioClassificationDemoTest(tf.test.TestCase, parameterized.TestCase):\n  @parameterized.parameters(\n      ('audio_browser_fft', 'mini_speech_command'),\n      ('audio_yamnet', 'mini_speech_command'),\n  )\n  def test_audio_classification_demo(self, spec, dataset):\n    with patch_data_loader():\n      with tempfile.TemporaryDirectory() as temp_dir:\n        # Use cached training data if exists.\n        data_dir = audio_classification_demo.download_speech_commands_dataset(",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo_test",
        "documentation": {}
    },
    {
        "label": "patch_data_loader",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo_test",
        "peekOfCode": "def patch_data_loader():\n  \"\"\"Patch to train partial dataset rather than all of them.\"\"\"\n  def side_effect(*args, **kwargs):\n    tf.compat.v1.logging.info('Train on partial dataset')\n    # This takes around 8 mins as it caches all files in the folder.\n    # We should be able to address this issue once the dataset is lazily loaded.\n    data_loader = from_folder_fn(*args, **kwargs)\n    if len(data_loader) > 10:  # Trim dataset to at most 10.\n      data_loader._size = 10\n      # TODO(b/171449557): Change this once the dataset is lazily loaded.",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo_test",
        "documentation": {}
    },
    {
        "label": "from_folder_fn",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo_test",
        "peekOfCode": "from_folder_fn = audio_classifier.DataLoader.from_folder\ndef patch_data_loader():\n  \"\"\"Patch to train partial dataset rather than all of them.\"\"\"\n  def side_effect(*args, **kwargs):\n    tf.compat.v1.logging.info('Train on partial dataset')\n    # This takes around 8 mins as it caches all files in the folder.\n    # We should be able to address this issue once the dataset is lazily loaded.\n    data_loader = from_folder_fn(*args, **kwargs)\n    if len(data_loader) > 10:  # Trim dataset to at most 10.\n      data_loader._size = 10",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.audio_classification_demo_test",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "peekOfCode": "class DataLoader(dataloader.DataLoader):\n  \"\"\"DataLoader for XOR problem.\"\"\"\n  @classmethod\n  def create(cls, spec, is_training=True, shuffle=False):\n    inputs = tf.data.Dataset.from_tensor_slices([(0., 0), (0, 1), (1, 0),\n                                                 (1, 1)])\n    outputs = tf.data.Dataset.from_tensor_slices([0, 1, 1, 0])\n    ds = tf.data.Dataset.zip((inputs, outputs))\n    ds = ds.repeat(10)\n    if shuffle:",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "documentation": {}
    },
    {
        "label": "BinaryClassificationBaseSpec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "peekOfCode": "class BinaryClassificationBaseSpec(abc.ABC):\n  \"\"\"Model spec for binary classification.\"\"\"\n  compat_tf_versions = (2,)\n  def __init__(self, model_dir=None, strategy=None):\n    self.model_dir = model_dir\n    if not model_dir:\n      self.model_dir = tempfile.mkdtemp()\n    tf.compat.v1.logging.info('Checkpoints are stored in %s', self.model_dir)\n    self.strategy = strategy or tf.distribute.get_strategy()\n  @abc.abstractmethod",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "documentation": {}
    },
    {
        "label": "Spec",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "peekOfCode": "class Spec(BinaryClassificationBaseSpec):\n  \"\"\"Spec for XOR problem, contains a model with a single hidden layer.\"\"\"\n  def preprocess_ds(self, ds, is_training=False):\n    @tf.function\n    def _add_noise(x, y):\n      \"\"\"Add some random noise on the input sample.\"\"\"\n      return tf.random.normal(tf.shape(x), stddev=0.1) + x, y\n    if is_training:\n      ds = ds.map(_add_noise)\n    return ds",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "documentation": {}
    },
    {
        "label": "BinaryClassifier",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "peekOfCode": "class BinaryClassifier(custom_model.CustomModel):\n  \"\"\"BinaryClassifier for train/inference and model export.\"\"\"\n  def __init__(self, spec, shuffle=True):\n    assert isinstance(spec, BinaryClassificationBaseSpec)\n    super(BinaryClassifier, self).__init__(spec, shuffle)\n  def train(self, train_data, validation_data, epochs=10, batch_size=4):\n    if len(train_data) < batch_size:\n      raise ValueError('The size of the train_data (%d) couldn\\'t be smaller '\n                       'than batch_size (%d). To solve this problem, set '\n                       'the batch_size smaller or increase the size of the '",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "peekOfCode": "def define_flags():\n  flags.DEFINE_string(\n      'export_dir', None,\n      'The directory to save exported TfLite/saved_model files.')\n  flags.mark_flag_as_required('export_dir')\nclass DataLoader(dataloader.DataLoader):\n  \"\"\"DataLoader for XOR problem.\"\"\"\n  @classmethod\n  def create(cls, spec, is_training=True, shuffle=False):\n    inputs = tf.data.Dataset.from_tensor_slices([(0., 0), (0, 1), (1, 0),",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "documentation": {}
    },
    {
        "label": "train_xor_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "peekOfCode": "def train_xor_model(export_dir):\n  \"\"\"Use deep learning to solve XOR problem and return the test accuracy.\"\"\"\n  spec = Spec(model_dir=export_dir)\n  data = DataLoader.create(spec, is_training=True)\n  train_data, validation_data = data.split(0.8)\n  classifier = BinaryClassifier(spec)\n  classifier.train(train_data, validation_data)\n  test_data = DataLoader.create(spec, is_training=False)\n  eval_result = classifier.evaluate(test_data)\n  eval_acc = eval_result['accuracy']",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "peekOfCode": "def main(_):\n  train_xor_model(FLAGS.export_dir)\nif __name__ == '__main__':\n  define_flags()\n  app.run(main)",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef define_flags():\n  flags.DEFINE_string(\n      'export_dir', None,\n      'The directory to save exported TfLite/saved_model files.')\n  flags.mark_flag_as_required('export_dir')\nclass DataLoader(dataloader.DataLoader):\n  \"\"\"DataLoader for XOR problem.\"\"\"\n  @classmethod\n  def create(cls, spec, is_training=True, shuffle=False):",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo",
        "documentation": {}
    },
    {
        "label": "DemoTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo_test",
        "peekOfCode": "class DemoTest(tf.test.TestCase):\n  def test_demo(self):\n    with tempfile.TemporaryDirectory() as temp_dir:\n      tflite_filename = os.path.join(temp_dir, 'model.tflite')\n      saved_model_filename = os.path.join(temp_dir,\n                                          'saved_model/saved_model.pb')\n      seed = 100\n      tf.random.set_seed(seed)\n      acc = custom_model_demo.train_xor_model(temp_dir)\n      self.assertEqual(acc, 1.)",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.custom_model_demo_test",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "peekOfCode": "def define_flags():\n  flags.DEFINE_string('export_dir', None,\n                      'The directory to save exported files.')\n  flags.DEFINE_string('spec', 'efficientnet_lite0',\n                      'The image classifier to run.')\n  flags.mark_flag_as_required('export_dir')\ndef download_demo_data(**kwargs):\n  \"\"\"Downloads demo data, and returns directory path.\"\"\"\n  data_dir = tf.keras.utils.get_file(\n      fname='flower_photos.tgz',",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "documentation": {}
    },
    {
        "label": "download_demo_data",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "peekOfCode": "def download_demo_data(**kwargs):\n  \"\"\"Downloads demo data, and returns directory path.\"\"\"\n  data_dir = tf.keras.utils.get_file(\n      fname='flower_photos.tgz',\n      origin='https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n      extract=True,\n      **kwargs)\n  return os.path.join(os.path.dirname(data_dir), 'flower_photos')  # folder name\ndef run(data_dir, export_dir, spec='efficientnet_lite0', **kwargs):\n  \"\"\"Runs demo.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "peekOfCode": "def run(data_dir, export_dir, spec='efficientnet_lite0', **kwargs):\n  \"\"\"Runs demo.\"\"\"\n  spec = model_spec.get(spec)\n  data = image_classifier.DataLoader.from_folder(data_dir)\n  train_data, rest_data = data.split(0.8)\n  validation_data, test_data = rest_data.split(0.5)\n  model = image_classifier.create(\n      train_data, model_spec=spec, validation_data=validation_data, **kwargs)\n  _, acc = model.evaluate(test_data)\n  print('Test accuracy: %f' % acc)",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "peekOfCode": "def main(_):\n  logging.set_verbosity(logging.INFO)\n  data_dir = download_demo_data()\n  export_dir = os.path.expanduser(FLAGS.export_dir)\n  run(data_dir, export_dir, spec=FLAGS.spec)\nif __name__ == '__main__':\n  define_flags()\n  app.run(main)",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef define_flags():\n  flags.DEFINE_string('export_dir', None,\n                      'The directory to save exported files.')\n  flags.DEFINE_string('spec', 'efficientnet_lite0',\n                      'The image classifier to run.')\n  flags.mark_flag_as_required('export_dir')\ndef download_demo_data(**kwargs):\n  \"\"\"Downloads demo data, and returns directory path.\"\"\"\n  data_dir = tf.keras.utils.get_file(",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo",
        "documentation": {}
    },
    {
        "label": "ImageClassificationDemoTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo_test",
        "peekOfCode": "class ImageClassificationDemoTest(tf.test.TestCase):\n  def test_image_classification_demo(self):\n    with patch_data_loader():\n      with tempfile.TemporaryDirectory() as temp_dir:\n        # Use cached training data if exists.\n        data_dir = image_classification_demo.download_demo_data(\n            cache_dir=test_util.get_cache_dir(temp_dir, 'flower_photos.tgz'),\n            file_hash='6f87fb78e9cc9ab41eff2015b380011d')\n        tflite_filename = os.path.join(temp_dir, 'model.tflite')\n        label_filename = os.path.join(temp_dir, 'labels.txt')",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo_test",
        "documentation": {}
    },
    {
        "label": "patch_data_loader",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo_test",
        "peekOfCode": "def patch_data_loader():\n  \"\"\"Patch to train partial dataset rather than all of them.\"\"\"\n  def side_effect(*args, **kwargs):\n    tf.compat.v1.logging.info('Train on partial dataset')\n    data_loader = from_folder_fn(*args, **kwargs)\n    if len(data_loader) > 10:  # Trim dataset to at most 10.\n      data_loader._size = 10\n      # TODO(b/171449557): Change this once the dataset is lazily loaded.\n      data_loader._dataset = data_loader._dataset.take(10)\n    return data_loader",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo_test",
        "documentation": {}
    },
    {
        "label": "from_folder_fn",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo_test",
        "peekOfCode": "from_folder_fn = image_classifier.DataLoader.from_folder\ndef patch_data_loader():\n  \"\"\"Patch to train partial dataset rather than all of them.\"\"\"\n  def side_effect(*args, **kwargs):\n    tf.compat.v1.logging.info('Train on partial dataset')\n    data_loader = from_folder_fn(*args, **kwargs)\n    if len(data_loader) > 10:  # Trim dataset to at most 10.\n      data_loader._size = 10\n      # TODO(b/171449557): Change this once the dataset is lazily loaded.\n      data_loader._dataset = data_loader._dataset.take(10)",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.image_classification_demo_test",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "peekOfCode": "def define_flags():\n  flags.DEFINE_string('export_dir', None,\n                      'The directory to save exported files.')\n  flags.mark_flag_as_required('export_dir')\n  flags.DEFINE_string('spec', 'bert_qa', 'The QA model to run.')\ndef download_demo_data(**kwargs):\n  \"\"\"Downloads demo data, and returns directory path.\"\"\"\n  train_data_path = tf.keras.utils.get_file(\n      fname='train-v1.1.json',\n      origin='https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json',",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "documentation": {}
    },
    {
        "label": "download_demo_data",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "peekOfCode": "def download_demo_data(**kwargs):\n  \"\"\"Downloads demo data, and returns directory path.\"\"\"\n  train_data_path = tf.keras.utils.get_file(\n      fname='train-v1.1.json',\n      origin='https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json',\n      **kwargs)\n  validation_data_path = tf.keras.utils.get_file(\n      fname='dev-v1.1.json',\n      origin='https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json',\n      **kwargs)",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "peekOfCode": "def run(train_data_path,\n        validation_data_path,\n        export_dir,\n        spec='bert_qa',\n        **kwargs):\n  \"\"\"Runs demo.\"\"\"\n  # Chooses model specification that represents model.\n  spec = model_spec.get(spec)\n  # Gets training data and validation data.\n  train_data = question_answer.DataLoader.from_squad(",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "peekOfCode": "def main(_):\n  logging.set_verbosity(logging.INFO)\n  train_data_path, validation_data_path = download_demo_data()\n  export_dir = os.path.expanduser(FLAGS.export_dir)\n  run(train_data_path, validation_data_path, export_dir, spec=FLAGS.spec)\nif __name__ == '__main__':\n  define_flags()\n  app.run(main)",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef define_flags():\n  flags.DEFINE_string('export_dir', None,\n                      'The directory to save exported files.')\n  flags.mark_flag_as_required('export_dir')\n  flags.DEFINE_string('spec', 'bert_qa', 'The QA model to run.')\ndef download_demo_data(**kwargs):\n  \"\"\"Downloads demo data, and returns directory path.\"\"\"\n  train_data_path = tf.keras.utils.get_file(\n      fname='train-v1.1.json',",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.question_answer_demo",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "peekOfCode": "def define_flags():\n  flags.DEFINE_string('data_dir', None, 'The directory to save dataset.')\n  flags.DEFINE_string('export_dir', None,\n                      'The directory to save exported files.')\n  flags.DEFINE_string('encoder_type', 'bow',\n                      'The recommendation encoder to run. (bow, cnn, lstm)')\n  flags.mark_flag_as_required('export_dir')\ndef download_data(download_dir):\n  \"\"\"Downloads demo data, and returns directory path.\"\"\"\n  return recommendation.DataLoader.download_and_extract_movielens(download_dir)",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "documentation": {}
    },
    {
        "label": "download_data",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "peekOfCode": "def download_data(download_dir):\n  \"\"\"Downloads demo data, and returns directory path.\"\"\"\n  return recommendation.DataLoader.download_and_extract_movielens(download_dir)\ndef get_input_spec(encoder_type: str,\n                   num_classes: int) -> recommendation.spec.InputSpec:\n  \"\"\"Gets input spec (for test).\n  Input spec defines how the input features are extracted.\n  Args:\n    encoder_type: str, case-insensitive {'CNN', 'LSTM', 'BOW'}.\n    num_classes: int, num of classes in vocabulary.",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "documentation": {}
    },
    {
        "label": "get_input_spec",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "peekOfCode": "def get_input_spec(encoder_type: str,\n                   num_classes: int) -> recommendation.spec.InputSpec:\n  \"\"\"Gets input spec (for test).\n  Input spec defines how the input features are extracted.\n  Args:\n    encoder_type: str, case-insensitive {'CNN', 'LSTM', 'BOW'}.\n    num_classes: int, num of classes in vocabulary.\n  Returns:\n    InputSpec.\n  \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "documentation": {}
    },
    {
        "label": "get_model_hparams",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "peekOfCode": "def get_model_hparams() -> recommendation.spec.ModelHParams:\n  \"\"\"Gets model hparams (for test).\n  ModelHParams defines the model architecture.\n  Returns:\n    ModelHParams.\n  \"\"\"\n  return recommendation.spec.ModelHParams(\n      hidden_layer_dims=[32, 32],  # Hidden layers dimension.\n      eval_top_k=[1, 5],  # Eval top 1 and top 5.\n      conv_num_filter_ratios=[2, 4],  # For CNN encoder, conv filter mutipler.",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "peekOfCode": "def run(data_dir, export_dir, batch_size=16, epochs=5, encoder_type='bow'):\n  \"\"\"Runs demo.\"\"\"\n  meta = recommendation.DataLoader.generate_movielens_dataset(data_dir)\n  num_classes = recommendation.DataLoader.get_num_classes(meta)\n  input_spec = get_input_spec(encoder_type, num_classes)\n  train_data = recommendation.DataLoader.from_movielens(data_dir, 'train',\n                                                        input_spec)\n  test_data = recommendation.DataLoader.from_movielens(data_dir, 'test',\n                                                       input_spec)\n  model_spec = ms.get(",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "peekOfCode": "def main(_):\n  logging.set_verbosity(logging.INFO)\n  export_dir = os.path.expanduser(FLAGS.export_dir)\n  data_dir = os.path.expanduser(FLAGS.data_dir)\n  extracted_dir = download_data(data_dir)\n  run(extracted_dir, export_dir, encoder_type=FLAGS.encoder_type)\nif __name__ == '__main__':\n  define_flags()\n  app.run(main)",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef define_flags():\n  flags.DEFINE_string('data_dir', None, 'The directory to save dataset.')\n  flags.DEFINE_string('export_dir', None,\n                      'The directory to save exported files.')\n  flags.DEFINE_string('encoder_type', 'bow',\n                      'The recommendation encoder to run. (bow, cnn, lstm)')\n  flags.mark_flag_as_required('export_dir')\ndef download_data(download_dir):\n  \"\"\"Downloads demo data, and returns directory path.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo",
        "documentation": {}
    },
    {
        "label": "RecommendationDemoTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo_test",
        "peekOfCode": "class RecommendationDemoTest(tf.test.TestCase):\n  def setUp(self):\n    super().setUp()\n    setup_testdata(self)\n  def test_recommendation_demo(self):\n    with _rt.patch_download_and_extract_data(self.dataset_dir):\n      data_dir = recommendation_demo.download_data(self.download_dir)\n    self.assertEqual(data_dir, self.dataset_dir)\n    export_dir = os.path.join(self.test_tempdir, 'export')\n    tflite_filename = os.path.join(export_dir, 'model.tflite')",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo_test",
        "documentation": {}
    },
    {
        "label": "setup_testdata",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo_test",
        "peekOfCode": "def setup_testdata(instance):\n  \"\"\"Setup testdata under download_dir, and unzip data to dataset_dir.\"\"\"\n  if not hasattr(instance, 'test_tempdir'):\n    instance.test_tempdir = tempfile.mkdtemp()\n  instance.download_dir = os.path.join(instance.test_tempdir, 'download')\n  # Copy zip file and unzip.\n  os.makedirs(instance.download_dir, exist_ok=True)\n  # Use existing copy of data, if exists; otherwise, download it.\n  try:\n    path = test_util.get_test_data_path('recommendation_movielens')",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo_test",
        "documentation": {}
    },
    {
        "label": "patch_data_loader",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo_test",
        "peekOfCode": "def patch_data_loader():\n  \"\"\"Patch to train/eval partial dataset rather than all of them.\"\"\"\n  def mocked_init(self, dataset, size, vocab):\n    \"\"\"Mocked init function with a smaller dataset.\"\"\"\n    size = 16  # small size for dataset.\n    self._dataset = dataset.take(size)\n    self._size = size\n    self.vocab = vocab\n    self.max_vocab_id = max(self.vocab.keys())\n  return mock.patch.object(recommendation.DataLoader, '__init__', mocked_init)",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.recommendation_demo_test",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "peekOfCode": "def define_flags():\n  flags.DEFINE_string('export_dir', None,\n                      'The directory to save exported files.')\n  flags.DEFINE_string('spec', 'mobilebert_classifier',\n                      'The text classifier to run.')\n  flags.mark_flag_as_required('export_dir')\ndef download_demo_data(**kwargs):\n  \"\"\"Downloads demo data, and returns directory path.\"\"\"\n  data_path = tf.keras.utils.get_file(\n      fname='SST-2.zip',",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "documentation": {}
    },
    {
        "label": "download_demo_data",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "peekOfCode": "def download_demo_data(**kwargs):\n  \"\"\"Downloads demo data, and returns directory path.\"\"\"\n  data_path = tf.keras.utils.get_file(\n      fname='SST-2.zip',\n      origin='https://dl.fbaipublicfiles.com/glue/data/SST-2.zip',\n      extract=True,\n      **kwargs)\n  return os.path.join(os.path.dirname(data_path), 'SST-2')  # folder name\ndef run(data_dir, export_dir, spec='mobilebert_classifier', **kwargs):\n  \"\"\"Runs demo.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "peekOfCode": "def run(data_dir, export_dir, spec='mobilebert_classifier', **kwargs):\n  \"\"\"Runs demo.\"\"\"\n  # Chooses model specification that represents model.\n  spec = model_spec.get(spec)\n  # Gets training data and validation data.\n  train_data = text_classifier.DataLoader.from_csv(\n      filename=os.path.join(os.path.join(data_dir, 'train.tsv')),\n      text_column='sentence',\n      label_column='label',\n      model_spec=spec,",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "peekOfCode": "def main(_):\n  logging.set_verbosity(logging.INFO)\n  data_dir = download_demo_data()\n  export_dir = os.path.expanduser(FLAGS.export_dir)\n  run(data_dir, export_dir, spec=FLAGS.spec)\nif __name__ == '__main__':\n  define_flags()\n  app.run(main)",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef define_flags():\n  flags.DEFINE_string('export_dir', None,\n                      'The directory to save exported files.')\n  flags.DEFINE_string('spec', 'mobilebert_classifier',\n                      'The text classifier to run.')\n  flags.mark_flag_as_required('export_dir')\ndef download_demo_data(**kwargs):\n  \"\"\"Downloads demo data, and returns directory path.\"\"\"\n  data_path = tf.keras.utils.get_file(",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo",
        "documentation": {}
    },
    {
        "label": "TextClassificationDemoTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo_test",
        "peekOfCode": "class TextClassificationDemoTest(tf.test.TestCase):\n  def test_text_classification_demo(self):\n    with patch_data_loader():\n      with tempfile.TemporaryDirectory() as temp_dir:\n        # Use cached training data if exists.\n        data_dir = text_classification_demo.download_demo_data(\n            cache_dir=test_util.get_cache_dir(temp_dir, 'SST-2.zip'),\n            file_hash='9f81648d4199384278b86e315dac217c')\n        tflite_filename = os.path.join(temp_dir, 'model.tflite')\n        label_filename = os.path.join(temp_dir, 'labels.txt')",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo_test",
        "documentation": {}
    },
    {
        "label": "patch_data_loader",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo_test",
        "peekOfCode": "def patch_data_loader():\n  \"\"\"Patch to train partial dataset rather than all of them.\"\"\"\n  def side_effect(*args, **kwargs):\n    tf.compat.v1.logging.info('Train on partial dataset')\n    data_loader = from_csv_fn(*args, **kwargs)\n    if len(data_loader) > 2:  # Trim dataset to at most 2.\n      data_loader._size = 2\n      # TODO(b/171449557): Change this once dataset is lazily loaded.\n      data_loader._dataset = data_loader._dataset.take(2)\n    return data_loader",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo_test",
        "documentation": {}
    },
    {
        "label": "from_csv_fn",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo_test",
        "description": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo_test",
        "peekOfCode": "from_csv_fn = text_classifier.DataLoader.from_csv\ndef patch_data_loader():\n  \"\"\"Patch to train partial dataset rather than all of them.\"\"\"\n  def side_effect(*args, **kwargs):\n    tf.compat.v1.logging.info('Train on partial dataset')\n    data_loader = from_csv_fn(*args, **kwargs)\n    if len(data_loader) > 2:  # Trim dataset to at most 2.\n      data_loader._size = 2\n      # TODO(b/171449557): Change this once dataset is lazily loaded.\n      data_loader._dataset = data_loader._dataset.take(2)",
        "detail": "examples.tensorflow_examples.lite.model_maker.demo.text_classification_demo_test",
        "documentation": {}
    },
    {
        "label": "GoldenApiTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.golden_api_test",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.golden_api_test",
        "peekOfCode": "class GoldenApiTest(tf.test.TestCase, parameterized.TestCase):\n  @parameterized.parameters(GOLDEN.items())\n  def test_golden_apis(self, package: str, import_lines: List[str]):\n    \"\"\"Test all golden API symbols.\"\"\"\n    import tflite_model_maker  # pylint: disable=g-import-not-at-top\n    for line in import_lines:\n      if not package:\n        continue\n      parts = package.split('.')\n      # Get `c`, for `from a import c` or `from a import b as c`.",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.golden_api_test",
        "documentation": {}
    },
    {
        "label": "GOLDEN",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.golden_api_test",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.golden_api_test",
        "peekOfCode": "GOLDEN = api_gen.load_golden('golden_api.json')\nclass GoldenApiTest(tf.test.TestCase, parameterized.TestCase):\n  @parameterized.parameters(GOLDEN.items())\n  def test_golden_apis(self, package: str, import_lines: List[str]):\n    \"\"\"Test all golden API symbols.\"\"\"\n    import tflite_model_maker  # pylint: disable=g-import-not-at-top\n    for line in import_lines:\n      if not package:\n        continue\n      parts = package.split('.')",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.golden_api_test",
        "documentation": {}
    },
    {
        "label": "get_required_packages",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "def get_required_packages():\n  \"\"\"Gets packages inside requirements.txt.\"\"\"\n  # Gets model maker's required packages\n  filename = 'requirements_nightly.txt' if nightly else 'requirements.txt'\n  fpath = BASE_DIR.joinpath(filename)\n  required_pkgs = _read_required_packages(fpath)\n  return required_pkgs\nextra_options = setup_util.PackageGen(BASE_DIR, ROOT_DIR, BUILD_DIR, nightly,\n                                      version, LIB_NAMESPACE, API_NAMESPACE,\n                                      INTERNAL_NAME).run()",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "nightly",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "nightly = False\nif '--nightly' in sys.argv:\n  nightly = True\n  sys.argv.remove('--nightly')\nproject_name = 'tflite-model-maker'\ndatestring = datetime.datetime.now().strftime('%Y%m%d%H%M')\nclassifiers = [\n    'Intended Audience :: Developers',\n    'License :: OSI Approved :: Apache Software License',\n    'Topic :: Scientific/Engineering',",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "project_name",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "project_name = 'tflite-model-maker'\ndatestring = datetime.datetime.now().strftime('%Y%m%d%H%M')\nclassifiers = [\n    'Intended Audience :: Developers',\n    'License :: OSI Approved :: Apache Software License',\n    'Topic :: Scientific/Engineering',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'Topic :: Scientific/Engineering :: Mathematics',\n    'Topic :: Software Development',\n    'Topic :: Software Development :: Libraries',",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "datestring",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "datestring = datetime.datetime.now().strftime('%Y%m%d%H%M')\nclassifiers = [\n    'Intended Audience :: Developers',\n    'License :: OSI Approved :: Apache Software License',\n    'Topic :: Scientific/Engineering',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'Topic :: Scientific/Engineering :: Mathematics',\n    'Topic :: Software Development',\n    'Topic :: Software Development :: Libraries',\n    'Topic :: Software Development :: Libraries :: Python Modules',",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "classifiers",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "classifiers = [\n    'Intended Audience :: Developers',\n    'License :: OSI Approved :: Apache Software License',\n    'Topic :: Scientific/Engineering',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'Topic :: Scientific/Engineering :: Mathematics',\n    'Topic :: Software Development',\n    'Topic :: Software Development :: Libraries',\n    'Topic :: Software Development :: Libraries :: Python Modules',\n]",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "BASE_DIR = pathlib.Path(os.path.abspath(__file__)).parents[1]\n# Path to root dir: <repo>\nROOT_DIR = BASE_DIR.parents[2]\n# Original namespace of the lib.\nLIB_NAMESPACE = 'tensorflow_examples.lite.model_maker'\n# Official package namespace for API. Used as code name.\nAPI_NAMESPACE = 'tflite_model_maker'\n# Internal package tflite_model_maker.python mapping internal packages.\nINTERNAL_NAME = 'python'\nMODEL_MAKER_CONSOLE = 'tflite_model_maker=tflite_model_maker.python.cli.cli:main'",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "ROOT_DIR",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "ROOT_DIR = BASE_DIR.parents[2]\n# Original namespace of the lib.\nLIB_NAMESPACE = 'tensorflow_examples.lite.model_maker'\n# Official package namespace for API. Used as code name.\nAPI_NAMESPACE = 'tflite_model_maker'\n# Internal package tflite_model_maker.python mapping internal packages.\nINTERNAL_NAME = 'python'\nMODEL_MAKER_CONSOLE = 'tflite_model_maker=tflite_model_maker.python.cli.cli:main'\n# Build dir `pip_package/src`: copy all source code and create a package.\nSRC_NAME = 'src'",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "LIB_NAMESPACE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "LIB_NAMESPACE = 'tensorflow_examples.lite.model_maker'\n# Official package namespace for API. Used as code name.\nAPI_NAMESPACE = 'tflite_model_maker'\n# Internal package tflite_model_maker.python mapping internal packages.\nINTERNAL_NAME = 'python'\nMODEL_MAKER_CONSOLE = 'tflite_model_maker=tflite_model_maker.python.cli.cli:main'\n# Build dir `pip_package/src`: copy all source code and create a package.\nSRC_NAME = 'src'\nBUILD_DIR = BASE_DIR.joinpath('pip_package').joinpath(SRC_NAME)\n# Setup options.",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "API_NAMESPACE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "API_NAMESPACE = 'tflite_model_maker'\n# Internal package tflite_model_maker.python mapping internal packages.\nINTERNAL_NAME = 'python'\nMODEL_MAKER_CONSOLE = 'tflite_model_maker=tflite_model_maker.python.cli.cli:main'\n# Build dir `pip_package/src`: copy all source code and create a package.\nSRC_NAME = 'src'\nBUILD_DIR = BASE_DIR.joinpath('pip_package').joinpath(SRC_NAME)\n# Setup options.\nsetup_options = {\n    'package_dir': {",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "INTERNAL_NAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "INTERNAL_NAME = 'python'\nMODEL_MAKER_CONSOLE = 'tflite_model_maker=tflite_model_maker.python.cli.cli:main'\n# Build dir `pip_package/src`: copy all source code and create a package.\nSRC_NAME = 'src'\nBUILD_DIR = BASE_DIR.joinpath('pip_package').joinpath(SRC_NAME)\n# Setup options.\nsetup_options = {\n    'package_dir': {\n        '': SRC_NAME\n    },",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "MODEL_MAKER_CONSOLE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "MODEL_MAKER_CONSOLE = 'tflite_model_maker=tflite_model_maker.python.cli.cli:main'\n# Build dir `pip_package/src`: copy all source code and create a package.\nSRC_NAME = 'src'\nBUILD_DIR = BASE_DIR.joinpath('pip_package').joinpath(SRC_NAME)\n# Setup options.\nsetup_options = {\n    'package_dir': {\n        '': SRC_NAME\n    },\n    'entry_points': {",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "SRC_NAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "SRC_NAME = 'src'\nBUILD_DIR = BASE_DIR.joinpath('pip_package').joinpath(SRC_NAME)\n# Setup options.\nsetup_options = {\n    'package_dir': {\n        '': SRC_NAME\n    },\n    'entry_points': {\n        'console_scripts': [MODEL_MAKER_CONSOLE,],\n    },",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "BUILD_DIR",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "BUILD_DIR = BASE_DIR.joinpath('pip_package').joinpath(SRC_NAME)\n# Setup options.\nsetup_options = {\n    'package_dir': {\n        '': SRC_NAME\n    },\n    'entry_points': {\n        'console_scripts': [MODEL_MAKER_CONSOLE,],\n    },\n}",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "setup_options",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "setup_options = {\n    'package_dir': {\n        '': SRC_NAME\n    },\n    'entry_points': {\n        'console_scripts': [MODEL_MAKER_CONSOLE,],\n    },\n}\nDESCRIPTION = ('TFLite Model Maker: a model customization library for on-device'\n               ' applications.')",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "DESCRIPTION",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "DESCRIPTION = ('TFLite Model Maker: a model customization library for on-device'\n               ' applications.')\nwith BASE_DIR.joinpath('README.md').open() as readme_file:\n  LONG_DESCRIPTION = readme_file.read()\ndef _read_required_packages(fpath):\n  with fpath.open() as f:\n    required_pkgs = [l.strip() for l in f.read().splitlines()]\n    required_pkgs = list(\n        filter(lambda line: line and not line.startswith('#'), required_pkgs))\n  return required_pkgs",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "extra_options",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "peekOfCode": "extra_options = setup_util.PackageGen(BASE_DIR, ROOT_DIR, BUILD_DIR, nightly,\n                                      version, LIB_NAMESPACE, API_NAMESPACE,\n                                      INTERNAL_NAME).run()\nsetup_options.update(extra_options)\nsetup(\n    name=project_name,\n    version=version,\n    description=DESCRIPTION,\n    long_description=LONG_DESCRIPTION,\n    long_description_content_type='text/markdown',",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup",
        "documentation": {}
    },
    {
        "label": "PackageGen",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.pip_package.setup_util",
        "description": "examples.tensorflow_examples.lite.model_maker.pip_package.setup_util",
        "peekOfCode": "class PackageGen:\n  \"\"\"Organizes package and generates APIs structure.\"\"\"\n  def __init__(\n      self,\n      base_dir,\n      root_dir,\n      build_dir,\n      nightly,\n      version,\n      lib_ns,",
        "detail": "examples.tensorflow_examples.lite.model_maker.pip_package.setup_util",
        "documentation": {}
    },
    {
        "label": "policy_v0",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def policy_v0():\n  \"\"\"Autoaugment policy that was used in AutoAugment Detection Paper.\"\"\"\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [('TranslateX_BBox', 0.6, 4), ('Equalize', 0.8, 10)],\n      [('TranslateY_Only_BBoxes', 0.2, 2), ('Cutout', 0.8, 8)],\n      [('Sharpness', 0.0, 8), ('ShearX_BBox', 0.4, 0)],\n      [('ShearY_BBox', 1.0, 2), ('TranslateY_Only_BBoxes', 0.6, 6)],",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "policy_v1",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def policy_v1():\n  \"\"\"Autoaugment policy that was used in AutoAugment Detection Paper.\"\"\"\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [('TranslateX_BBox', 0.6, 4), ('Equalize', 0.8, 10)],\n      [('TranslateY_Only_BBoxes', 0.2, 2), ('Cutout', 0.8, 8)],\n      [('Sharpness', 0.0, 8), ('ShearX_BBox', 0.4, 0)],\n      [('ShearY_BBox', 1.0, 2), ('TranslateY_Only_BBoxes', 0.6, 6)],",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "policy_vtest",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def policy_vtest():\n  \"\"\"Autoaugment test policy for debugging.\"\"\"\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [('TranslateX_BBox', 1.0, 4), ('Equalize', 1.0, 10)],\n  ]\n  return policy\ndef policy_v2():",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "policy_v2",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def policy_v2():\n  \"\"\"Additional policy that performs well on object detection.\"\"\"\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [('Color', 0.0, 6), ('Cutout', 0.6, 8), ('Sharpness', 0.4, 8)],\n      [('Rotate_BBox', 0.4, 8), ('Sharpness', 0.4, 2),\n       ('Rotate_BBox', 0.8, 10)],\n      [('TranslateY_BBox', 1.0, 8), ('AutoContrast', 0.8, 2)],",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "policy_v3",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def policy_v3():\n  \"\"\"\"Additional policy that performs well on object detection.\"\"\"\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [('Posterize', 0.8, 2), ('TranslateX_BBox', 1.0, 8)],\n      [('BBox_Cutout', 0.2, 10), ('Sharpness', 1.0, 8)],\n      [('Rotate_BBox', 0.6, 8), ('Rotate_BBox', 0.8, 10)],\n      [('Equalize', 0.8, 10), ('AutoContrast', 0.2, 10)],",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "blend",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def blend(image1, image2, factor):\n  \"\"\"Blend image1 and image2 using 'factor'.\n  Factor can be above 0.0.  A value of 0.0 means only image1 is used.\n  A value of 1.0 means only image2 is used.  A value between 0.0 and\n  1.0 means we linearly interpolate the pixel values between the two\n  images.  A value greater than 1.0 \"extrapolates\" the difference\n  between the two pixel values, and we clip the results to values\n  between 0 and 255.\n  Args:\n    image1: An image Tensor of type uint8.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "cutout",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def cutout(image, pad_size, replace=0):\n  \"\"\"Apply cutout (https://arxiv.org/abs/1708.04552) to image.\n  This operation applies a (2*pad_size x 2*pad_size) mask of zeros to\n  a random location within `img`. The pixel values filled in will be of the\n  value `replace`. The located where the mask will be applied is randomly\n  chosen uniformly over the whole image.\n  Args:\n    image: An image Tensor of type uint8.\n    pad_size: Specifies how big the zero mask that will be generated is that\n      is applied to the image. The mask will be of size",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "solarize",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def solarize(image, threshold=128):\n  # For each pixel in the image, select the pixel\n  # if the value is less than the threshold.\n  # Otherwise, subtract 255 from the pixel.\n  return tf.where(image < threshold, image, 255 - image)\ndef solarize_add(image, addition=0, threshold=128):\n  # For each pixel in the image less than threshold\n  # we add 'addition' amount to it and then clip the\n  # pixel value to be between 0 and 255. The value\n  # of 'addition' is between -128 and 128.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "solarize_add",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def solarize_add(image, addition=0, threshold=128):\n  # For each pixel in the image less than threshold\n  # we add 'addition' amount to it and then clip the\n  # pixel value to be between 0 and 255. The value\n  # of 'addition' is between -128 and 128.\n  added_image = tf.cast(image, tf.int64) + addition\n  added_image = tf.cast(tf.clip_by_value(added_image, 0, 255), tf.uint8)\n  return tf.where(image < threshold, added_image, image)\ndef color(image, factor):\n  \"\"\"Equivalent of PIL Color.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "color",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def color(image, factor):\n  \"\"\"Equivalent of PIL Color.\"\"\"\n  degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n  return blend(degenerate, image, factor)\ndef contrast(image, factor):\n  \"\"\"Equivalent of PIL Contrast.\"\"\"\n  degenerate = tf.image.rgb_to_grayscale(image)\n  # Cast before calling tf.histogram.\n  degenerate = tf.cast(degenerate, tf.int32)\n  # Compute the grayscale histogram, then compute the mean pixel value,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "contrast",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def contrast(image, factor):\n  \"\"\"Equivalent of PIL Contrast.\"\"\"\n  degenerate = tf.image.rgb_to_grayscale(image)\n  # Cast before calling tf.histogram.\n  degenerate = tf.cast(degenerate, tf.int32)\n  # Compute the grayscale histogram, then compute the mean pixel value,\n  # and create a constant image size of that value.  Use that as the\n  # blending degenerate target of the original image.\n  mean = tf.reduce_mean(tf.cast(degenerate, tf.float32))\n  degenerate = tf.ones_like(degenerate, dtype=tf.float32) * mean",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "brightness",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def brightness(image, factor):\n  \"\"\"Equivalent of PIL Brightness.\"\"\"\n  degenerate = tf.zeros_like(image)\n  return blend(degenerate, image, factor)\ndef posterize(image, bits):\n  \"\"\"Equivalent of PIL Posterize.\"\"\"\n  shift = 8 - bits\n  return tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift)\ndef rotate(image, degrees, replace):\n  \"\"\"Rotates the image by degrees either clockwise or counterclockwise.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "posterize",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def posterize(image, bits):\n  \"\"\"Equivalent of PIL Posterize.\"\"\"\n  shift = 8 - bits\n  return tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift)\ndef rotate(image, degrees, replace):\n  \"\"\"Rotates the image by degrees either clockwise or counterclockwise.\n  Args:\n    image: An image Tensor of type uint8.\n    degrees: Float, a scalar angle in degrees to rotate all images by. If\n      degrees is positive the image will be rotated clockwise otherwise it will",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "rotate",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def rotate(image, degrees, replace):\n  \"\"\"Rotates the image by degrees either clockwise or counterclockwise.\n  Args:\n    image: An image Tensor of type uint8.\n    degrees: Float, a scalar angle in degrees to rotate all images by. If\n      degrees is positive the image will be rotated clockwise otherwise it will\n      be rotated counterclockwise.\n    replace: A one or three value 1D tensor to fill empty pixels caused by\n      the rotate operation.\n  Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "random_shift_bbox",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def random_shift_bbox(image, bbox, pixel_scaling, replace,\n                      new_min_bbox_coords=None):\n  \"\"\"Move the bbox and the image content to a slightly new random location.\n  Args:\n    image: 3D uint8 Tensor.\n    bbox: 1D Tensor that has 4 elements (min_y, min_x, max_y, max_x)\n      of type float that represents the normalized coordinates between 0 and 1.\n      The potential values for the new min corner of the bbox will be between\n      [old_min - pixel_scaling * bbox_height/2,\n       old_min - pixel_scaling * bbox_height/2].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "rotate_only_bboxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def rotate_only_bboxes(image, bboxes, prob, degrees, replace):\n  \"\"\"Apply rotate to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, rotate, func_changes_bbox, degrees, replace)\ndef shear_x_only_bboxes(image, bboxes, prob, level, replace):\n  \"\"\"Apply shear_x to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "shear_x_only_bboxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def shear_x_only_bboxes(image, bboxes, prob, level, replace):\n  \"\"\"Apply shear_x to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, shear_x, func_changes_bbox, level, replace)\ndef shear_y_only_bboxes(image, bboxes, prob, level, replace):\n  \"\"\"Apply shear_y to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "shear_y_only_bboxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def shear_y_only_bboxes(image, bboxes, prob, level, replace):\n  \"\"\"Apply shear_y to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, shear_y, func_changes_bbox, level, replace)\ndef translate_x_only_bboxes(image, bboxes, prob, pixels, replace):\n  \"\"\"Apply translate_x to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "translate_x_only_bboxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def translate_x_only_bboxes(image, bboxes, prob, pixels, replace):\n  \"\"\"Apply translate_x to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, translate_x, func_changes_bbox, pixels, replace)\ndef translate_y_only_bboxes(image, bboxes, prob, pixels, replace):\n  \"\"\"Apply translate_y to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "translate_y_only_bboxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def translate_y_only_bboxes(image, bboxes, prob, pixels, replace):\n  \"\"\"Apply translate_y to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, translate_y, func_changes_bbox, pixels, replace)\ndef flip_only_bboxes(image, bboxes, prob):\n  \"\"\"Apply flip_lr to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "flip_only_bboxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def flip_only_bboxes(image, bboxes, prob):\n  \"\"\"Apply flip_lr to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, tf.image.flip_left_right, func_changes_bbox)\ndef solarize_only_bboxes(image, bboxes, prob, threshold):\n  \"\"\"Apply solarize to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "solarize_only_bboxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def solarize_only_bboxes(image, bboxes, prob, threshold):\n  \"\"\"Apply solarize to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, solarize, func_changes_bbox, threshold)\ndef equalize_only_bboxes(image, bboxes, prob):\n  \"\"\"Apply equalize to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "equalize_only_bboxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def equalize_only_bboxes(image, bboxes, prob):\n  \"\"\"Apply equalize to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, equalize, func_changes_bbox)\ndef cutout_only_bboxes(image, bboxes, prob, pad_size, replace):\n  \"\"\"Apply cutout to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "cutout_only_bboxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def cutout_only_bboxes(image, bboxes, prob, pad_size, replace):\n  \"\"\"Apply cutout to each bbox in the image with probability prob.\"\"\"\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, cutout, func_changes_bbox, pad_size, replace)\ndef _rotate_bbox(bbox, image_height, image_width, degrees):\n  \"\"\"Rotates the bbox coordinated by degrees.\n  Args:\n    bbox: 1D Tensor that has 4 elements (min_y, min_x, max_y, max_x)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "rotate_with_bboxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def rotate_with_bboxes(image, bboxes, degrees, replace):\n  \"\"\"Equivalent of PIL Rotate that rotates the image and bbox.\n  Args:\n    image: 3D uint8 Tensor.\n    bboxes: 2D Tensor that is a list of the bboxes in the image. Each bbox\n      has 4 elements (min_y, min_x, max_y, max_x) of type float.\n    degrees: Float, a scalar angle in degrees to rotate all images by. If\n      degrees is positive the image will be rotated clockwise otherwise it will\n      be rotated counterclockwise.\n    replace: A one or three value 1D tensor to fill empty pixels.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "translate_x",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def translate_x(image, pixels, replace):\n  \"\"\"Equivalent of PIL Translate in X dimension.\"\"\"\n  image = image_ops.translate(wrap(image), [-pixels, 0])\n  return unwrap(image, replace)\ndef translate_y(image, pixels, replace):\n  \"\"\"Equivalent of PIL Translate in Y dimension.\"\"\"\n  image = image_ops.translate(wrap(image), [0, -pixels])\n  return unwrap(image, replace)\ndef _shift_bbox(bbox, image_height, image_width, pixels, shift_horizontal):\n  \"\"\"Shifts the bbox coordinates by pixels.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "translate_y",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def translate_y(image, pixels, replace):\n  \"\"\"Equivalent of PIL Translate in Y dimension.\"\"\"\n  image = image_ops.translate(wrap(image), [0, -pixels])\n  return unwrap(image, replace)\ndef _shift_bbox(bbox, image_height, image_width, pixels, shift_horizontal):\n  \"\"\"Shifts the bbox coordinates by pixels.\n  Args:\n    bbox: 1D Tensor that has 4 elements (min_y, min_x, max_y, max_x)\n      of type float that represents the normalized coordinates between 0 and 1.\n    image_height: Int, height of the image.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "translate_bbox",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def translate_bbox(image, bboxes, pixels, replace, shift_horizontal):\n  \"\"\"Equivalent of PIL Translate in X/Y dimension that shifts image and bbox.\n  Args:\n    image: 3D uint8 Tensor.\n    bboxes: 2D Tensor that is a list of the bboxes in the image. Each bbox\n      has 4 elements (min_y, min_x, max_y, max_x) of type float with values\n      between [0, 1].\n    pixels: An int. How many pixels to shift the image and bboxes\n    replace: A one or three value 1D tensor to fill empty pixels.\n    shift_horizontal: Boolean. If true then shift in X dimension else shift in",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "shear_x",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def shear_x(image, level, replace):\n  \"\"\"Equivalent of PIL Shearing in X dimension.\"\"\"\n  # Shear parallel to x axis is a projective transform\n  # with a matrix form of:\n  # [1  level\n  #  0  1].\n  image = image_ops.transform(\n      wrap(image), [1., level, 0., 0., 1., 0., 0., 0.])\n  return unwrap(image, replace)\ndef shear_y(image, level, replace):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "shear_y",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def shear_y(image, level, replace):\n  \"\"\"Equivalent of PIL Shearing in Y dimension.\"\"\"\n  # Shear parallel to y axis is a projective transform\n  # with a matrix form of:\n  # [1  0\n  #  level  1].\n  image = image_ops.transform(\n      wrap(image), [1., 0., 0., level, 1., 0., 0., 0.])\n  return unwrap(image, replace)\ndef _shear_bbox(bbox, image_height, image_width, level, shear_horizontal):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "shear_with_bboxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def shear_with_bboxes(image, bboxes, level, replace, shear_horizontal):\n  \"\"\"Applies Shear Transformation to the image and shifts the bboxes.\n  Args:\n    image: 3D uint8 Tensor.\n    bboxes: 2D Tensor that is a list of the bboxes in the image. Each bbox\n      has 4 elements (min_y, min_x, max_y, max_x) of type float with values\n      between [0, 1].\n    level: Float. How much to shear the image. This value will be between\n      -0.3 to 0.3.\n    replace: A one or three value 1D tensor to fill empty pixels.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "autocontrast",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def autocontrast(image):\n  \"\"\"Implements Autocontrast function from PIL using TF ops.\n  Args:\n    image: A 3D uint8 tensor.\n  Returns:\n    The image after it has had autocontrast applied to it and will be of type\n    uint8.\n  \"\"\"\n  def scale_channel(image):\n    \"\"\"Scale the 2D image using the autocontrast rule.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "sharpness",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def sharpness(image, factor):\n  \"\"\"Implements Sharpness function from PIL using TF ops.\"\"\"\n  orig_image = image\n  image = tf.cast(image, tf.float32)\n  # Make image 4D for conv operation.\n  image = tf.expand_dims(image, 0)\n  # SMOOTH PIL Kernel.\n  kernel = tf.constant(\n      [[1, 1, 1], [1, 5, 1], [1, 1, 1]], dtype=tf.float32,\n      shape=[3, 3, 1, 1]) / 13.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "equalize",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def equalize(image):\n  \"\"\"Implements Equalize function from PIL using TF ops.\"\"\"\n  def scale_channel(im, c):\n    \"\"\"Scale the data in the channel to implement equalize.\"\"\"\n    im = tf.cast(im[:, :, c], tf.int32)\n    # Compute the histogram of the image channel.\n    histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n    # For the purposes of computing the step, filter out the nonzeros.\n    nonzero = tf.where(tf.not_equal(histo, 0))\n    nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "wrap",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def wrap(image):\n  \"\"\"Returns 'image' with an extra channel set to all 1s.\"\"\"\n  shape = tf.shape(image)\n  extended_channel = tf.ones([shape[0], shape[1], 1], image.dtype)\n  extended = tf.concat([image, extended_channel], 2)\n  return extended\ndef unwrap(image, replace):\n  \"\"\"Unwraps an image produced by wrap.\n  Where there is a 0 in the last channel for every spatial position,\n  the rest of the three channels in that spatial dimension are grayed",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "unwrap",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def unwrap(image, replace):\n  \"\"\"Unwraps an image produced by wrap.\n  Where there is a 0 in the last channel for every spatial position,\n  the rest of the three channels in that spatial dimension are grayed\n  (set to 128).  Operations like translate and shear on a wrapped\n  Tensor will leave 0s in empty locations.  Some transformations look\n  at the intensity of values to do preprocessing, and we want these\n  empty pixels to assume the 'average' value, rather than pure black.\n  Args:\n    image: A 3D Image Tensor with 4 channels.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "bbox_cutout",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def bbox_cutout(image, bboxes, pad_fraction, replace_with_mean):\n  \"\"\"Applies cutout to the image according to bbox information.\n  This is a cutout variant that using bbox information to make more informed\n  decisions on where to place the cutout mask.\n  Args:\n    image: 3D uint8 Tensor.\n    bboxes: 2D Tensor that is a list of the bboxes in the image. Each bbox\n      has 4 elements (min_y, min_x, max_y, max_x) of type float with values\n      between [0, 1].\n    pad_fraction: Float that specifies how large the cutout mask should be in",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "level_to_arg",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def level_to_arg(hparams):\n  return {\n      'AutoContrast': lambda level: (),\n      'Equalize': lambda level: (),\n      'Posterize': lambda level: (int((level/_MAX_LEVEL) * 4),),\n      'Solarize': lambda level: (int((level/_MAX_LEVEL) * 256),),\n      'SolarizeAdd': lambda level: (int((level/_MAX_LEVEL) * 110),),\n      'Color': _enhance_level_to_arg,\n      'Contrast': _enhance_level_to_arg,\n      'Brightness': _enhance_level_to_arg,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "bbox_wrapper",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def bbox_wrapper(func):\n  \"\"\"Adds a bboxes function argument to func and returns unchanged bboxes.\"\"\"\n  def wrapper(images, bboxes, *args, **kwargs):\n    return (func(images, *args, **kwargs), bboxes)\n  return wrapper\ndef _parse_policy_info(name, prob, level, replace_value, augmentation_hparams):\n  \"\"\"Return the function that corresponds to `name` and update `level` param.\"\"\"\n  func = NAME_TO_FUNC[name]\n  args = level_to_arg(augmentation_hparams)[name](level)\n  # Check to see if prob is passed into function. This is used for operations",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "select_and_apply_random_policy",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def select_and_apply_random_policy(policies, image, bboxes):\n  \"\"\"Select a random policy from `policies` and apply it to `image`.\"\"\"\n  policy_to_select = tf.random_uniform([], maxval=len(policies), dtype=tf.int32)\n  # Note that using tf.case instead of tf.conds would result in significantly\n  # larger graphs and would even break export for some larger policies.\n  for (i, policy) in enumerate(policies):\n    image, bboxes = tf.cond(\n        tf.equal(i, policy_to_select),\n        lambda selected_policy=policy: selected_policy(image, bboxes),\n        lambda: (image, bboxes))",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "build_and_apply_nas_policy",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def build_and_apply_nas_policy(policies, image, bboxes,\n                               augmentation_hparams):\n  \"\"\"Build a policy from the given policies passed in and apply to image.\n  Args:\n    policies: list of lists of tuples in the form `(func, prob, level)`, `func`\n      is a string name of the augmentation function, `prob` is the probability\n      of applying the `func` operation, `level` is the input argument for\n      `func`.\n    image: tf.Tensor that the resulting policy will be applied to.\n    bboxes: tf.Tensor of shape [N, 4] representing ground truth boxes that are",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "distort_image_with_autoaugment",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def distort_image_with_autoaugment(image,\n                                   bboxes,\n                                   augmentation_name):\n  \"\"\"Applies the AutoAugment policy to `image` and `bboxes`.\n  Args:\n    image: `Tensor` of shape [height, width, 3] representing an image.\n    bboxes: `Tensor` of shape [N, 4] representing ground truth boxes that are\n      normalized between [0, 1].\n    augmentation_name: The name of the AutoAugment policy to use. The available\n      options are `v0`, `v1`, `v2`, `v3` and `test`. `v0` is the policy used for",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "distort_image_with_randaugment",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "def distort_image_with_randaugment(image, bboxes, num_layers, magnitude):\n  \"\"\"Applies the RandAugment to `image` and `bboxes`.\"\"\"\n  replace_value = [128, 128, 128]\n  tf.logging.info('Using RandAugment.')\n  augmentation_hparams = hparams_config.Config(\n      dict(\n          cutout_max_pad_fraction=0.75,\n          cutout_bbox_replace_with_mean=False,\n          cutout_const=100,\n          translate_const=250,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "_MAX_LEVEL",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "_MAX_LEVEL = 10.\n# Represents an invalid bounding box that is used for checking for padding\n# lists of bounding box coordinates for a few augmentation operations\n_INVALID_BOX = [[-1.0, -1.0, -1.0, -1.0]]\ndef policy_v0():\n  \"\"\"Autoaugment policy that was used in AutoAugment Detection Paper.\"\"\"\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "_INVALID_BOX",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "_INVALID_BOX = [[-1.0, -1.0, -1.0, -1.0]]\ndef policy_v0():\n  \"\"\"Autoaugment policy that was used in AutoAugment Detection Paper.\"\"\"\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [('TranslateX_BBox', 0.6, 4), ('Equalize', 0.8, 10)],\n      [('TranslateY_Only_BBoxes', 0.2, 2), ('Cutout', 0.8, 8)],\n      [('Sharpness', 0.0, 8), ('ShearX_BBox', 0.4, 0)],",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "NAME_TO_FUNC",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "peekOfCode": "NAME_TO_FUNC = {\n    'AutoContrast': autocontrast,\n    'Equalize': equalize,\n    'Posterize': posterize,\n    'Solarize': solarize,\n    'SolarizeAdd': solarize_add,\n    'Color': color,\n    'Contrast': contrast,\n    'Brightness': brightness,\n    'Sharpness': sharpness,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.autoaugment",
        "documentation": {}
    },
    {
        "label": "GridMask",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.gridmask",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.gridmask",
        "peekOfCode": "class GridMask(object):\n  \"\"\"GridMask class for grid masking augmentation.\"\"\"\n  def __init__(self,\n               prob=0.6,\n               ratio=0.6,\n               rotate=10,\n               gridmask_size_ratio=0.5,\n               fill=1,\n               interpolation=\"BILINEAR\"):\n    \"\"\"initialization.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.gridmask",
        "documentation": {}
    },
    {
        "label": "gridmask",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.gridmask",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.gridmask",
        "peekOfCode": "def gridmask(image,\n             boxes,\n             prob=0.5,\n             ratio=0.6,\n             rotate=10,\n             gridmask_size_ratio=0.5,\n             fill=1):\n  \"\"\"Callable instance of GridMask and transforms input image.\"\"\"\n  gridmask_obj = GridMask(\n      prob=prob,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.gridmask",
        "documentation": {}
    },
    {
        "label": "Mosaic",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.mosaic",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.mosaic",
        "peekOfCode": "class Mosaic:\n  \"\"\"Mosaic Augmentation class.\n  1. Mosaic sub images will not be preserving aspect ratio of original images.\n  2. Tested on static graphs and eager  execution.\n  3. This Implementation of mosaic augmentation is tested in tf2.x.\n  \"\"\"\n  def __init__(self,\n               out_size: Tuple(int, int)=(680, 680),\n               n_images: int = 4,\n               _minimum_mosaic_image_dim: int = 25):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.aug.mosaic",
        "documentation": {}
    },
    {
        "label": "policy_v0",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def policy_v0():\n  \"\"\"Autoaugment policy that was used in AutoAugment Paper.\"\"\"\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [('Equalize', 0.8, 1), ('ShearY', 0.8, 4)],\n      [('Color', 0.4, 9), ('Equalize', 0.6, 3)],\n      [('Color', 0.4, 1), ('Rotate', 0.6, 8)],\n      [('Solarize', 0.8, 3), ('Equalize', 0.4, 7)],",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "policy_vtest",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def policy_vtest():\n  \"\"\"Autoaugment test policy for debugging.\"\"\"\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [('TranslateX', 1.0, 4), ('Equalize', 1.0, 10)],\n  ]\n  return policy\ndef blend(image1, image2, factor):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "blend",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def blend(image1, image2, factor):\n  \"\"\"Blend image1 and image2 using 'factor'.\n  Factor can be above 0.0.  A value of 0.0 means only image1 is used.\n  A value of 1.0 means only image2 is used.  A value between 0.0 and\n  1.0 means we linearly interpolate the pixel values between the two\n  images.  A value greater than 1.0 \"extrapolates\" the difference\n  between the two pixel values, and we clip the results to values\n  between 0 and 255.\n  Args:\n    image1: An image Tensor of type uint8.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "cutout",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def cutout(image, pad_size, replace=0):\n  \"\"\"Apply cutout (https://arxiv.org/abs/1708.04552) to image.\n  This operation applies a (2*pad_size x 2*pad_size) mask of zeros to\n  a random location within `img`. The pixel values filled in will be of the\n  value `replace`. The located where the mask will be applied is randomly\n  chosen uniformly over the whole image.\n  Args:\n    image: An image Tensor of type uint8.\n    pad_size: Specifies how big the zero mask that will be generated is that\n      is applied to the image. The mask will be of size",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "solarize",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def solarize(image, threshold=128):\n  # For each pixel in the image, select the pixel\n  # if the value is less than the threshold.\n  # Otherwise, subtract 255 from the pixel.\n  return tf.where(image < threshold, image, 255 - image)\ndef solarize_add(image, addition=0, threshold=128):\n  # For each pixel in the image less than threshold\n  # we add 'addition' amount to it and then clip the\n  # pixel value to be between 0 and 255. The value\n  # of 'addition' is between -128 and 128.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "solarize_add",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def solarize_add(image, addition=0, threshold=128):\n  # For each pixel in the image less than threshold\n  # we add 'addition' amount to it and then clip the\n  # pixel value to be between 0 and 255. The value\n  # of 'addition' is between -128 and 128.\n  added_image = tf.cast(image, tf.int64) + addition\n  added_image = tf.cast(tf.clip_by_value(added_image, 0, 255), tf.uint8)\n  return tf.where(image < threshold, added_image, image)\ndef color(image, factor):\n  \"\"\"Equivalent of PIL Color.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "color",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def color(image, factor):\n  \"\"\"Equivalent of PIL Color.\"\"\"\n  degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n  return blend(degenerate, image, factor)\ndef contrast(image, factor):\n  \"\"\"Equivalent of PIL Contrast.\"\"\"\n  degenerate = tf.image.rgb_to_grayscale(image)\n  # Cast before calling tf.histogram.\n  degenerate = tf.cast(degenerate, tf.int32)\n  # Compute the grayscale histogram, then compute the mean pixel value,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "contrast",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def contrast(image, factor):\n  \"\"\"Equivalent of PIL Contrast.\"\"\"\n  degenerate = tf.image.rgb_to_grayscale(image)\n  # Cast before calling tf.histogram.\n  degenerate = tf.cast(degenerate, tf.int32)\n  # Compute the grayscale histogram, then compute the mean pixel value,\n  # and create a constant image size of that value.  Use that as the\n  # blending degenerate target of the original image.\n  hist = tf.histogram_fixed_width(degenerate, [0, 255], nbins=256)\n  mean = tf.reduce_sum(tf.cast(hist, tf.float32)) / 256.0",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "brightness",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def brightness(image, factor):\n  \"\"\"Equivalent of PIL Brightness.\"\"\"\n  degenerate = tf.zeros_like(image)\n  return blend(degenerate, image, factor)\ndef posterize(image, bits):\n  \"\"\"Equivalent of PIL Posterize.\"\"\"\n  shift = 8 - bits\n  return tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift)\ndef rotate(image, degrees, replace):\n  \"\"\"Rotates the image by degrees either clockwise or counterclockwise.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "posterize",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def posterize(image, bits):\n  \"\"\"Equivalent of PIL Posterize.\"\"\"\n  shift = 8 - bits\n  return tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift)\ndef rotate(image, degrees, replace):\n  \"\"\"Rotates the image by degrees either clockwise or counterclockwise.\n  Args:\n    image: An image Tensor of type uint8.\n    degrees: Float, a scalar angle in degrees to rotate all images by. If\n      degrees is positive the image will be rotated clockwise otherwise it will",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "rotate",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def rotate(image, degrees, replace):\n  \"\"\"Rotates the image by degrees either clockwise or counterclockwise.\n  Args:\n    image: An image Tensor of type uint8.\n    degrees: Float, a scalar angle in degrees to rotate all images by. If\n      degrees is positive the image will be rotated clockwise otherwise it will\n      be rotated counterclockwise.\n    replace: A one or three value 1D tensor to fill empty pixels caused by\n      the rotate operation.\n  Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "translate_x",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def translate_x(image, pixels, replace):\n  \"\"\"Equivalent of PIL Translate in X dimension.\"\"\"\n  image = contrib_image.translate(wrap(image), [-pixels, 0])\n  return unwrap(image, replace)\ndef translate_y(image, pixels, replace):\n  \"\"\"Equivalent of PIL Translate in Y dimension.\"\"\"\n  image = contrib_image.translate(wrap(image), [0, -pixels])\n  return unwrap(image, replace)\ndef shear_x(image, level, replace):\n  \"\"\"Equivalent of PIL Shearing in X dimension.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "translate_y",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def translate_y(image, pixels, replace):\n  \"\"\"Equivalent of PIL Translate in Y dimension.\"\"\"\n  image = contrib_image.translate(wrap(image), [0, -pixels])\n  return unwrap(image, replace)\ndef shear_x(image, level, replace):\n  \"\"\"Equivalent of PIL Shearing in X dimension.\"\"\"\n  # Shear parallel to x axis is a projective transform\n  # with a matrix form of:\n  # [1  level\n  #  0  1].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "shear_x",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def shear_x(image, level, replace):\n  \"\"\"Equivalent of PIL Shearing in X dimension.\"\"\"\n  # Shear parallel to x axis is a projective transform\n  # with a matrix form of:\n  # [1  level\n  #  0  1].\n  image = contrib_image.transform(\n      wrap(image), [1., level, 0., 0., 1., 0., 0., 0.])\n  return unwrap(image, replace)\ndef shear_y(image, level, replace):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "shear_y",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def shear_y(image, level, replace):\n  \"\"\"Equivalent of PIL Shearing in Y dimension.\"\"\"\n  # Shear parallel to y axis is a projective transform\n  # with a matrix form of:\n  # [1  0\n  #  level  1].\n  image = contrib_image.transform(\n      wrap(image), [1., 0., 0., level, 1., 0., 0., 0.])\n  return unwrap(image, replace)\ndef autocontrast(image):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "autocontrast",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def autocontrast(image):\n  \"\"\"Implements Autocontrast function from PIL using TF ops.\n  Args:\n    image: A 3D uint8 tensor.\n  Returns:\n    The image after it has had autocontrast applied to it and will be of type\n    uint8.\n  \"\"\"\n  def scale_channel(image):\n    \"\"\"Scale the 2D image using the autocontrast rule.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "sharpness",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def sharpness(image, factor):\n  \"\"\"Implements Sharpness function from PIL using TF ops.\"\"\"\n  orig_image = image\n  image = tf.cast(image, tf.float32)\n  # Make image 4D for conv operation.\n  image = tf.expand_dims(image, 0)\n  # SMOOTH PIL Kernel.\n  kernel = tf.constant(\n      [[1, 1, 1], [1, 5, 1], [1, 1, 1]], dtype=tf.float32,\n      shape=[3, 3, 1, 1]) / 13.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "equalize",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def equalize(image):\n  \"\"\"Implements Equalize function from PIL using TF ops.\"\"\"\n  def scale_channel(im, c):\n    \"\"\"Scale the data in the channel to implement equalize.\"\"\"\n    im = tf.cast(im[:, :, c], tf.int32)\n    # Compute the histogram of the image channel.\n    histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n    # For the purposes of computing the step, filter out the nonzeros.\n    nonzero = tf.where(tf.not_equal(histo, 0))\n    nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "invert",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def invert(image):\n  \"\"\"Inverts the image pixels.\"\"\"\n  image = tf.convert_to_tensor(image)\n  return 255 - image\ndef wrap(image):\n  \"\"\"Returns 'image' with an extra channel set to all 1s.\"\"\"\n  shape = tf.shape(image)\n  extended_channel = tf.ones([shape[0], shape[1], 1], image.dtype)\n  extended = tf.concat([image, extended_channel], 2)\n  return extended",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "wrap",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def wrap(image):\n  \"\"\"Returns 'image' with an extra channel set to all 1s.\"\"\"\n  shape = tf.shape(image)\n  extended_channel = tf.ones([shape[0], shape[1], 1], image.dtype)\n  extended = tf.concat([image, extended_channel], 2)\n  return extended\ndef unwrap(image, replace):\n  \"\"\"Unwraps an image produced by wrap.\n  Where there is a 0 in the last channel for every spatial position,\n  the rest of the three channels in that spatial dimension are grayed",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "unwrap",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def unwrap(image, replace):\n  \"\"\"Unwraps an image produced by wrap.\n  Where there is a 0 in the last channel for every spatial position,\n  the rest of the three channels in that spatial dimension are grayed\n  (set to 128).  Operations like translate and shear on a wrapped\n  Tensor will leave 0s in empty locations.  Some transformations look\n  at the intensity of values to do preprocessing, and we want these\n  empty pixels to assume the 'average' value, rather than pure black.\n  Args:\n    image: A 3D Image Tensor with 4 channels.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "level_to_arg",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def level_to_arg(hparams):\n  return {\n      'AutoContrast': lambda level: (),\n      'Equalize': lambda level: (),\n      'Invert': lambda level: (),\n      'Rotate': _rotate_level_to_arg,\n      'Posterize': lambda level: (int((level/_MAX_LEVEL) * 4),),\n      'Solarize': lambda level: (int((level/_MAX_LEVEL) * 256),),\n      'SolarizeAdd': lambda level: (int((level/_MAX_LEVEL) * 110),),\n      'Color': _enhance_level_to_arg,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "select_and_apply_random_policy",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def select_and_apply_random_policy(policies, image):\n  \"\"\"Select a random policy from `policies` and apply it to `image`.\"\"\"\n  policy_to_select = tf.random_uniform([], maxval=len(policies), dtype=tf.int32)\n  # Note that using tf.case instead of tf.conds would result in significantly\n  # larger graphs and would even break export for some larger policies.\n  for (i, policy) in enumerate(policies):\n    image = tf.cond(\n        tf.equal(i, policy_to_select),\n        lambda selected_policy=policy: selected_policy(image),\n        lambda: image)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "build_and_apply_nas_policy",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def build_and_apply_nas_policy(policies, image,\n                               augmentation_hparams):\n  \"\"\"Build a policy from the given policies passed in and apply to image.\n  Args:\n    policies: list of lists of tuples in the form `(func, prob, level)`, `func`\n      is a string name of the augmentation function, `prob` is the probability\n      of applying the `func` operation, `level` is the input argument for\n      `func`.\n    image: tf.Tensor that the resulting policy will be applied to.\n    augmentation_hparams: Hparams associated with the NAS learned policy.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "distort_image_with_autoaugment",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def distort_image_with_autoaugment(image, augmentation_name):\n  \"\"\"Applies the AutoAugment policy to `image`.\n  AutoAugment is from the paper: https://arxiv.org/abs/1805.09501.\n  Args:\n    image: `Tensor` of shape [height, width, 3] representing an image.\n    augmentation_name: The name of the AutoAugment policy to use. The available\n      options are `v0` and `test`. `v0` is the policy used for\n      all of the results in the paper and was found to achieve the best results\n      on the COCO dataset. `v1`, `v2` and `v3` are additional good policies\n      found on the COCO dataset that have slight variation in what operations",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "distort_image_with_randaugment",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "def distort_image_with_randaugment(image, num_layers, magnitude):\n  \"\"\"Applies the RandAugment policy to `image`.\n  RandAugment is from the paper https://arxiv.org/abs/1909.13719,\n  Args:\n    image: `Tensor` of shape [height, width, 3] representing an image.\n    num_layers: Integer, the number of augmentation transformations to apply\n      sequentially to an image. Represented as (N) in the paper. Usually best\n      values will be in the range [1, 3].\n    magnitude: Integer, shared magnitude across all augmentation operations.\n      Represented as (M) in the paper. Usually best values are in the range",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "_MAX_LEVEL",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "_MAX_LEVEL = 10.\ndef policy_v0():\n  \"\"\"Autoaugment policy that was used in AutoAugment Paper.\"\"\"\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [('Equalize', 0.8, 1), ('ShearY', 0.8, 4)],\n      [('Color', 0.4, 9), ('Equalize', 0.6, 3)],\n      [('Color', 0.4, 1), ('Rotate', 0.6, 8)],",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "NAME_TO_FUNC",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "peekOfCode": "NAME_TO_FUNC = {\n    'AutoContrast': autocontrast,\n    'Equalize': equalize,\n    'Invert': invert,\n    'Rotate': rotate,\n    'Posterize': posterize,\n    'Solarize': solarize,\n    'SolarizeAdd': solarize_add,\n    'Color': color,\n    'Contrast': contrast,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.autoaugment",
        "documentation": {}
    },
    {
        "label": "get_model_builder",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.backbone_factory",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.backbone_factory",
        "peekOfCode": "def get_model_builder(model_name):\n  \"\"\"Get the model_builder module for a given model name.\"\"\"\n  if model_name.startswith('efficientnet-lite'):\n    return efficientnet_lite_builder\n  elif model_name.startswith('efficientnet-'):\n    return efficientnet_builder\n  else:\n    raise ValueError('Unknown model name {}'.format(model_name))\ndef get_model(model_name, override_params=None, model_dir=None):\n  \"\"\"A helper function to create and return model.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.backbone_factory",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.backbone_factory",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.backbone_factory",
        "peekOfCode": "def get_model(model_name, override_params=None, model_dir=None):\n  \"\"\"A helper function to create and return model.\n  Args:\n    model_name: string, the predefined model name.\n    override_params: A dictionary of params for overriding. Fields must exist in\n      efficientnet_model.GlobalParams.\n    model_dir: string, optional model dir for saving configs.\n  Returns:\n    created model\n  Raises:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.backbone_factory",
        "documentation": {}
    },
    {
        "label": "BlockDecoder",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "peekOfCode": "class BlockDecoder(object):\n  \"\"\"Block Decoder for readability.\"\"\"\n  def _decode_block_string(self, block_string):\n    \"\"\"Gets a block through a string notation of arguments.\"\"\"\n    assert isinstance(block_string, str)\n    ops = block_string.split('_')\n    options = {}\n    for op in ops:\n      splits = re.split(r'(\\d.*)', op)\n      if len(splits) >= 2:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "documentation": {}
    },
    {
        "label": "efficientnet_params",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "peekOfCode": "def efficientnet_params(model_name):\n  \"\"\"Get efficientnet params based on model name.\"\"\"\n  params_dict = {\n      # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n      'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n      'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n      'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n      'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n      'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n      'efficientnet-b5': (1.6, 2.2, 456, 0.4),",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "documentation": {}
    },
    {
        "label": "swish",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "peekOfCode": "def swish(features, use_native=True, use_hard=False):\n  \"\"\"Computes the Swish activation function.\n  We provide three alternatives:\n    - Native tf.nn.swish, use less memory during training than composable swish.\n    - Quantization friendly hard swish.\n    - A composable swish, equivalent to tf.nn.swish, but more general for\n      finetuning and TF-Hub.\n  Args:\n    features: A `Tensor` representing preactivation values.\n    use_native: Whether to use the native swish from tf.nn that uses a custom",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "documentation": {}
    },
    {
        "label": "efficientnet",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "peekOfCode": "def efficientnet(width_coefficient=None,\n                 depth_coefficient=None,\n                 dropout_rate=0.2,\n                 survival_prob=0.8):\n  \"\"\"Creates a efficientnet model.\"\"\"\n  global_params = efficientnet_model.GlobalParams(\n      blocks_args=_DEFAULT_BLOCKS_ARGS,\n      batch_norm_momentum=0.99,\n      batch_norm_epsilon=1e-3,\n      dropout_rate=dropout_rate,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "documentation": {}
    },
    {
        "label": "get_model_params",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "peekOfCode": "def get_model_params(model_name, override_params):\n  \"\"\"Get the block args and global params for a given model.\"\"\"\n  if model_name.startswith('efficientnet'):\n    width_coefficient, depth_coefficient, _, dropout_rate = (\n        efficientnet_params(model_name))\n    global_params = efficientnet(\n        width_coefficient, depth_coefficient, dropout_rate)\n  else:\n    raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n  if override_params:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "documentation": {}
    },
    {
        "label": "build_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "peekOfCode": "def build_model(images,\n                model_name,\n                training,\n                override_params=None,\n                model_dir=None,\n                fine_tuning=False,\n                features_only=False,\n                pooled_features_only=False):\n  \"\"\"A helper function to create a model and return predicted logits.\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "documentation": {}
    },
    {
        "label": "build_model_base",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "peekOfCode": "def build_model_base(images, model_name, training, override_params=None):\n  \"\"\"Create a base feature network and return the features before pooling.\n  Args:\n    images: input images tensor.\n    model_name: string, the predefined model name.\n    training: boolean, whether the model is constructed for training.\n    override_params: A dictionary of params for overriding. Fields must exist in\n      efficientnet_model.GlobalParams.\n  Returns:\n    features: base features before pooling.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "documentation": {}
    },
    {
        "label": "_DEFAULT_BLOCKS_ARGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "peekOfCode": "_DEFAULT_BLOCKS_ARGS = [\n    'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n    'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n    'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n    'r1_k3_s11_e6_i192_o320_se0.25',\n]\ndef efficientnet(width_coefficient=None,\n                 depth_coefficient=None,\n                 dropout_rate=0.2,\n                 survival_prob=0.8):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_builder",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite_params",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "peekOfCode": "def efficientnet_lite_params(model_name):\n  \"\"\"Get efficientnet params based on model name.\"\"\"\n  params_dict = {\n      # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n      'efficientnet-lite0': (1.0, 1.0, 224, 0.2),\n      'efficientnet-lite1': (1.0, 1.1, 240, 0.2),\n      'efficientnet-lite2': (1.1, 1.2, 260, 0.3),\n      'efficientnet-lite3': (1.2, 1.4, 280, 0.3),\n      'efficientnet-lite4': (1.4, 1.8, 300, 0.3),\n  }",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "documentation": {}
    },
    {
        "label": "efficientnet_lite",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "peekOfCode": "def efficientnet_lite(width_coefficient=None,\n                      depth_coefficient=None,\n                      dropout_rate=0.2,\n                      survival_prob=0.8):\n  \"\"\"Creates a efficientnet model.\"\"\"\n  global_params = efficientnet_model.GlobalParams(\n      blocks_args=_DEFAULT_BLOCKS_ARGS,\n      batch_norm_momentum=0.99,\n      batch_norm_epsilon=1e-3,\n      dropout_rate=dropout_rate,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "documentation": {}
    },
    {
        "label": "get_model_params",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "peekOfCode": "def get_model_params(model_name, override_params):\n  \"\"\"Get the block args and global params for a given model.\"\"\"\n  if model_name.startswith('efficientnet-lite'):\n    width_coefficient, depth_coefficient, _, dropout_rate = (\n        efficientnet_lite_params(model_name))\n    global_params = efficientnet_lite(\n        width_coefficient, depth_coefficient, dropout_rate)\n  else:\n    raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n  if override_params:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "documentation": {}
    },
    {
        "label": "build_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "peekOfCode": "def build_model(images,\n                model_name,\n                training,\n                override_params=None,\n                model_dir=None,\n                fine_tuning=False,\n                features_only=False,\n                pooled_features_only=False):\n  \"\"\"A helper function to create a model and return predicted logits.\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "documentation": {}
    },
    {
        "label": "build_model_base",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "peekOfCode": "def build_model_base(images, model_name, training, override_params=None):\n  \"\"\"Create a base feature network and return the features before pooling.\n  Args:\n    images: input images tensor.\n    model_name: string, the predefined model name.\n    training: boolean, whether the model is constructed for training.\n    override_params: A dictionary of params for overriding. Fields must exist in\n      efficientnet_model.GlobalParams.\n  Returns:\n    features: base features before pooling.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "documentation": {}
    },
    {
        "label": "MEAN_RGB",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "peekOfCode": "MEAN_RGB = [127.0, 127.0, 127.0]\nSTDDEV_RGB = [128.0, 128.0, 128.0]\ndef efficientnet_lite_params(model_name):\n  \"\"\"Get efficientnet params based on model name.\"\"\"\n  params_dict = {\n      # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n      'efficientnet-lite0': (1.0, 1.0, 224, 0.2),\n      'efficientnet-lite1': (1.0, 1.1, 240, 0.2),\n      'efficientnet-lite2': (1.1, 1.2, 260, 0.3),\n      'efficientnet-lite3': (1.2, 1.4, 280, 0.3),",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "documentation": {}
    },
    {
        "label": "STDDEV_RGB",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "peekOfCode": "STDDEV_RGB = [128.0, 128.0, 128.0]\ndef efficientnet_lite_params(model_name):\n  \"\"\"Get efficientnet params based on model name.\"\"\"\n  params_dict = {\n      # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n      'efficientnet-lite0': (1.0, 1.0, 224, 0.2),\n      'efficientnet-lite1': (1.0, 1.1, 240, 0.2),\n      'efficientnet-lite2': (1.1, 1.2, 260, 0.3),\n      'efficientnet-lite3': (1.2, 1.4, 280, 0.3),\n      'efficientnet-lite4': (1.4, 1.8, 300, 0.3),",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "documentation": {}
    },
    {
        "label": "_DEFAULT_BLOCKS_ARGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "peekOfCode": "_DEFAULT_BLOCKS_ARGS = [\n    'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n    'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n    'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n    'r1_k3_s11_e6_i192_o320_se0.25',\n]\ndef efficientnet_lite(width_coefficient=None,\n                      depth_coefficient=None,\n                      dropout_rate=0.2,\n                      survival_prob=0.8):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_lite_builder",
        "documentation": {}
    },
    {
        "label": "SE",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "class SE(tf.keras.layers.Layer):\n  \"\"\"Squeeze-and-excitation layer.\"\"\"\n  def __init__(self, global_params, se_filters, output_filters, name=None):\n    super().__init__(name=name)\n    self._local_pooling = global_params.local_pooling\n    self._data_format = global_params.data_format\n    self._relu_fn = global_params.relu_fn or tf.nn.swish\n    # Squeeze and Excitation layer.\n    self._se_reduce = tf.keras.layers.Conv2D(\n        se_filters,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "SuperPixel",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "class SuperPixel(tf.keras.layers.Layer):\n  \"\"\"Super pixel layer.\"\"\"\n  def __init__(self, block_args, global_params, name=None):\n    super().__init__(name=name)\n    self._superpixel = tf.keras.layers.Conv2D(\n        block_args.input_filters,\n        kernel_size=[2, 2],\n        strides=[2, 2],\n        kernel_initializer=conv_kernel_initializer,\n        padding='same',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "MBConvBlock",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "class MBConvBlock(tf.keras.layers.Layer):\n  \"\"\"A class of MBConv: Mobile Inverted Residual Bottleneck.\n  Attributes:\n    endpoints: dict. A list of internal tensors.\n  \"\"\"\n  def __init__(self, block_args, global_params, name=None):\n    \"\"\"Initializes a MBConv block.\n    Args:\n      block_args: BlockArgs, arguments to create a Block.\n      global_params: GlobalParams, a set of global parameters.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "MBConvBlockWithoutDepthwise",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "class MBConvBlockWithoutDepthwise(MBConvBlock):\n  \"\"\"MBConv-like block without depthwise convolution and squeeze-and-excite.\"\"\"\n  def _build(self):\n    \"\"\"Builds block according to the arguments.\"\"\"\n    filters = self._block_args.input_filters * self._block_args.expand_ratio\n    # pylint: disable=g-long-lambda\n    cid = itertools.count(0)\n    get_conv_name = lambda: 'conv2d' + ('' if not next(cid) else '_' + str(\n        next(cid) // 2))\n    # pylint: enable=g-long-lambda",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "Stem",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "class Stem(tf.keras.layers.Layer):\n  \"\"\"Stem layer at the begining of the network.\"\"\"\n  def __init__(self, global_params, stem_filters, name=None):\n    super().__init__(name=name)\n    self._conv_stem = tf.keras.layers.Conv2D(\n        filters=round_filters(stem_filters, global_params,\n                              global_params.fix_head_stem),\n        kernel_size=[3, 3],\n        strides=[2, 2],\n        kernel_initializer=conv_kernel_initializer,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "Head",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "class Head(tf.keras.layers.Layer):\n  \"\"\"Head layer for network outputs.\"\"\"\n  def __init__(self, global_params, name=None):\n    super().__init__(name=name)\n    self.endpoints = {}\n    self._global_params = global_params\n    self._conv_head = tf.keras.layers.Conv2D(\n        filters=round_filters(1280, global_params, global_params.fix_head_stem),\n        kernel_size=[1, 1],\n        strides=[1, 1],",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "Model",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "class Model(tf.keras.Model):\n  \"\"\"A class implements tf.keras.Model.\n    Reference: https://arxiv.org/abs/1807.11626\n  \"\"\"\n  def __init__(self, blocks_args=None, global_params=None, name=None):\n    \"\"\"Initializes an `Model` instance.\n    Args:\n      blocks_args: A list of BlockArgs to construct block modules.\n      global_params: GlobalParams, a set of global parameters.\n      name: A string of layer name.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "conv_kernel_initializer",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "def conv_kernel_initializer(shape, dtype=None, partition_info=None):\n  \"\"\"Initialization for convolutional kernels.\n  The main difference with tf.variance_scaling_initializer is that\n  tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n  standard deviation, whereas here we use a normal distribution. Similarly,\n  tf.initializers.variance_scaling uses a truncated normal with\n  a corrected standard deviation.\n  Args:\n    shape: shape of variable\n    dtype: dtype of variable",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "dense_kernel_initializer",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "def dense_kernel_initializer(shape, dtype=None, partition_info=None):\n  \"\"\"Initialization for dense kernels.\n  This initialization is equal to\n    tf.variance_scaling_initializer(scale=1.0/3.0, mode='fan_out',\n                                    distribution='uniform').\n  It is written out explicitly here for clarity.\n  Args:\n    shape: shape of variable\n    dtype: dtype of variable\n    partition_info: unused",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "superpixel_kernel_initializer",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "def superpixel_kernel_initializer(shape, dtype='float32', partition_info=None):\n  \"\"\"Initializes superpixel kernels.\n  This is inspired by space-to-depth transformation that is mathematically\n  equivalent before and after the transformation. But we do the space-to-depth\n  via a convolution. Moreover, we make the layer trainable instead of direct\n  transform, we can initialization it this way so that the model can learn not\n  to do anything but keep it mathematically equivalent, when improving\n  performance.\n  Args:\n    shape: shape of variable",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "round_filters",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "def round_filters(filters, global_params, skip=False):\n  \"\"\"Round number of filters based on depth multiplier.\"\"\"\n  multiplier = global_params.width_coefficient\n  divisor = global_params.depth_divisor\n  min_depth = global_params.min_depth\n  if skip or not multiplier:\n    return filters\n  filters *= multiplier\n  min_depth = min_depth or divisor\n  new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "round_repeats",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "def round_repeats(repeats, global_params, skip=False):\n  \"\"\"Round number of filters based on depth multiplier.\"\"\"\n  multiplier = global_params.depth_coefficient\n  if skip or not multiplier:\n    return repeats\n  return int(math.ceil(multiplier * repeats))\nclass SE(tf.keras.layers.Layer):\n  \"\"\"Squeeze-and-excitation layer.\"\"\"\n  def __init__(self, global_params, se_filters, output_filters, name=None):\n    super().__init__(name=name)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "GlobalParams",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "GlobalParams = collections.namedtuple('GlobalParams', [\n    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate', 'data_format',\n    'num_classes', 'width_coefficient', 'depth_coefficient', 'depth_divisor',\n    'min_depth', 'survival_prob', 'relu_fn', 'batch_norm', 'use_se',\n    'local_pooling', 'condconv_num_experts', 'clip_projection_output',\n    'blocks_args', 'fix_head_stem', 'grad_checkpoint'\n])\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\nBlockArgs = collections.namedtuple('BlockArgs', [\n    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "GlobalParams.__new__.__defaults__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\nBlockArgs = collections.namedtuple('BlockArgs', [\n    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n    'expand_ratio', 'id_skip', 'strides', 'se_ratio', 'conv_type', 'fused_conv',\n    'super_pixel', 'condconv'\n])\n# defaults will be a public argument for namedtuple in Python 3.7\n# https://docs.python.org/3/library/collections.html#collections.namedtuple\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\ndef conv_kernel_initializer(shape, dtype=None, partition_info=None):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "BlockArgs",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "BlockArgs = collections.namedtuple('BlockArgs', [\n    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n    'expand_ratio', 'id_skip', 'strides', 'se_ratio', 'conv_type', 'fused_conv',\n    'super_pixel', 'condconv'\n])\n# defaults will be a public argument for namedtuple in Python 3.7\n# https://docs.python.org/3/library/collections.html#collections.namedtuple\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\ndef conv_kernel_initializer(shape, dtype=None, partition_info=None):\n  \"\"\"Initialization for convolutional kernels.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "BlockArgs.__new__.__defaults__",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "peekOfCode": "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\ndef conv_kernel_initializer(shape, dtype=None, partition_info=None):\n  \"\"\"Initialization for convolutional kernels.\n  The main difference with tf.variance_scaling_initializer is that\n  tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n  standard deviation, whereas here we use a normal distribution. Similarly,\n  tf.initializers.variance_scaling uses a truncated normal with\n  a corrected standard deviation.\n  Args:\n    shape: shape of variable",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.efficientnet_model",
        "documentation": {}
    },
    {
        "label": "distorted_bounding_box_crop",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "peekOfCode": "def distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "documentation": {}
    },
    {
        "label": "preprocess_for_train",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "peekOfCode": "def preprocess_for_train(image_bytes,\n                         use_bfloat16,\n                         image_size=IMAGE_SIZE,\n                         augment_name=None,\n                         randaug_num_layers=None,\n                         randaug_magnitude=None,\n                         resize_method=None):\n  \"\"\"Preprocesses the given image for evaluation.\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "documentation": {}
    },
    {
        "label": "preprocess_for_eval",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "peekOfCode": "def preprocess_for_eval(image_bytes,\n                        use_bfloat16,\n                        image_size=IMAGE_SIZE,\n                        resize_method=None):\n  \"\"\"Preprocesses the given image for evaluation.\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    use_bfloat16: `bool` for whether to use bfloat16.\n    image_size: image size.\n    resize_method: if None, use bicubic.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "documentation": {}
    },
    {
        "label": "preprocess_image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "peekOfCode": "def preprocess_image(image_bytes,\n                     is_training=False,\n                     use_bfloat16=False,\n                     image_size=IMAGE_SIZE,\n                     augment_name=None,\n                     randaug_num_layers=None,\n                     randaug_magnitude=None,\n                     resize_method=None):\n  \"\"\"Preprocesses the given image.\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "documentation": {}
    },
    {
        "label": "IMAGE_SIZE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "peekOfCode": "IMAGE_SIZE = 224\nCROP_PADDING = 32\ndef distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "documentation": {}
    },
    {
        "label": "CROP_PADDING",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "peekOfCode": "CROP_PADDING = 32\ndef distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n  See `tf.image.sample_distorted_bounding_box` for more documentation.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.preprocessing",
        "documentation": {}
    },
    {
        "label": "TrainableModel",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.train_backbone",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.train_backbone",
        "peekOfCode": "class TrainableModel(efficientnet_model.Model):\n  \"\"\"Wraps efficientnet to make a keras trainable model.\n  Handles efficientnet's multiple outputs and adds weight decay.\n  \"\"\"\n  def __init__(self,\n               blocks_args=None,\n               global_params=None,\n               name=None,\n               weight_decay=0.0):\n    super().__init__(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.train_backbone",
        "documentation": {}
    },
    {
        "label": "create_dataset",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.train_backbone",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.train_backbone",
        "peekOfCode": "def create_dataset(dataset: tf.data.Dataset, num_classes: int,\n                   is_training: bool) -> tf.data.Dataset:\n  \"\"\"Produces a full, augmented dataset from the inptu dataset.\"\"\"\n  _, _, resolution, _ = efficientnet_builder.efficientnet_params(\n      FLAGS.model_name)\n  def process_data(image, label):\n    image = preprocessing.preprocess_image(\n        image,\n        is_training=is_training,\n        use_bfloat16=FLAGS.strategy == 'tpus',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.train_backbone",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.train_backbone",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.train_backbone",
        "peekOfCode": "def main(_) -> None:\n  if FLAGS.strategy == 'tpu':\n    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n    tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\n    tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\n    ds_strategy = tf.distribute.TPUStrategy(tpu_cluster_resolver)\n    logging.info('All devices: %s', tf.config.list_logical_devices('TPU'))\n  elif FLAGS.strategy == 'gpus':\n    ds_strategy = tf.distribute.MirroredStrategy()",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.train_backbone",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.train_backbone",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.train_backbone",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef create_dataset(dataset: tf.data.Dataset, num_classes: int,\n                   is_training: bool) -> tf.data.Dataset:\n  \"\"\"Produces a full, augmented dataset from the inptu dataset.\"\"\"\n  _, _, resolution, _ = efficientnet_builder.efficientnet_params(\n      FLAGS.model_name)\n  def process_data(image, label):\n    image = preprocessing.preprocess_image(\n        image,\n        is_training=is_training,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.backbone.train_backbone",
        "documentation": {}
    },
    {
        "label": "create_tf_example",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_coco_tfrecord",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_coco_tfrecord",
        "peekOfCode": "def create_tf_example(image,\n                      image_dir,\n                      bbox_annotations=None,\n                      category_index=None,\n                      caption_annotations=None,\n                      include_masks=False):\n  \"\"\"Converts image and annotations to a tf.Example proto.\n  Args:\n    image: dict with keys: [u'license', u'file_name', u'coco_url', u'height',\n      u'width', u'date_captured', u'flickr_url', u'id']",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_coco_tfrecord",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_coco_tfrecord",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_coco_tfrecord",
        "peekOfCode": "def main(_):\n  assert FLAGS.image_dir, '`image_dir` missing.'\n  assert (FLAGS.image_info_file or FLAGS.object_annotations_file or\n          FLAGS.caption_annotations_file), ('All annotation files are '\n                                            'missing.')\n  if FLAGS.image_info_file:\n    image_info_file = FLAGS.image_info_file\n  elif FLAGS.object_annotations_file:\n    image_info_file = FLAGS.object_annotations_file\n  else:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_coco_tfrecord",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_coco_tfrecord",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_coco_tfrecord",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef create_tf_example(image,\n                      image_dir,\n                      bbox_annotations=None,\n                      category_index=None,\n                      caption_annotations=None,\n                      include_masks=False):\n  \"\"\"Converts image and annotations to a tf.Example proto.\n  Args:\n    image: dict with keys: [u'license', u'file_name', u'coco_url', u'height',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_coco_tfrecord",
        "documentation": {}
    },
    {
        "label": "UniqueId",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "peekOfCode": "class UniqueId(object):\n  \"\"\"Class to get the unique {image/ann}_id each time calling the functions.\"\"\"\n  def __init__(self):\n    self.image_id = 0\n    self.ann_id = 0\n  def get_image_id(self):\n    self.image_id += 1\n    return self.image_id\n  def get_ann_id(self):\n    self.ann_id += 1",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "peekOfCode": "def define_flags():\n  \"\"\"Define the flags.\"\"\"\n  flags.DEFINE_string('data_dir', '',\n                      'Root directory to raw PASCAL VOC dataset.')\n  flags.DEFINE_string('set', 'train', 'Convert training set, validation set or '\n                      'merged set.')\n  flags.DEFINE_string('annotations_dir', 'Annotations',\n                      '(Relative) path to annotations directory.')\n  flags.DEFINE_string('year', 'VOC2007', 'Desired challenge year.')\n  flags.DEFINE_string('output_path', '', 'Path to output TFRecord and json.')",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "documentation": {}
    },
    {
        "label": "dict_to_tf_example",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "peekOfCode": "def dict_to_tf_example(data,\n                       images_dir,\n                       label_map_dict,\n                       unique_id,\n                       ignore_difficult_instances=False,\n                       ann_json_dict=None):\n  \"\"\"Convert XML derived dict to tf.Example proto.\n  Notice that this function normalizes the bounding box coordinates provided\n  by the raw data.\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "peekOfCode": "def main(_):\n  if FLAGS.set not in SETS:\n    raise ValueError('set must be in : {}'.format(SETS))\n  if FLAGS.year not in YEARS:\n    raise ValueError('year must be in : {}'.format(YEARS))\n  if not FLAGS.output_path:\n    raise ValueError('output_path cannot be empty.')\n  data_dir = FLAGS.data_dir\n  years = ['VOC2007', 'VOC2012']\n  if FLAGS.year != 'merged':",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "peekOfCode": "FLAGS = flags.FLAGS\nSETS = ['train', 'val', 'trainval', 'test']\nYEARS = ['VOC2007', 'VOC2012', 'merged']\npascal_label_map_dict = {\n    'background': 0,\n    'aeroplane': 1,\n    'bicycle': 2,\n    'bird': 3,\n    'boat': 4,\n    'bottle': 5,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "documentation": {}
    },
    {
        "label": "SETS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "peekOfCode": "SETS = ['train', 'val', 'trainval', 'test']\nYEARS = ['VOC2007', 'VOC2012', 'merged']\npascal_label_map_dict = {\n    'background': 0,\n    'aeroplane': 1,\n    'bicycle': 2,\n    'bird': 3,\n    'boat': 4,\n    'bottle': 5,\n    'bus': 6,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "documentation": {}
    },
    {
        "label": "YEARS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "peekOfCode": "YEARS = ['VOC2007', 'VOC2012', 'merged']\npascal_label_map_dict = {\n    'background': 0,\n    'aeroplane': 1,\n    'bicycle': 2,\n    'bird': 3,\n    'boat': 4,\n    'bottle': 5,\n    'bus': 6,\n    'car': 7,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "documentation": {}
    },
    {
        "label": "pascal_label_map_dict",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "peekOfCode": "pascal_label_map_dict = {\n    'background': 0,\n    'aeroplane': 1,\n    'bicycle': 2,\n    'bird': 3,\n    'boat': 4,\n    'bottle': 5,\n    'bus': 6,\n    'car': 7,\n    'cat': 8,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.create_pascal_tfrecord",
        "documentation": {}
    },
    {
        "label": "RecordInspect",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.inspect_tfrecords",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.inspect_tfrecords",
        "peekOfCode": "class RecordInspect:\n  \"\"\"Inspection Class.\"\"\"\n  def __init__(self, config):\n    \"\"\"Initializes RecordInspect with passed config.\n    Args:\n        config: config file to initialize input_fn.\n    \"\"\"\n    self.input_fn = dataloader.InputReader(\n        FLAGS.file_pattern,\n        is_training=not FLAGS.eval,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.inspect_tfrecords",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.inspect_tfrecords",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.inspect_tfrecords",
        "peekOfCode": "def main(_):\n  # Parse and override hparams\n  config = hparams_config.get_detection_config(FLAGS.model_name)\n  config.override(FLAGS.hparams)\n  # Parse image size in case it is in string format.\n  config.image_size = utils.parse_image_size(config.image_size)\n  try:\n    recordinspect = RecordInspect(config)\n    recordinspect.visualize()\n  except Exception as e:  # pylint: disable=broad-except",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.inspect_tfrecords",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.inspect_tfrecords",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.inspect_tfrecords",
        "peekOfCode": "FLAGS = flags.FLAGS\nclass RecordInspect:\n  \"\"\"Inspection Class.\"\"\"\n  def __init__(self, config):\n    \"\"\"Initializes RecordInspect with passed config.\n    Args:\n        config: config file to initialize input_fn.\n    \"\"\"\n    self.input_fn = dataloader.InputReader(\n        FLAGS.file_pattern,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.inspect_tfrecords",
        "documentation": {}
    },
    {
        "label": "create_category_index",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.label_map_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.label_map_util",
        "peekOfCode": "def create_category_index(categories):\n  \"\"\"Creates dictionary of COCO compatible categories keyed by category id.\n  Args:\n    categories: a list of dicts, each of which has the following keys:\n      'id': (required) an integer id uniquely identifying this category.\n      'name': (required) string representing category name\n        e.g., 'cat', 'dog', 'pizza'.\n  Returns:\n    category_index: a dict containing the same entries as categories, but keyed\n      by the 'id' field of each category.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.label_map_util",
        "documentation": {}
    },
    {
        "label": "get_max_label_map_index",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.label_map_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.label_map_util",
        "peekOfCode": "def get_max_label_map_index(label_map):\n  \"\"\"Get maximum index in label map.\n  Args:\n    label_map: a StringIntLabelMapProto\n  Returns:\n    an integer\n  \"\"\"\n  return max([item.id for item in label_map.item])\ndef convert_label_map_to_categories(label_map,\n                                    max_num_classes,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.label_map_util",
        "documentation": {}
    },
    {
        "label": "convert_label_map_to_categories",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.label_map_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.label_map_util",
        "peekOfCode": "def convert_label_map_to_categories(label_map,\n                                    max_num_classes,\n                                    use_display_name=True):\n  \"\"\"Given label map proto returns categories list compatible with eval.\n  This function converts label map proto and returns a list of dicts, each of\n  which  has the following keys:\n    'id': (required) an integer id uniquely identifying this category.\n    'name': (required) string representing category name\n      e.g., 'cat', 'dog', 'pizza'.\n    'keypoints': (optional) a dictionary of keypoint string 'label' to integer",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.label_map_util",
        "documentation": {}
    },
    {
        "label": "create_class_agnostic_category_index",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.label_map_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.label_map_util",
        "peekOfCode": "def create_class_agnostic_category_index():\n  \"\"\"Creates a category index with a single `object` class.\"\"\"\n  return {1: {'id': 1, 'name': 'object'}}",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.label_map_util",
        "documentation": {}
    },
    {
        "label": "int64_feature",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "peekOfCode": "def int64_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\ndef int64_list_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\ndef bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\ndef bytes_list_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\ndef float_list_feature(value):\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "documentation": {}
    },
    {
        "label": "int64_list_feature",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "peekOfCode": "def int64_list_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\ndef bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\ndef bytes_list_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\ndef float_list_feature(value):\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\ndef read_examples_list(path):\n  \"\"\"Read list of training or validation examples.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "documentation": {}
    },
    {
        "label": "bytes_feature",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "peekOfCode": "def bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\ndef bytes_list_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\ndef float_list_feature(value):\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\ndef read_examples_list(path):\n  \"\"\"Read list of training or validation examples.\n  The file is assumed to contain a single example per line where the first\n  token in the line is an identifier that allows us to find the image and",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "documentation": {}
    },
    {
        "label": "bytes_list_feature",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "peekOfCode": "def bytes_list_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\ndef float_list_feature(value):\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\ndef read_examples_list(path):\n  \"\"\"Read list of training or validation examples.\n  The file is assumed to contain a single example per line where the first\n  token in the line is an identifier that allows us to find the image and\n  annotation xml for that example.\n  For example, the line:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "documentation": {}
    },
    {
        "label": "float_list_feature",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "peekOfCode": "def float_list_feature(value):\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\ndef read_examples_list(path):\n  \"\"\"Read list of training or validation examples.\n  The file is assumed to contain a single example per line where the first\n  token in the line is an identifier that allows us to find the image and\n  annotation xml for that example.\n  For example, the line:\n  xyz 3\n  would allow us to find files xyz.jpg and xyz.xml (the 3 would be ignored).",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "documentation": {}
    },
    {
        "label": "read_examples_list",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "peekOfCode": "def read_examples_list(path):\n  \"\"\"Read list of training or validation examples.\n  The file is assumed to contain a single example per line where the first\n  token in the line is an identifier that allows us to find the image and\n  annotation xml for that example.\n  For example, the line:\n  xyz 3\n  would allow us to find files xyz.jpg and xyz.xml (the 3 would be ignored).\n  Args:\n    path: absolute path to examples list file.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "documentation": {}
    },
    {
        "label": "recursive_parse_xml_to_dict",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "peekOfCode": "def recursive_parse_xml_to_dict(xml):\n  \"\"\"Recursively parses XML contents to python dict.\n  We assume that `object` tags are the only ones that can appear\n  multiple times at the same level of a tree.\n  Args:\n    xml: xml tree obtained by parsing XML file contents using lxml.etree\n  Returns:\n    Python dictionary holding XML contents.\n  \"\"\"\n  if not len(xml):  # pylint: disable=g-explicit-length-test",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "documentation": {}
    },
    {
        "label": "open_sharded_output_tfrecords",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "peekOfCode": "def open_sharded_output_tfrecords(exit_stack, base_path, num_shards):\n  \"\"\"Opens all TFRecord shards for writing and adds them to an exit stack.\n  Args:\n    exit_stack: A context2.ExitStack used to automatically closed the TFRecords\n      opened in this function.\n    base_path: The base path for all shards\n    num_shards: The number of shards\n  Returns:\n    The list of opened TFRecords. Position k in the list corresponds to shard k.\n  \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataset.tfrecord_util",
        "documentation": {}
    },
    {
        "label": "Anchors",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "peekOfCode": "class Anchors():\n  \"\"\"Multi-scale anchors class.\"\"\"\n  def __init__(self, min_level, max_level, num_scales, aspect_ratios,\n               anchor_scale, image_size):\n    \"\"\"Constructs multiscale anchors.\n    Args:\n      min_level: integer number of minimum level of the output feature pyramid.\n      max_level: integer number of maximum level of the output feature pyramid.\n      num_scales: integer number representing intermediate scales added\n        on each level. For instances, num_scales=2 adds two additional",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "documentation": {}
    },
    {
        "label": "AnchorLabeler",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "peekOfCode": "class AnchorLabeler(object):\n  \"\"\"Labeler for multiscale anchor boxes.\"\"\"\n  def __init__(self, anchors, num_classes, match_threshold=0.5):\n    \"\"\"Constructs anchor labeler to assign labels to anchors.\n    Args:\n      anchors: an instance of class Anchors.\n      num_classes: integer number representing number of classes in the dataset.\n      match_threshold: float number between 0 and 1 representing the threshold\n        to assign positive labels for anchors.\n    \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "documentation": {}
    },
    {
        "label": "decode_box_outputs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "peekOfCode": "def decode_box_outputs(pred_boxes, anchor_boxes):\n  \"\"\"Transforms relative regression coordinates to absolute positions.\n  Network predictions are normalized and relative to a given anchor; this\n  reverses the transformation and outputs absolute coordinates for the input\n  image.\n  Args:\n    pred_boxes: predicted box regression targets.\n    anchor_boxes: anchors on all feature levels.\n  Returns:\n    outputs: bounding boxes.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "documentation": {}
    },
    {
        "label": "decode_anchors_to_centersize",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "peekOfCode": "def decode_anchors_to_centersize(pred_boxes, anchor_boxes):\n  \"\"\"Transforms anchor boxes' encoding from box-corner to center-size.\n  Box-corner encoding is of form: {ymin, ymax, xmin, xmax}\n  Center-size encoding is of form: {y_center, x_center, height, width}\n  This is used for TFLite's custom NMS post-processing.\n  Args:\n    pred_boxes: predicted box regression targets.\n    anchor_boxes: anchors on all feature levels.\n  Returns:\n    outputs: anchor_boxes in center-size encoding.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "documentation": {}
    },
    {
        "label": "MAX_DETECTION_POINTS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "peekOfCode": "MAX_DETECTION_POINTS = 5000\ndef decode_box_outputs(pred_boxes, anchor_boxes):\n  \"\"\"Transforms relative regression coordinates to absolute positions.\n  Network predictions are normalized and relative to a given anchor; this\n  reverses the transformation and outputs absolute coordinates for the input\n  image.\n  Args:\n    pred_boxes: predicted box regression targets.\n    anchor_boxes: anchors on all feature levels.\n  Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.anchors",
        "documentation": {}
    },
    {
        "label": "FNode",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "peekOfCode": "class FNode(tf.keras.layers.Layer):\n  \"\"\"A Keras Layer implementing BiFPN Node.\"\"\"\n  def __init__(self,\n               feat_level,\n               inputs_offsets,\n               fpn_num_filters,\n               apply_bn_for_resampling,\n               is_training_bn,\n               conv_after_downsample,\n               conv_bn_act_pattern,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "documentation": {}
    },
    {
        "label": "OpAfterCombine",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "peekOfCode": "class OpAfterCombine(tf.keras.layers.Layer):\n  \"\"\"Operation after combining input features during feature fusiong.\"\"\"\n  def __init__(self,\n               is_training_bn,\n               conv_bn_act_pattern,\n               separable_conv,\n               fpn_num_filters,\n               act_type,\n               data_format,\n               strategy,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "documentation": {}
    },
    {
        "label": "ResampleFeatureMap",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "peekOfCode": "class ResampleFeatureMap(tf.keras.layers.Layer):\n  \"\"\"Resample feature map for downsampling or upsampling.\"\"\"\n  def __init__(self,\n               feat_level,\n               target_num_channels,\n               apply_bn=False,\n               is_training_bn=None,\n               conv_after_downsample=False,\n               strategy=None,\n               data_format=None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "documentation": {}
    },
    {
        "label": "ClassNet",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "peekOfCode": "class ClassNet(tf.keras.layers.Layer):\n  \"\"\"Object class prediction network.\"\"\"\n  def __init__(self,\n               num_classes=90,\n               num_anchors=9,\n               num_filters=32,\n               min_level=3,\n               max_level=7,\n               is_training_bn=False,\n               act_type='swish',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "documentation": {}
    },
    {
        "label": "BoxNet",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "peekOfCode": "class BoxNet(tf.keras.layers.Layer):\n  \"\"\"Box regression network.\"\"\"\n  def __init__(self,\n               num_anchors=9,\n               num_filters=32,\n               min_level=3,\n               max_level=7,\n               is_training_bn=False,\n               act_type='swish',\n               repeats=4,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "documentation": {}
    },
    {
        "label": "SegmentationHead",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "peekOfCode": "class SegmentationHead(tf.keras.layers.Layer):\n  \"\"\"Keras layer for semantic segmentation head.\"\"\"\n  def __init__(self,\n               num_classes,\n               num_filters,\n               min_level,\n               max_level,\n               data_format,\n               is_training_bn,\n               act_type,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "documentation": {}
    },
    {
        "label": "FPNCells",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "peekOfCode": "class FPNCells(tf.keras.layers.Layer):\n  \"\"\"FPN cells.\"\"\"\n  def __init__(self, config, name='fpn_cells'):\n    super().__init__(name=name)\n    self.config = config\n    if config.fpn_config:\n      self.fpn_config = config.fpn_config\n    else:\n      self.fpn_config = fpn_configs.get_fpn_config(config.fpn_name,\n                                                   config.min_level,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "documentation": {}
    },
    {
        "label": "FPNCell",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "peekOfCode": "class FPNCell(tf.keras.layers.Layer):\n  \"\"\"A single FPN cell.\"\"\"\n  def __init__(self, config, name='fpn_cell'):\n    super().__init__(name=name)\n    self.config = config\n    if config.fpn_config:\n      self.fpn_config = config.fpn_config\n    else:\n      self.fpn_config = fpn_configs.get_fpn_config(config.fpn_name,\n                                                   config.min_level,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "documentation": {}
    },
    {
        "label": "EfficientDetNet",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "peekOfCode": "class EfficientDetNet(tf.keras.Model):\n  \"\"\"EfficientDet keras network without pre/post-processing.\"\"\"\n  def __init__(self,\n               model_name=None,\n               config=None,\n               name='',\n               feature_only=False):\n    \"\"\"Initialize model.\"\"\"\n    super().__init__(name=name)\n    config = config or hparams_config.get_efficientdet_config(model_name)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "documentation": {}
    },
    {
        "label": "EfficientDetModel",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "peekOfCode": "class EfficientDetModel(EfficientDetNet):\n  \"\"\"EfficientDet full keras model with pre and post processing.\"\"\"\n  def _preprocessing(self,\n                     raw_images,\n                     image_size,\n                     mean_rgb,\n                     stddev_rgb,\n                     mode=None):\n    \"\"\"Preprocess images before feeding to the network.\"\"\"\n    if not mode:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "documentation": {}
    },
    {
        "label": "add_n",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "peekOfCode": "def add_n(nodes):\n  \"\"\"A customized add_n to add up a list of tensors.\"\"\"\n  # tf.add_n is not supported by EdgeTPU, while tf.reduce_sum is not supported\n  # by GPU and runs slow on EdgeTPU because of the 5-dimension op.\n  with tf.name_scope('add_n'):\n    new_node = nodes[0]\n    for n in nodes[1:]:\n      new_node = new_node + n\n    return new_node\nclass FNode(tf.keras.layers.Layer):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.efficientdet_keras",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval",
        "peekOfCode": "def main(_):\n  config = hparams_config.get_efficientdet_config(FLAGS.model_name)\n  config.override(FLAGS.hparams)\n  config.val_json_file = FLAGS.val_json_file\n  config.nms_configs.max_nms_inputs = anchors.MAX_DETECTION_POINTS\n  config.drop_remainder = False  # eval all examples w/o drop.\n  config.image_size = utils.parse_image_size(config['image_size'])\n  if config.strategy == 'tpu':\n    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef main(_):\n  config = hparams_config.get_efficientdet_config(FLAGS.model_name)\n  config.override(FLAGS.hparams)\n  config.val_json_file = FLAGS.val_json_file\n  config.nms_configs.max_nms_inputs = anchors.MAX_DETECTION_POINTS\n  config.drop_remainder = False  # eval all examples w/o drop.\n  config.image_size = utils.parse_image_size(config['image_size'])\n  if config.strategy == 'tpu':\n    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval",
        "documentation": {}
    },
    {
        "label": "LiteRunner",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval_tflite",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval_tflite",
        "peekOfCode": "class LiteRunner(object):\n  \"\"\"Runs inference with TF Lite model.\"\"\"\n  def __init__(self, tflite_model_path, only_network=False):\n    \"\"\"Initializes Lite runner with tflite model file.\n    Args:\n      tflite_model_path: Path to TFLite model file.\n      only_network: boolean, If True, TFLite model only contains EfficientDetNet\n        without post-processing NMS op. If False, TFLite model contains custom\n        NMS op.\n    \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval_tflite",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval_tflite",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval_tflite",
        "peekOfCode": "def define_flags():\n  \"\"\"Define the flags.\"\"\"\n  flags.DEFINE_integer('eval_samples', None, 'Number of eval samples.')\n  flags.DEFINE_string('val_file_pattern', None,\n                      'Glob for eval tfrecords, e.g. coco/val-*.tfrecord.')\n  flags.DEFINE_string('val_json_file', None,\n                      'Groudtruth, e.g. annotations/instances_val2017.json.')\n  flags.DEFINE_string('model_name', 'efficientdet-d0', 'Model name to use.')\n  flags.DEFINE_string('tflite_path', None, 'Path to TFLite model.')\n  flags.DEFINE_bool(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval_tflite",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval_tflite",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval_tflite",
        "peekOfCode": "def main(_):\n  config = hparams_config.get_efficientdet_config(FLAGS.model_name)\n  config.override(FLAGS.hparams)\n  config.val_json_file = FLAGS.val_json_file\n  config.nms_configs.max_nms_inputs = anchors.MAX_DETECTION_POINTS\n  config.drop_remainder = False  # eval all examples w/o drop.\n  config.image_size = utils.parse_image_size(config['image_size'])\n  # Evaluator for AP calculation.\n  label_map = label_util.get_label_map(config.label_map)\n  evaluator = coco_metric.EvaluationMetric(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval_tflite",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval_tflite",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval_tflite",
        "peekOfCode": "FLAGS = flags.FLAGS\nDEFAULT_SCALE, DEFAULT_ZERO_POINT = 0, 0\ndef define_flags():\n  \"\"\"Define the flags.\"\"\"\n  flags.DEFINE_integer('eval_samples', None, 'Number of eval samples.')\n  flags.DEFINE_string('val_file_pattern', None,\n                      'Glob for eval tfrecords, e.g. coco/val-*.tfrecord.')\n  flags.DEFINE_string('val_json_file', None,\n                      'Groudtruth, e.g. annotations/instances_val2017.json.')\n  flags.DEFINE_string('model_name', 'efficientdet-d0', 'Model name to use.')",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.eval_tflite",
        "documentation": {}
    },
    {
        "label": "bifpn_config",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.fpn_configs",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.fpn_configs",
        "peekOfCode": "def bifpn_config(min_level, max_level, weight_method):\n  \"\"\"A dynamic bifpn config that can adapt to different min/max levels.\"\"\"\n  p = hparams_config.Config()\n  p.weight_method = weight_method or 'fastattn'\n  # Node id starts from the input features and monotonically increase whenever\n  # a new node is added. Here is an example for level P3 - P7:\n  #     P7 (4)              P7\" (12)\n  #     P6 (3)    P6' (5)   P6\" (11)\n  #     P5 (2)    P5' (6)   P5\" (10)\n  #     P4 (1)    P4' (7)   P4\" (9)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.fpn_configs",
        "documentation": {}
    },
    {
        "label": "qufpn_config",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.fpn_configs",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.fpn_configs",
        "peekOfCode": "def qufpn_config(min_level, max_level, weight_method=None):\n  \"\"\"A dynamic quad fpn config that can adapt to different min/max levels.\"\"\"\n  # It extends the idea of BiFPN, and has four paths:\n  #   (up_down -> bottom_up) + (bottom_up -> up_down).\n  # See test for an example for level 2 and 7.\n  p = hparams_config.Config()\n  p.weight_method = weight_method or 'fastattn'\n  p.quad_method = 'fastattn'\n  num_levels = max_level - min_level + 1\n  node_ids = {min_level + i: [i] for i in range(num_levels)}",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.fpn_configs",
        "documentation": {}
    },
    {
        "label": "get_fpn_config",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.fpn_configs",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.fpn_configs",
        "peekOfCode": "def get_fpn_config(fpn_name, min_level, max_level, weight_method):\n  \"\"\"Get fpn related configuration.\"\"\"\n  if not fpn_name:\n    fpn_name = 'bifpn'\n  name_to_config = {\n      'bifpn': bifpn_config(min_level, max_level, weight_method),\n      'qufpn': qufpn_config(min_level, max_level, weight_method),\n      # legacy only: to be deprecated.\n      'bifpn_dyn': bifpn_config(min_level, max_level, weight_method),\n  }",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.fpn_configs",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer",
        "peekOfCode": "def main(_):\n  # pylint: disable=line-too-long\n  # Prepare images and checkpoints: please run these commands in shell.\n  # !mkdir tmp\n  # !wget https://user-images.githubusercontent.com/11736571/77320690-099af300-6d37-11ea-9d86-24f14dc2d540.png -O tmp/img.png\n  # !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d0.tar.gz -O tmp/efficientdet-d0.tar.gz\n  # !tar zxf tmp/efficientdet-d0.tar.gz -C tmp\n  imgs = [np.array(Image.open(FLAGS.image_path))]\n  # Create model config.\n  config = hparams_config.get_efficientdet_config(FLAGS.model_name)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef main(_):\n  # pylint: disable=line-too-long\n  # Prepare images and checkpoints: please run these commands in shell.\n  # !mkdir tmp\n  # !wget https://user-images.githubusercontent.com/11736571/77320690-099af300-6d37-11ea-9d86-24f14dc2d540.png -O tmp/img.png\n  # !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d0.tar.gz -O tmp/efficientdet-d0.tar.gz\n  # !tar zxf tmp/efficientdet-d0.tar.gz -C tmp\n  imgs = [np.array(Image.open(FLAGS.image_path))]\n  # Create model config.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer",
        "documentation": {}
    },
    {
        "label": "ExportNetwork",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer_lib",
        "peekOfCode": "class ExportNetwork(tf.Module):\n  def __init__(self, model):\n    super().__init__()\n    self.model = model\n  @tf.function\n  def __call__(self, imgs):\n    return tf.nest.flatten(self.model(imgs, training=False))\nclass ExportModel(tf.Module):\n  \"\"\"Model to be exported as SavedModel/TFLite format.\"\"\"\n  def __init__(self, model, pre_mode='infer', post_mode='global'):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer_lib",
        "documentation": {}
    },
    {
        "label": "ExportModel",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer_lib",
        "peekOfCode": "class ExportModel(tf.Module):\n  \"\"\"Model to be exported as SavedModel/TFLite format.\"\"\"\n  def __init__(self, model, pre_mode='infer', post_mode='global'):\n    super().__init__()\n    self.model = model\n    self.pre_mode = pre_mode\n    self.post_mode = post_mode\n  @tf.function\n  def __call__(self, imgs):\n    return self.model(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer_lib",
        "documentation": {}
    },
    {
        "label": "ServingDriver",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer_lib",
        "peekOfCode": "class ServingDriver:\n  \"\"\"A driver for serving single or batch images.\n  This driver supports serving with image files or arrays, with configurable\n  batch size.\n  Example 1. Serving streaming image contents:\n    driver = inference.ServingDriver(\n      'efficientdet-d0', '/tmp/efficientdet-d0', batch_size=1)\n    driver.build()\n    for m in image_iterator():\n      predictions = driver.serve_files([m])",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer_lib",
        "documentation": {}
    },
    {
        "label": "visualize_image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer_lib",
        "peekOfCode": "def visualize_image(image,\n                    boxes,\n                    classes,\n                    scores,\n                    label_map=None,\n                    min_score_thresh=0.01,\n                    max_boxes_to_draw=1000,\n                    line_thickness=2,\n                    **kwargs):\n  \"\"\"Visualizes a given image.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.infer_lib",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.inspector",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.inspector",
        "peekOfCode": "def main(_):\n  tf.config.run_functions_eagerly(FLAGS.debug)\n  devices = tf.config.list_physical_devices('GPU')\n  for device in devices:\n    tf.config.experimental.set_memory_growth(device, True)\n  model_config = hparams_config.get_detection_config(FLAGS.model_name)\n  model_config.override(FLAGS.hparams)  # Add custom overrides\n  model_config.is_training_bn = False\n  if FLAGS.image_size != -1:\n    model_config.image_size = FLAGS.image_size",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.inspector",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.inspector",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.inspector",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef main(_):\n  tf.config.run_functions_eagerly(FLAGS.debug)\n  devices = tf.config.list_physical_devices('GPU')\n  for device in devices:\n    tf.config.experimental.set_memory_growth(device, True)\n  model_config = hparams_config.get_detection_config(FLAGS.model_name)\n  model_config.override(FLAGS.hparams)  # Add custom overrides\n  model_config.is_training_bn = False\n  if FLAGS.image_size != -1:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.inspector",
        "documentation": {}
    },
    {
        "label": "get_label_map",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.label_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.label_util",
        "peekOfCode": "def get_label_map(mapping):\n  \"\"\"Get label id map based on the name, filename, or dict.\"\"\"\n  # case 1: if it is None or dict, just return it.\n  if not mapping or isinstance(mapping, dict):\n    return mapping\n  if isinstance(mapping, hparams_config.Config):\n    return mapping.as_dict()\n  # case 2: if it is a yaml file, load it to a dict and return the dict.\n  assert isinstance(mapping, str), 'mapping must be dict or str.'\n  if mapping.endswith('.yaml'):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.label_util",
        "documentation": {}
    },
    {
        "label": "coco",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.label_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.label_util",
        "peekOfCode": "coco = {\n    # 0: 'background',\n    1: 'person',\n    2: 'bicycle',\n    3: 'car',\n    4: 'motorcycle',\n    5: 'airplane',\n    6: 'bus',\n    7: 'train',\n    8: 'truck',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.label_util",
        "documentation": {}
    },
    {
        "label": "voc",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.label_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.label_util",
        "peekOfCode": "voc = {\n    # 0: 'background',\n    1: 'aeroplane',\n    2: 'bicycle',\n    3: 'bird',\n    4: 'boat',\n    5: 'bottle',\n    6: 'bus',\n    7: 'car',\n    8: 'cat',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.label_util",
        "documentation": {}
    },
    {
        "label": "waymo",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.label_util",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.label_util",
        "peekOfCode": "waymo = {\n    # 0: 'background',\n    1: 'vehicle',\n    2: 'pedestrian',\n    3: 'cyclist',\n}\ndef get_label_map(mapping):\n  \"\"\"Get label id map based on the name, filename, or dict.\"\"\"\n  # case 1: if it is None or dict, just return it.\n  if not mapping or isinstance(mapping, dict):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.label_util",
        "documentation": {}
    },
    {
        "label": "to_list",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def to_list(inputs):\n  if isinstance(inputs, dict):\n    return [inputs[k] for k in sorted(inputs.keys())]\n  if isinstance(inputs, list):\n    return inputs\ndef batch_map_fn(map_fn, inputs, *args):\n  \"\"\"Apply map_fn at batch dimension.\"\"\"\n  if isinstance(inputs[0], (list, tuple)):\n    batch_size = len(inputs[0])\n  else:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "batch_map_fn",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def batch_map_fn(map_fn, inputs, *args):\n  \"\"\"Apply map_fn at batch dimension.\"\"\"\n  if isinstance(inputs[0], (list, tuple)):\n    batch_size = len(inputs[0])\n  else:\n    batch_size = inputs[0].shape.as_list()[0]\n  if not batch_size:\n    # handle dynamic batch size: tf.vectorized_map is faster than tf.map_fn.\n    return tf.vectorized_map(map_fn, inputs, *args)\n  outputs = []",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "clip_boxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def clip_boxes(boxes: T, image_size: int) -> T:\n  \"\"\"Clip boxes to fit the image size.\"\"\"\n  image_size = utils.parse_image_size(image_size) * 2\n  return tf.clip_by_value(boxes, [0], image_size)\ndef merge_class_box_level_outputs(params, cls_outputs: List[T],\n                                  box_outputs: List[T]) -> Tuple[T, T]:\n  \"\"\"Concatenates class and box of all levels into one tensor.\"\"\"\n  cls_outputs_all, box_outputs_all = [], []\n  batch_size = tf.shape(cls_outputs[0])[0]\n  for level in range(0, params['max_level'] - params['min_level'] + 1):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "merge_class_box_level_outputs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def merge_class_box_level_outputs(params, cls_outputs: List[T],\n                                  box_outputs: List[T]) -> Tuple[T, T]:\n  \"\"\"Concatenates class and box of all levels into one tensor.\"\"\"\n  cls_outputs_all, box_outputs_all = [], []\n  batch_size = tf.shape(cls_outputs[0])[0]\n  for level in range(0, params['max_level'] - params['min_level'] + 1):\n    if params['data_format'] == 'channels_first':\n      cls_outputs[level] = tf.transpose(cls_outputs[level], [0, 2, 3, 1])\n      box_outputs[level] = tf.transpose(box_outputs[level], [0, 2, 3, 1])\n    cls_outputs_all.append(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "topk_class_boxes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def topk_class_boxes(params, cls_outputs: T,\n                     box_outputs: T) -> Tuple[T, T, T, T]:\n  \"\"\"Pick the topk class and box outputs.\"\"\"\n  batch_size = tf.shape(cls_outputs)[0]\n  num_classes = params['num_classes']\n  max_nms_inputs = params['nms_configs'].get('max_nms_inputs', 0)\n  if max_nms_inputs > 0:\n    # Prune anchors and detections to only keep max_nms_inputs.\n    # Due to some issues, top_k is currently slow in graph model.\n    logging.info('use max_nms_inputs for pre-nms topk.')",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "pre_nms",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def pre_nms(params, cls_outputs, box_outputs, topk=True):\n  \"\"\"Detection post processing before nms.\n  It takes the multi-level class and box predictions from network, merge them\n  into unified tensors, and compute boxes, scores, and classes.\n  Args:\n    params: a dict of parameters.\n    cls_outputs: a list of tensors for classes, each tensor denotes a level of\n      logits with shape [N, H, W, num_class * num_anchors].\n    box_outputs: a list of tensors for boxes, each tensor ddenotes a level of\n      boxes with shape [N, H, W, 4 * num_anchors].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "nms",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def nms(params, boxes: T, scores: T, classes: T,\n        padded: bool) -> Tuple[T, T, T, T]:\n  \"\"\"Non-maximum suppression.\n  Args:\n    params: a dict of parameters.\n    boxes: a tensor with shape [N, 4], where N is the number of boxes. Box\n      format is [y_min, x_min, y_max, x_max].\n    scores: a tensor with shape [N].\n    classes: a tensor with shape [N].\n    padded: a bool vallue indicating whether the results are padded.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "postprocess_combined",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def postprocess_combined(params, cls_outputs, box_outputs, image_scales=None):\n  \"\"\"Post processing with combined NMS.\n  Leverage the tf combined NMS. It is fast on TensorRT, but slow on CPU/GPU.\n  Args:\n    params: a dict of parameters.\n    cls_outputs: a list of tensors for classes, each tensor denotes a level of\n      logits with shape [N, H, W, num_class * num_anchors].\n    box_outputs: a list of tensors for boxes, each tensor ddenotes a level of\n      boxes with shape [N, H, W, 4 * num_anchors]. Each box format is [y_min,\n      x_min, y_max, x_man].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "tflite_nms_implements_signature",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def tflite_nms_implements_signature(params):\n  \"\"\"`experimental_implements` signature for TFLite's custom NMS op.\n  This signature encodes the arguments to correctly initialize TFLite's custom\n  post-processing op in the MLIR converter.\n  For details on `experimental_implements` see here:\n  https://www.tensorflow.org/api_docs/python/tf/function\n  Args:\n    params: a dict of parameters.\n  Returns:\n    String encoding of a map from attribute keys to values.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "tflite_pre_nms",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def tflite_pre_nms(params, cls_outputs, box_outputs):\n  \"\"\"Pre-NMS that is compatible with TFLite's custom NMS op.\n  For details, see tensorflow/lite/kernels/detection_postprocess.cc\n  Args:\n    params: a dict of parameters.\n    cls_outputs: a list of tensors for classes, each tensor denotes a level of\n      logits with shape [1, H, W, num_class * num_anchors].\n    box_outputs: a list of tensors for boxes, each tensor ddenotes a level of\n      boxes with shape [1, H, W, 4 * num_anchors]. Each box format is [y_min,\n      x_min, y_max, x_man].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "postprocess_tflite",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def postprocess_tflite(params, cls_outputs, box_outputs):\n  \"\"\"Post processing for conversion to TFLite.\n  Mathematically same as postprocess_global, except that the last portion of the\n  TF graph constitutes a dummy `tf.function` that contains an annotation for\n  conversion to TFLite's custom NMS op. Using this custom op allows features\n  like post-training quantization & accelerator support.\n  NOTE: This function does NOT return a valid output, and is only meant to\n  generate a SavedModel for TFLite conversion via MLIR.\n  For TFLite op details, see tensorflow/lite/kernels/detection_postprocess.cc\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "postprocess_global",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def postprocess_global(params, cls_outputs, box_outputs, image_scales=None):\n  \"\"\"Post processing with global NMS.\n  A fast but less accurate version of NMS. The idea is to treat the scores for\n  different classes in a unified way, and perform NMS globally for all classes.\n  Args:\n    params: a dict of parameters.\n    cls_outputs: a list of tensors for classes, each tensor denotes a level of\n      logits with shape [N, H, W, num_class * num_anchors].\n    box_outputs: a list of tensors for boxes, each tensor ddenotes a level of\n      boxes with shape [N, H, W, 4 * num_anchors]. Each box format is [y_min,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "per_class_nms",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def per_class_nms(params, boxes, scores, classes, image_scales=None):\n  \"\"\"Per-class nms, a utility for postprocess_per_class.\n  Args:\n    params: a dict of parameters.\n    boxes: A tensor with shape [N, K, 4], where N is batch_size, K is num_boxes.\n      Box format is [y_min, x_min, y_max, x_max].\n    scores: A tensor with shape [N, K].\n    classes: A tensor with shape [N, K].\n    image_scales: scaling factor or the final image and bounding boxes.\n  Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "postprocess_per_class",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def postprocess_per_class(params, cls_outputs, box_outputs, image_scales=None):\n  \"\"\"Post processing with per class NMS.\n  An accurate but relatively slow version of NMS. The idea is to perform NMS for\n  each class, and then combine them.\n  Args:\n    params: a dict of parameters.\n    cls_outputs: a list of tensors for classes, each tensor denotes a level of\n      logits with shape [N, H, W, num_class * num_anchors].\n    box_outputs: a list of tensors for boxes, each tensor ddenotes a level of\n      boxes with shape [N, H, W, 4 * num_anchors]. Each box format is [y_min,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "generate_detections_from_nms_output",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def generate_detections_from_nms_output(nms_boxes_bs,\n                                        nms_classes_bs,\n                                        nms_scores_bs,\n                                        image_ids,\n                                        original_image_widths=None,\n                                        flip=False):\n  \"\"\"Generating [id, x, y, w, h, score, class] from NMS outputs.\"\"\"\n  image_ids_bs = tf.cast(tf.expand_dims(image_ids, -1), nms_scores_bs.dtype)\n  if flip:\n    detections_bs = [",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "generate_detections",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def generate_detections(params,\n                        cls_outputs,\n                        box_outputs,\n                        image_scales,\n                        image_ids,\n                        flip=False,\n                        pre_class_nms=True):\n  \"\"\"A legacy interface for generating [id, x, y, w, h, score, class].\"\"\"\n  _, width = utils.parse_image_size(params['image_size'])\n  original_image_widths = tf.expand_dims(image_scales, -1) * width",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "transform_detections",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "def transform_detections(detections):\n  \"\"\"A transforms detections in [id, x1, y1, x2, y2, score, class] form to [id, x, y, w, h, score, class].\"\"\"\n  return tf.stack(  #\n      [\n          detections[:, :, 0],\n          detections[:, :, 1],\n          detections[:, :, 2],\n          detections[:, :, 3] - detections[:, :, 1],\n          detections[:, :, 4] - detections[:, :, 2],\n          detections[:, :, 5],",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "T = tf.Tensor  # a shortcut for typing check.\nCLASS_OFFSET = 1\n# TFLite-specific constants.\nTFLITE_MAX_CLASSES_PER_DETECTION = 1\nTFLITE_DETECTION_POSTPROCESS_FUNC = 'TFLite_Detection_PostProcess'\n# TFLite fast NMS == postprocess_global (less accurate)\n# TFLite regular NMS == postprocess_per_class\nTFLITE_USE_REGULAR_NMS = False\ndef to_list(inputs):\n  if isinstance(inputs, dict):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "CLASS_OFFSET",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "CLASS_OFFSET = 1\n# TFLite-specific constants.\nTFLITE_MAX_CLASSES_PER_DETECTION = 1\nTFLITE_DETECTION_POSTPROCESS_FUNC = 'TFLite_Detection_PostProcess'\n# TFLite fast NMS == postprocess_global (less accurate)\n# TFLite regular NMS == postprocess_per_class\nTFLITE_USE_REGULAR_NMS = False\ndef to_list(inputs):\n  if isinstance(inputs, dict):\n    return [inputs[k] for k in sorted(inputs.keys())]",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "TFLITE_MAX_CLASSES_PER_DETECTION",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "TFLITE_MAX_CLASSES_PER_DETECTION = 1\nTFLITE_DETECTION_POSTPROCESS_FUNC = 'TFLite_Detection_PostProcess'\n# TFLite fast NMS == postprocess_global (less accurate)\n# TFLite regular NMS == postprocess_per_class\nTFLITE_USE_REGULAR_NMS = False\ndef to_list(inputs):\n  if isinstance(inputs, dict):\n    return [inputs[k] for k in sorted(inputs.keys())]\n  if isinstance(inputs, list):\n    return inputs",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "TFLITE_DETECTION_POSTPROCESS_FUNC",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "TFLITE_DETECTION_POSTPROCESS_FUNC = 'TFLite_Detection_PostProcess'\n# TFLite fast NMS == postprocess_global (less accurate)\n# TFLite regular NMS == postprocess_per_class\nTFLITE_USE_REGULAR_NMS = False\ndef to_list(inputs):\n  if isinstance(inputs, dict):\n    return [inputs[k] for k in sorted(inputs.keys())]\n  if isinstance(inputs, list):\n    return inputs\ndef batch_map_fn(map_fn, inputs, *args):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "TFLITE_USE_REGULAR_NMS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "peekOfCode": "TFLITE_USE_REGULAR_NMS = False\ndef to_list(inputs):\n  if isinstance(inputs, dict):\n    return [inputs[k] for k in sorted(inputs.keys())]\n  if isinstance(inputs, list):\n    return inputs\ndef batch_map_fn(map_fn, inputs, *args):\n  \"\"\"Apply map_fn at batch dimension.\"\"\"\n  if isinstance(inputs[0], (list, tuple)):\n    batch_size = len(inputs[0])",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.postprocess",
        "documentation": {}
    },
    {
        "label": "create_mask",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "peekOfCode": "def create_mask(pred_mask):\n  pred_mask = tf.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[..., tf.newaxis]\n  return pred_mask[0]\ndef normalize(input_image, input_mask):\n  input_image = tf.cast(input_image, tf.float32) / 255.0\n  input_mask -= 1\n  return input_image, input_mask\ndef load_image_train(datapoint):\n  \"\"\"Load images for training.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "documentation": {}
    },
    {
        "label": "normalize",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "peekOfCode": "def normalize(input_image, input_mask):\n  input_image = tf.cast(input_image, tf.float32) / 255.0\n  input_mask -= 1\n  return input_image, input_mask\ndef load_image_train(datapoint):\n  \"\"\"Load images for training.\"\"\"\n  input_image = tf.image.resize(datapoint['image'], (512, 512))\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n  if tf.random.uniform(()) > 0.5:\n    input_image = tf.image.flip_left_right(input_image)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "documentation": {}
    },
    {
        "label": "load_image_train",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "peekOfCode": "def load_image_train(datapoint):\n  \"\"\"Load images for training.\"\"\"\n  input_image = tf.image.resize(datapoint['image'], (512, 512))\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n  if tf.random.uniform(()) > 0.5:\n    input_image = tf.image.flip_left_right(input_image)\n    input_mask = tf.image.flip_left_right(input_mask)\n  input_image, input_mask = normalize(input_image, input_mask)\n  return input_image, input_mask\ndef load_image_test(datapoint):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "documentation": {}
    },
    {
        "label": "load_image_test",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "peekOfCode": "def load_image_test(datapoint):\n  input_image = tf.image.resize(datapoint['image'], (512, 512))\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n  input_image, input_mask = normalize(input_image, input_mask)\n  return input_image, input_mask\ndef main(_):\n  dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n  train_examples = info.splits['train'].num_examples\n  batch_size = 8\n  steps_per_epoch = train_examples // batch_size",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "peekOfCode": "def main(_):\n  dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n  train_examples = info.splits['train'].num_examples\n  batch_size = 8\n  steps_per_epoch = train_examples // batch_size\n  train = dataset['train'].map(\n      load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  test = dataset['test'].map(load_image_test)\n  train_dataset = train.cache().shuffle(1000).batch(batch_size).repeat()\n  train_dataset = train_dataset.prefetch(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.segmentation",
        "documentation": {}
    },
    {
        "label": "quantize",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.tfmot",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.tfmot",
        "peekOfCode": "def quantize(layer, quantize_config=None):\n  if quantize_config is None:\n    quantize_config = default_8bit_quantize_configs.Default8BitOutputQuantizeConfig(\n    )\n  return quantize_wrapper.QuantizeWrapper(\n      layer, quantize_config=quantize_config)\noptimzation_methods = {\n    'prune': tfmot.sparsity.keras.prune_low_magnitude,\n    'quantize': quantize\n}",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.tfmot",
        "documentation": {}
    },
    {
        "label": "set_config",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.tfmot",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.tfmot",
        "peekOfCode": "def set_config(configs):\n  for key in configs:\n    if key == 'prune':\n      optimzation_methods[key] = functools.partial(\n          tfmot.sparsity.keras.prune_low_magnitude, **configs[key])\n    if key == 'quantize':\n      optimzation_methods[key] = functools.partial(quantize, **configs[key])\ndef get_method(method):\n  if method not in optimzation_methods:\n    raise KeyError(f'only support {optimzation_methods.keys()}')",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.tfmot",
        "documentation": {}
    },
    {
        "label": "get_method",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.tfmot",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.tfmot",
        "peekOfCode": "def get_method(method):\n  if method not in optimzation_methods:\n    raise KeyError(f'only support {optimzation_methods.keys()}')\n  return optimzation_methods[method]",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.tfmot",
        "documentation": {}
    },
    {
        "label": "optimzation_methods",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.tfmot",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.tfmot",
        "peekOfCode": "optimzation_methods = {\n    'prune': tfmot.sparsity.keras.prune_low_magnitude,\n    'quantize': quantize\n}\ndef set_config(configs):\n  for key in configs:\n    if key == 'prune':\n      optimzation_methods[key] = functools.partial(\n          tfmot.sparsity.keras.prune_low_magnitude, **configs[key])\n    if key == 'quantize':",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.tfmot",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "peekOfCode": "def define_flags():\n  \"\"\"Define the flags.\"\"\"\n  # Cloud TPU Cluster Resolvers\n  flags.DEFINE_string(\n      'tpu',\n      default=None,\n      help='The Cloud TPU to use for training. This should be either the name '\n      'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 '\n      'url.')\n  flags.DEFINE_string(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "documentation": {}
    },
    {
        "label": "setup_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "peekOfCode": "def setup_model(model, config):\n  \"\"\"Build and compile model.\"\"\"\n  model.build((None, *config.image_size, 3))\n  model.compile(\n      steps_per_execution=config.steps_per_execution,\n      optimizer=train_lib.get_optimizer(config.as_dict()),\n      loss={\n          train_lib.BoxLoss.__name__:\n              train_lib.BoxLoss(\n                  config.delta, reduction=tf.keras.losses.Reduction.NONE),",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "documentation": {}
    },
    {
        "label": "init_experimental",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "peekOfCode": "def init_experimental(config):\n  \"\"\"Serialize train config to model directory.\"\"\"\n  tf.io.gfile.makedirs(config.model_dir)\n  config_file = os.path.join(config.model_dir, 'config.yaml')\n  if not tf.io.gfile.exists(config_file):\n    tf.io.gfile.GFile(config_file, 'w').write(str(config))\ndef main(_):\n  # Parse and override hparams\n  config = hparams_config.get_detection_config(FLAGS.model_name)\n  config.override(FLAGS.hparams)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "peekOfCode": "def main(_):\n  # Parse and override hparams\n  config = hparams_config.get_detection_config(FLAGS.model_name)\n  config.override(FLAGS.hparams)\n  if FLAGS.num_epochs:  # NOTE: remove this flag after updating all docs.\n    config.num_epochs = FLAGS.num_epochs\n  # Parse image size in case it is in string format.\n  config.image_size = utils.parse_image_size(config.image_size)\n  if FLAGS.use_xla and FLAGS.strategy != 'tpu':\n    tf.config.optimizer.set_jit(True)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef define_flags():\n  \"\"\"Define the flags.\"\"\"\n  # Cloud TPU Cluster Resolvers\n  flags.DEFINE_string(\n      'tpu',\n      default=None,\n      help='The Cloud TPU to use for training. This should be either the name '\n      'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 '\n      'url.')",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train",
        "documentation": {}
    },
    {
        "label": "UpdatePruningStep",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class UpdatePruningStep(tf.keras.callbacks.Callback):\n  \"\"\"Keras callback which updates pruning wrappers with the optimizer step.\n  This callback must be used when training a model which needs to be pruned. Not\n  doing so will throw an error.\n  Example:\n  ```python\n  model.fit(x, y,\n      callbacks=[UpdatePruningStep()])\n  ```\n  \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "PruningSummaries",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class PruningSummaries(tf.keras.callbacks.TensorBoard):\n  \"\"\"A Keras callback for adding pruning summaries to tensorboard.\n  Logs the sparsity(%) and threshold at a given iteration step.\n  \"\"\"\n  def __init__(self, log_dir, update_freq='epoch', **kwargs):\n    if not isinstance(log_dir, str) or not log_dir:\n      raise ValueError(\n          '`log_dir` must be a non-empty string. You passed `log_dir`='\n          '{input}.'.format(input=log_dir))\n    super().__init__(log_dir=log_dir, update_freq=update_freq, **kwargs)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "StepwiseLrSchedule",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class StepwiseLrSchedule(tf.optimizers.schedules.LearningRateSchedule):\n  \"\"\"Stepwise learning rate schedule.\"\"\"\n  def __init__(self, adjusted_lr: float, lr_warmup_init: float,\n               lr_warmup_step: int, first_lr_drop_step: int,\n               second_lr_drop_step: int):\n    \"\"\"Build a StepwiseLrSchedule.\n    Args:\n      adjusted_lr: `float`, The initial learning rate.\n      lr_warmup_init: `float`, The warm up learning rate.\n      lr_warmup_step: `int`, The warm up step.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "CosineLrSchedule",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class CosineLrSchedule(tf.optimizers.schedules.LearningRateSchedule):\n  \"\"\"Cosine learning rate schedule.\"\"\"\n  def __init__(self, adjusted_lr: float, lr_warmup_init: float,\n               lr_warmup_step: int, total_steps: int):\n    \"\"\"Build a CosineLrSchedule.\n    Args:\n      adjusted_lr: `float`, The initial learning rate.\n      lr_warmup_init: `float`, The warm up learning rate.\n      lr_warmup_step: `int`, The warm up step.\n      total_steps: `int`, Total train steps.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "PolynomialLrSchedule",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class PolynomialLrSchedule(tf.optimizers.schedules.LearningRateSchedule):\n  \"\"\"Polynomial learning rate schedule.\"\"\"\n  def __init__(self, adjusted_lr: float, lr_warmup_init: float,\n               lr_warmup_step: int, power: float, total_steps: int):\n    \"\"\"Build a PolynomialLrSchedule.\n    Args:\n      adjusted_lr: `float`, The initial learning rate.\n      lr_warmup_init: `float`, The warm up learning rate.\n      lr_warmup_step: `int`, The warm up step.\n      power: `float`, power.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "COCOCallback",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class COCOCallback(tf.keras.callbacks.Callback):\n  \"\"\"A utility for COCO eval callback.\"\"\"\n  def __init__(self, test_dataset, update_freq=None):\n    super().__init__()\n    self.test_dataset = test_dataset\n    self.update_freq = update_freq\n  def set_model(self, model: tf.keras.Model):\n    self.model = model\n    config = model.config\n    self.config = config",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "DisplayCallback",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class DisplayCallback(tf.keras.callbacks.Callback):\n  \"\"\"Display inference result callback.\"\"\"\n  def __init__(self, sample_image, output_dir, update_freq=None):\n    super().__init__()\n    image_file = tf.io.read_file(sample_image)\n    self.sample_image = tf.expand_dims(\n        tf.image.decode_jpeg(image_file, channels=3), axis=0)\n    self.update_freq = update_freq\n    self.output_dir = output_dir\n  def set_model(self, model: tf.keras.Model):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "AdversarialLoss",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class AdversarialLoss(tf.keras.losses.Loss):\n  \"\"\"Adversarial keras loss wrapper.\"\"\"\n  # TODO(fsx950223): WIP\n  def __init__(self, adv_config, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.adv_config = adv_config\n    self.model = None\n    self.loss_fn = None\n    self.tape = None\n    self.built = False",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class FocalLoss(tf.keras.losses.Loss):\n  \"\"\"Compute the focal loss between `logits` and the golden `target` values.\n  Focal loss = -(1-pt)^gamma * log(pt)\n  where pt is the probability of being classified to the true class.\n  \"\"\"\n  def __init__(self, alpha, gamma, label_smoothing=0.0, **kwargs):\n    \"\"\"Initialize focal loss.\n    Args:\n      alpha: A float32 scalar multiplying alpha to the loss from positive\n        examples and (1-alpha) to the loss from negative examples.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "BoxLoss",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class BoxLoss(tf.keras.losses.Loss):\n  \"\"\"L2 box regression loss.\"\"\"\n  def __init__(self, delta=0.1, **kwargs):\n    \"\"\"Initialize box loss.\n    Args:\n      delta: `float`, the point where the huber loss function changes from a\n        quadratic to linear. It is typically around the mean value of regression\n        target. For instances, the regression targets of 512x512 input with 6\n        anchors on P3-P7 pyramid is about [0.1, 0.1, 0.2, 0.2].\n      **kwargs: other params.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "BoxIouLoss",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class BoxIouLoss(tf.keras.losses.Loss):\n  \"\"\"Box iou loss.\"\"\"\n  def __init__(self, iou_loss_type, min_level, max_level, num_scales,\n               aspect_ratios, anchor_scale, image_size, **kwargs):\n    super().__init__(**kwargs)\n    self.iou_loss_type = iou_loss_type\n    self.input_anchors = anchors.Anchors(min_level, max_level, num_scales,\n                                         aspect_ratios, anchor_scale,\n                                         image_size)\n  @tf.autograph.experimental.do_not_convert",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "EfficientDetNetTrain",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class EfficientDetNetTrain(efficientdet_keras.EfficientDetNet):\n  \"\"\"A customized trainer for EfficientDet.\n  see https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\n  \"\"\"\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    log_dir = os.path.join(self.config.model_dir, 'train_images')\n    self.summary_writer = tf.summary.create_file_writer(log_dir)\n  def _freeze_vars(self):\n    if self.config.var_freeze_expr:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "EfficientDetNetTrainHub",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "class EfficientDetNetTrainHub(EfficientDetNetTrain):\n  \"\"\"EfficientDetNetTrain for Hub module.\"\"\"\n  def __init__(self, config, hub_module_url, name=''):\n    super(efficientdet_keras.EfficientDetNet, self).__init__(name=name)\n    self.config = config\n    self.hub_module_url = hub_module_url\n    self.base_model = hub.KerasLayer(hub_module_url, trainable=True)\n    # class/box output prediction network.\n    num_anchors = len(config.aspect_ratios) * config.num_scales\n    conv2d_layer = efficientdet_keras.ClassNet.conv2d_layer(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "update_learning_rate_schedule_parameters",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "def update_learning_rate_schedule_parameters(params):\n  \"\"\"Updates params that are related to the learning rate schedule.\"\"\"\n  batch_size = params['batch_size']\n  # Learning rate is proportional to the batch size\n  params['adjusted_learning_rate'] = (params['learning_rate'] * batch_size / 64)\n  steps_per_epoch = params['steps_per_epoch']\n  params['lr_warmup_step'] = int(params['lr_warmup_epoch'] * steps_per_epoch)\n  params['first_lr_drop_step'] = int(params['first_lr_drop_epoch'] *\n                                     steps_per_epoch)\n  params['second_lr_drop_step'] = int(params['second_lr_drop_epoch'] *",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "learning_rate_schedule",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "def learning_rate_schedule(params):\n  \"\"\"Learning rate schedule based on global step.\"\"\"\n  update_learning_rate_schedule_parameters(params)\n  lr_decay_method = params['lr_decay_method']\n  if lr_decay_method == 'stepwise':\n    return StepwiseLrSchedule(params['adjusted_learning_rate'],\n                              params['lr_warmup_init'],\n                              params['lr_warmup_step'],\n                              params['first_lr_drop_step'],\n                              params['second_lr_drop_step'])",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "get_optimizer",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "def get_optimizer(params):\n  \"\"\"Get optimizer.\"\"\"\n  learning_rate = learning_rate_schedule(params)\n  momentum = params['momentum']\n  if params['optimizer'].lower() == 'sgd':\n    logging.info('Use SGD optimizer')\n    optimizer = tf.keras.optimizers.SGD(learning_rate, momentum=momentum)\n  elif params['optimizer'].lower() == 'adam':\n    logging.info('Use Adam optimizer')\n    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=momentum)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "get_callbacks",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "peekOfCode": "def get_callbacks(params, val_dataset=None):\n  \"\"\"Get callbacks for given params.\"\"\"\n  if params['moving_average_decay']:\n    avg_callback = AverageModelCheckpoint(\n        filepath=os.path.join(params['model_dir'], 'emackpt-{epoch:d}'),\n        verbose=params['verbose'],\n        save_freq=params['save_freq'],\n        save_weights_only=True,\n        update_weights=False)\n    callbacks = [avg_callback]",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.train_lib",
        "documentation": {}
    },
    {
        "label": "build_batch_norm",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "peekOfCode": "def build_batch_norm(is_training_bn: bool,\n                     beta_initializer: Text = 'zeros',\n                     gamma_initializer: Text = 'ones',\n                     data_format: Text = 'channels_last',\n                     momentum: float = 0.99,\n                     epsilon: float = 1e-3,\n                     strategy: Optional[Text] = None,\n                     name: Text = 'tpu_batch_normalization'):\n  \"\"\"Build a batch normalization layer.\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "documentation": {}
    },
    {
        "label": "get_ema_vars",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "peekOfCode": "def get_ema_vars(model):\n  \"\"\"Get all exponential moving average (ema) variables.\"\"\"\n  ema_vars = model.trainable_weights\n  for v in model.weights:\n    # We maintain mva for batch norm moving mean and variance as well.\n    if 'moving_mean' in v.name or 'moving_variance' in v.name:\n      ema_vars.append(v)\n  ema_vars_dict = dict()\n  # Remove duplicate vars\n  for var in ema_vars:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "documentation": {}
    },
    {
        "label": "average_name",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "peekOfCode": "def average_name(ema, var):\n  \"\"\"Returns the name of the `Variable` holding the average for `var`.\n  A hacker for tf2.\n  Args:\n    ema: A `ExponentialMovingAverage` object.\n    var: A `Variable` object.\n  Returns:\n    A string: The name of the variable that will be used or was used\n    by the `ExponentialMovingAverage class` to hold the moving average of `var`.\n  \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "documentation": {}
    },
    {
        "label": "load_from_hub_checkpoint",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "peekOfCode": "def load_from_hub_checkpoint(model, ckpt_path_or_file):\n  \"\"\"Loads EfficientDetNet weights from EfficientDetNetTrainHub checkpoint.\"\"\"\n  def _get_cpt_var_name(var_name):\n    for name_prefix, hub_name_prefix in HUB_CPT_NAME.items():\n      if var_name.startswith(name_prefix):\n        cpt_var_name = var_name[len(name_prefix):]  # remove the name_prefix\n        cpt_var_name = cpt_var_name.replace('/', '.S')\n        cpt_var_name = hub_name_prefix + '/' + cpt_var_name\n        if name_prefix:\n          cpt_var_name = cpt_var_name.replace(':0', '')",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "documentation": {}
    },
    {
        "label": "restore_ckpt",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "peekOfCode": "def restore_ckpt(model,\n                 ckpt_path_or_file,\n                 ema_decay=0.9998,\n                 skip_mismatch=True,\n                 exclude_layers=None):\n  \"\"\"Restore variables from a given checkpoint.\n  Args:\n    model: the keras model to be restored.\n    ckpt_path_or_file: the path or file for checkpoint.\n    ema_decay: ema decay rate. If None or zero or negative value, disable ema.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "documentation": {}
    },
    {
        "label": "fp16_to_fp32_nested",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "peekOfCode": "def fp16_to_fp32_nested(input_nested):\n  \"\"\"Convert fp16 tensors in a nested structure to fp32.\n  Args:\n    input_nested: A Python dict, values being Tensor or Python list/tuple of\n      Tensor or Non-Tensor.\n  Returns:\n    A Python dict with the same structure as `tensor_dict`,\n    with all bfloat16 tensors converted to float32.\n  \"\"\"\n  if isinstance(input_nested, tf.Tensor):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "documentation": {}
    },
    {
        "label": "HUB_CPT_NAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "peekOfCode": "HUB_CPT_NAME = collections.OrderedDict([('class_net/class-predict/', 'classes'),\n                                        ('box_net/box-predict/', 'boxes'),\n                                        ('', 'base_model')])\ndef build_batch_norm(is_training_bn: bool,\n                     beta_initializer: Text = 'zeros',\n                     gamma_initializer: Text = 'ones',\n                     data_format: Text = 'channels_last',\n                     momentum: float = 0.99,\n                     epsilon: float = 1e-3,\n                     strategy: Optional[Text] = None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.util_keras",
        "documentation": {}
    },
    {
        "label": "vectorized_iou",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "peekOfCode": "def vectorized_iou(clusters, detection):\n  \"\"\"Calculates the ious for box with each element of clusters.\"\"\"\n  x11, y11, x12, y12 = tf.split(clusters[:, 1:5], 4, axis=1)\n  x21, y21, x22, y22 = tf.split(detection[1:5], 4)\n  xa = tf.maximum(x11, x21)\n  ya = tf.maximum(y11, y21)\n  xb = tf.minimum(x12, x22)\n  yb = tf.minimum(y12, y22)\n  inter_area = tf.maximum((xb - xa), 0) * tf.maximum((yb - ya), 0)\n  boxa_area = (x12 - x11) * (y12 - y11)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "documentation": {}
    },
    {
        "label": "find_matching_cluster",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "peekOfCode": "def find_matching_cluster(clusters, detection):\n  \"\"\"Returns the index of the highest iou matching cluster for detection.\"\"\"\n  if not clusters:\n    return -1\n  ious = vectorized_iou(tf.stack(clusters), detection)\n  ious = tf.reshape(ious, [len(clusters)])\n  if tf.math.reduce_max(ious) < 0.55:\n    # returns -1 if no iou is higher than 0.55.\n    return -1\n  return tf.argmax(ious)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "documentation": {}
    },
    {
        "label": "weighted_average",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "peekOfCode": "def weighted_average(samples, weights):\n  return tf.math.reduce_sum(samples * weights) / tf.math.reduce_sum(weights)\ndef average_detections(detections, num_models):\n  \"\"\"Takes a list of detections and returns the average, both in box co-ordinates and confidence.\"\"\"\n  num_detections = len(detections)\n  detections = tf.stack(detections)\n  return [\n      detections[0][0],\n      weighted_average(detections[:, 1], detections[:, 5]),\n      weighted_average(detections[:, 2], detections[:, 5]),",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "documentation": {}
    },
    {
        "label": "average_detections",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "peekOfCode": "def average_detections(detections, num_models):\n  \"\"\"Takes a list of detections and returns the average, both in box co-ordinates and confidence.\"\"\"\n  num_detections = len(detections)\n  detections = tf.stack(detections)\n  return [\n      detections[0][0],\n      weighted_average(detections[:, 1], detections[:, 5]),\n      weighted_average(detections[:, 2], detections[:, 5]),\n      weighted_average(detections[:, 3], detections[:, 5]),\n      weighted_average(detections[:, 4], detections[:, 5]),",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "documentation": {}
    },
    {
        "label": "ensemble_detections",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "peekOfCode": "def ensemble_detections(params, detections, num_models):\n  \"\"\"Ensembles a group of detections by clustering the detections and returning the average of the clusters.\"\"\"\n  all_clusters = []\n  for cid in range(params['num_classes']):\n    indices = tf.where(tf.equal(detections[:, 6], cid))\n    if indices.shape[0] == 0:\n      continue\n    class_detections = tf.gather_nd(detections, indices)\n    clusters = []\n    cluster_averages = []",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.keras.wbf",
        "documentation": {}
    },
    {
        "label": "ArgMaxMatcher",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.argmax_matcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.argmax_matcher",
        "peekOfCode": "class ArgMaxMatcher(matcher.Matcher):\n  \"\"\"Matcher based on highest value.\n  This class computes matches from a similarity matrix. Each column is matched\n  to a single row.\n  To support object detection target assignment this class enables setting both\n  matched_threshold (upper threshold) and unmatched_threshold (lower thresholds)\n  defining three categories of similarity which define whether examples are\n  positive, negative, or ignored:\n  (1) similarity >= matched_threshold: Highest similarity. Matched/Positive!\n  (2) matched_threshold > similarity >= unmatched_threshold: Medium similarity.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.argmax_matcher",
        "documentation": {}
    },
    {
        "label": "BoxCoder",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "peekOfCode": "class BoxCoder(object):\n  \"\"\"Abstract base class for box coder.\"\"\"\n  __metaclass__ = ABCMeta\n  @abstractproperty\n  def code_size(self):\n    \"\"\"Return the size of each code.\n    This number is a constant and should agree with the output of the `encode`\n    op (e.g. if rel_codes is the output of self.encode(...), then it should have\n    shape [N, code_size()]).  This abstractproperty should be overridden by\n    implementations.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "documentation": {}
    },
    {
        "label": "batch_decode",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "peekOfCode": "def batch_decode(encoded_boxes, box_coder, anchors):\n  \"\"\"Decode a batch of encoded boxes.\n  This op takes a batch of encoded bounding boxes and transforms\n  them to a batch of bounding boxes specified by their corners in\n  the order of [y_min, x_min, y_max, x_max].\n  Args:\n    encoded_boxes: a float32 tensor of shape [batch_size, num_anchors,\n      code_size] representing the location of the objects.\n    box_coder: a BoxCoder object.\n    anchors: a BoxList of anchors used to encode `encoded_boxes`.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "documentation": {}
    },
    {
        "label": "FASTER_RCNN",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "peekOfCode": "FASTER_RCNN = 'faster_rcnn'\nKEYPOINT = 'keypoint'\nMEAN_STDDEV = 'mean_stddev'\nSQUARE = 'square'\nclass BoxCoder(object):\n  \"\"\"Abstract base class for box coder.\"\"\"\n  __metaclass__ = ABCMeta\n  @abstractproperty\n  def code_size(self):\n    \"\"\"Return the size of each code.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "documentation": {}
    },
    {
        "label": "KEYPOINT",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "peekOfCode": "KEYPOINT = 'keypoint'\nMEAN_STDDEV = 'mean_stddev'\nSQUARE = 'square'\nclass BoxCoder(object):\n  \"\"\"Abstract base class for box coder.\"\"\"\n  __metaclass__ = ABCMeta\n  @abstractproperty\n  def code_size(self):\n    \"\"\"Return the size of each code.\n    This number is a constant and should agree with the output of the `encode`",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "documentation": {}
    },
    {
        "label": "MEAN_STDDEV",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "peekOfCode": "MEAN_STDDEV = 'mean_stddev'\nSQUARE = 'square'\nclass BoxCoder(object):\n  \"\"\"Abstract base class for box coder.\"\"\"\n  __metaclass__ = ABCMeta\n  @abstractproperty\n  def code_size(self):\n    \"\"\"Return the size of each code.\n    This number is a constant and should agree with the output of the `encode`\n    op (e.g. if rel_codes is the output of self.encode(...), then it should have",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "documentation": {}
    },
    {
        "label": "SQUARE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "peekOfCode": "SQUARE = 'square'\nclass BoxCoder(object):\n  \"\"\"Abstract base class for box coder.\"\"\"\n  __metaclass__ = ABCMeta\n  @abstractproperty\n  def code_size(self):\n    \"\"\"Return the size of each code.\n    This number is a constant and should agree with the output of the `encode`\n    op (e.g. if rel_codes is the output of self.encode(...), then it should have\n    shape [N, code_size()]).  This abstractproperty should be overridden by",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_coder",
        "documentation": {}
    },
    {
        "label": "BoxList",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_list",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_list",
        "peekOfCode": "class BoxList(object):\n  \"\"\"Box collection.\"\"\"\n  def __init__(self, boxes):\n    \"\"\"Constructs box collection.\n    Args:\n      boxes: a tensor of shape [N, 4] representing box corners\n    Raises:\n      ValueError: if invalid dimensions for bbox data or if bbox data is not in\n          float32 format.\n    \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.box_list",
        "documentation": {}
    },
    {
        "label": "FasterRcnnBoxCoder",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.faster_rcnn_box_coder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.faster_rcnn_box_coder",
        "peekOfCode": "class FasterRcnnBoxCoder(box_coder.BoxCoder):\n  \"\"\"Faster RCNN box coder.\"\"\"\n  def __init__(self, scale_factors=None):\n    \"\"\"Constructor for FasterRcnnBoxCoder.\n    Args:\n      scale_factors: List of 4 positive scalars to scale ty, tx, th and tw.\n        If set to None, does not perform scaling. For Faster RCNN,\n        the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0].\n    \"\"\"\n    if scale_factors:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.faster_rcnn_box_coder",
        "documentation": {}
    },
    {
        "label": "EPSILON",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.faster_rcnn_box_coder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.faster_rcnn_box_coder",
        "peekOfCode": "EPSILON = 1e-8\nclass FasterRcnnBoxCoder(box_coder.BoxCoder):\n  \"\"\"Faster RCNN box coder.\"\"\"\n  def __init__(self, scale_factors=None):\n    \"\"\"Constructor for FasterRcnnBoxCoder.\n    Args:\n      scale_factors: List of 4 positive scalars to scale ty, tx, th and tw.\n        If set to None, does not perform scaling. For Faster RCNN,\n        the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0].\n    \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.faster_rcnn_box_coder",
        "documentation": {}
    },
    {
        "label": "Match",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.matcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.matcher",
        "peekOfCode": "class Match(object):\n  \"\"\"Class to store results from the matcher.\n  This class is used to store the results from the matcher. It provides\n  convenient methods to query the matching results.\n  \"\"\"\n  def __init__(self, match_results):\n    \"\"\"Constructs a Match object.\n    Args:\n      match_results: Integer tensor of shape [N] with (1) match_results[i]>=0,\n        meaning that column i is matched with row match_results[i].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.matcher",
        "documentation": {}
    },
    {
        "label": "Matcher",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.matcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.matcher",
        "peekOfCode": "class Matcher(object):\n  \"\"\"Abstract base class for matcher.\n  \"\"\"\n  __metaclass__ = abc.ABCMeta\n  def match(self, similarity_matrix, scope=None, **params):\n    \"\"\"Computes matches among row and column indices and returns the result.\n    Computes matches among the row and column indices based on the similarity\n    matrix and optional arguments.\n    Args:\n      similarity_matrix: Float tensor of shape [N, M] with pairwise similarity",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.matcher",
        "documentation": {}
    },
    {
        "label": "keypoint_flip_horizontal",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "peekOfCode": "def keypoint_flip_horizontal(keypoints, flip_point, flip_permutation,\n                             scope=None):\n  \"\"\"Flips the keypoints horizontally around the flip_point.\n  This operation flips the x coordinate for each keypoint around the flip_point\n  and also permutes the keypoints in a manner specified by flip_permutation.\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    flip_point:  (float) scalar tensor representing the x coordinate to flip the\n      keypoints around.\n    flip_permutation: rank 1 int32 tensor containing the keypoint flip",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "documentation": {}
    },
    {
        "label": "random_horizontal_flip",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "peekOfCode": "def random_horizontal_flip(image,\n                           boxes=None,\n                           masks=None,\n                           keypoints=None,\n                           keypoint_flip_permutation=None,\n                           seed=None):\n  \"\"\"Randomly flips the image and detections horizontally.\n  The probability of flipping the image is 50%.\n  Args:\n    image: rank 3 float32 tensor with shape [height, width, channels].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "documentation": {}
    },
    {
        "label": "resize_to_range",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "peekOfCode": "def resize_to_range(image,\n                    masks=None,\n                    min_dimension=None,\n                    max_dimension=None,\n                    method=tf.image.ResizeMethod.BILINEAR,\n                    align_corners=False,\n                    pad_to_max_dimension=False):\n  \"\"\"Resizes an image so its dimensions are within the provided value.\n  The output size can be described by two cases:\n  1. If the image can be rescaled so its minimum dimension is equal to the",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "documentation": {}
    },
    {
        "label": "box_list_scale",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "peekOfCode": "def box_list_scale(boxlist, y_scale, x_scale, scope=None):\n  \"\"\"scale box coordinates in x and y dimensions.\n  Args:\n    boxlist: BoxList holding N boxes\n    y_scale: (float) scalar tensor\n    x_scale: (float) scalar tensor\n    scope: name scope.\n  Returns:\n    boxlist: BoxList holding N boxes\n  \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "documentation": {}
    },
    {
        "label": "keypoint_scale",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "peekOfCode": "def keypoint_scale(keypoints, y_scale, x_scale, scope=None):\n  \"\"\"Scales keypoint coordinates in x and y dimensions.\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    y_scale: (float) scalar tensor\n    x_scale: (float) scalar tensor\n    scope: name scope.\n  Returns:\n    new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n  \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "documentation": {}
    },
    {
        "label": "scale_boxes_to_pixel_coordinates",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "peekOfCode": "def scale_boxes_to_pixel_coordinates(image, boxes, keypoints=None):\n  \"\"\"Scales boxes from normalized to pixel coordinates.\n  Args:\n    image: A 3D float32 tensor of shape [height, width, channels].\n    boxes: A 2D float32 tensor of shape [num_boxes, 4] containing the bounding\n      boxes in normalized coordinates. Each row is of the form\n      [ymin, xmin, ymax, xmax].\n    keypoints: (optional) rank 3 float32 tensor with shape\n      [num_instances, num_keypoints, 2]. The keypoints are in y-x normalized\n      coordinates.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.preprocessor",
        "documentation": {}
    },
    {
        "label": "RegionSimilarityCalculator",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "peekOfCode": "class RegionSimilarityCalculator(object):\n  \"\"\"Abstract base class for region similarity calculator.\"\"\"\n  __metaclass__ = ABCMeta\n  def compare(self, boxlist1, boxlist2, scope=None):\n    \"\"\"Computes matrix of pairwise similarity between BoxLists.\n    This op (to be overridden) computes a measure of pairwise similarity between\n    the boxes in the given BoxLists. Higher values indicate more similarity.\n    Note that this method simply measures similarity and does not explicitly\n    perform a matching.\n    Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "documentation": {}
    },
    {
        "label": "IouSimilarity",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "peekOfCode": "class IouSimilarity(RegionSimilarityCalculator):\n  \"\"\"Class to compute similarity based on Intersection over Union (IOU) metric.\n  This class computes pairwise similarity between two BoxLists based on IOU.\n  \"\"\"\n  def _compare(self, boxlist1, boxlist2):\n    \"\"\"Compute pairwise IOU similarity between the two BoxLists.\n    Args:\n      boxlist1: BoxList holding N boxes.\n      boxlist2: BoxList holding M boxes.\n    Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "peekOfCode": "def area(boxlist, scope=None):\n  \"\"\"Computes area of boxes.\n  Args:\n    boxlist: BoxList holding N boxes\n    scope: name scope.\n  Returns:\n    a tensor with shape [N] representing box areas.\n  \"\"\"\n  with tf.name_scope(scope, 'Area'):\n    y_min, x_min, y_max, x_max = tf.split(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "documentation": {}
    },
    {
        "label": "intersection",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "peekOfCode": "def intersection(boxlist1, boxlist2, scope=None):\n  \"\"\"Compute pairwise intersection areas between boxes.\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n    scope: name scope.\n  Returns:\n    a tensor with shape [N, M] representing pairwise intersections\n  \"\"\"\n  with tf.name_scope(scope, 'Intersection'):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "documentation": {}
    },
    {
        "label": "iou",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "peekOfCode": "def iou(boxlist1, boxlist2, scope=None):\n  \"\"\"Computes pairwise intersection-over-union between box collections.\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n    scope: name scope.\n  Returns:\n    a tensor with shape [N, M] representing pairwise iou scores.\n  \"\"\"\n  with tf.name_scope(scope, 'IOU'):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.region_similarity_calculator",
        "documentation": {}
    },
    {
        "label": "assert_shape_equal",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.shape_utils",
        "peekOfCode": "def assert_shape_equal(shape_a, shape_b):\n  \"\"\"Asserts that shape_a and shape_b are equal.\n  If the shapes are static, raises a ValueError when the shapes\n  mismatch.\n  If the shapes are dynamic, raises a tf InvalidArgumentError when the shapes\n  mismatch.\n  Args:\n    shape_a: a list containing shape of the first tensor.\n    shape_b: a list containing shape of the second tensor.\n  Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.shape_utils",
        "documentation": {}
    },
    {
        "label": "combined_static_and_dynamic_shape",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.shape_utils",
        "peekOfCode": "def combined_static_and_dynamic_shape(tensor):\n  \"\"\"Returns a list containing static and dynamic values for the dimensions.\n  Returns a list of static and dynamic values for shape dimensions. This is\n  useful to preserve static shapes when available in reshape operation.\n  Args:\n    tensor: A tensor of any type.\n  Returns:\n    A list of size tensor.shape.ndims containing integers or a scalar tensor.\n  \"\"\"\n  static_tensor_shape = tensor.shape.as_list()",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.shape_utils",
        "documentation": {}
    },
    {
        "label": "TargetAssigner",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.target_assigner",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.target_assigner",
        "peekOfCode": "class TargetAssigner(object):\n  \"\"\"Target assigner to compute classification and regression targets.\"\"\"\n  def __init__(self, similarity_calc, matcher, box_coder,\n               negative_class_weight=1.0, unmatched_cls_target=None):\n    \"\"\"Construct Object Detection Target Assigner.\n    Args:\n      similarity_calc: a RegionSimilarityCalculator\n      matcher: Matcher used to match groundtruth to anchors.\n      box_coder: BoxCoder used to encode matching groundtruth boxes with\n        respect to anchors.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.target_assigner",
        "documentation": {}
    },
    {
        "label": "KEYPOINTS_FIELD_NAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.target_assigner",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.target_assigner",
        "peekOfCode": "KEYPOINTS_FIELD_NAME = 'keypoints'\nclass TargetAssigner(object):\n  \"\"\"Target assigner to compute classification and regression targets.\"\"\"\n  def __init__(self, similarity_calc, matcher, box_coder,\n               negative_class_weight=1.0, unmatched_cls_target=None):\n    \"\"\"Construct Object Detection Target Assigner.\n    Args:\n      similarity_calc: a RegionSimilarityCalculator\n      matcher: Matcher used to match groundtruth to anchors.\n      box_coder: BoxCoder used to encode matching groundtruth boxes with",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.target_assigner",
        "documentation": {}
    },
    {
        "label": "TfExampleDecoder",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.tf_example_decoder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.tf_example_decoder",
        "peekOfCode": "class TfExampleDecoder(object):\n  \"\"\"Tensorflow Example proto decoder.\"\"\"\n  def __init__(self, include_mask=False, regenerate_source_id=False):\n    self._include_mask = include_mask\n    self._regenerate_source_id = regenerate_source_id\n    self._keys_to_features = {\n        'image/encoded': tf.FixedLenFeature((), tf.string),\n        'image/source_id': tf.FixedLenFeature((), tf.string, ''),\n        'image/height': tf.FixedLenFeature((), tf.int64, -1),\n        'image/width': tf.FixedLenFeature((), tf.int64, -1),",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.object_detection.tf_example_decoder",
        "documentation": {}
    },
    {
        "label": "pad_tensor",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def pad_tensor(t, length):\n  \"\"\"Pads the input tensor with 0s along the first dimension up to the length.\n  Args:\n    t: the input tensor, assuming the rank is at least 1.\n    length: a tensor of shape [1]  or an integer, indicating the first dimension\n      of the input tensor t after padding, assuming length <= t.shape[0].\n  Returns:\n    padded_t: the padded tensor, whose first dimension is length. If the length\n      is an integer, the first dimension of padded_t is set to length\n      statically.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "clip_tensor",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def clip_tensor(t, length):\n  \"\"\"Clips the input tensor along the first dimension up to the length.\n  Args:\n    t: the input tensor, assuming the rank is at least 1.\n    length: a tensor of shape [1]  or an integer, indicating the first dimension\n      of the input tensor t after clipping, assuming length <= t.shape[0].\n  Returns:\n    clipped_t: the clipped tensor, whose first dimension is length. If the\n      length is an integer, the first dimension of clipped_t is set to length\n      statically.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "pad_or_clip_tensor",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def pad_or_clip_tensor(t, length):\n  \"\"\"Pad or clip the input tensor along the first dimension.\n  Args:\n    t: the input tensor, assuming the rank is at least 1.\n    length: a tensor of shape [1]  or an integer, indicating the first dimension\n      of the input tensor t after processing.\n  Returns:\n    processed_t: the processed tensor, whose first dimension is length. If the\n      length is an integer, the first dimension of the processed tensor is set\n      to length statically.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "pad_or_clip_nd",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def pad_or_clip_nd(tensor, output_shape):\n  \"\"\"Pad or Clip given tensor to the output shape.\n  Args:\n    tensor: Input tensor to pad or clip.\n    output_shape: A list of integers / scalar tensors (or None for dynamic dim)\n      representing the size to pad or clip each dimension of the input tensor.\n  Returns:\n    Input tensor padded and clipped to the output shape.\n  \"\"\"\n  tensor_shape = tf.shape(tensor)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "combined_static_and_dynamic_shape",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def combined_static_and_dynamic_shape(tensor):\n  \"\"\"Returns a list containing static and dynamic values for the dimensions.\n  Returns a list of static and dynamic values for shape dimensions. This is\n  useful to preserve static shapes when available in reshape operation.\n  Args:\n    tensor: A tensor of any type.\n  Returns:\n    A list of size tensor.shape.ndims containing integers or a scalar tensor.\n  \"\"\"\n  static_tensor_shape = tensor.shape.as_list()",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "static_or_dynamic_map_fn",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def static_or_dynamic_map_fn(fn, elems, dtype=None,\n                             parallel_iterations=32, back_prop=True):\n  \"\"\"Runs map_fn as a (static) for loop when possible.\n  This function rewrites the map_fn as an explicit unstack input -> for loop\n  over function calls -> stack result combination.  This allows our graphs to\n  be acyclic when the batch size is static.\n  For comparison, see https://www.tensorflow.org/api_docs/python/tf/map_fn.\n  Note that `static_or_dynamic_map_fn` currently is not *fully* interchangeable\n  with the default tf.map_fn function as it does not accept nested inputs (only\n  Tensors or lists of Tensors).  Likewise, the output of `fn` can only be a",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "check_min_image_dim",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def check_min_image_dim(min_dim, image_tensor):\n  \"\"\"Checks that the image width/height are greater than some number.\n  This function is used to check that the width and height of an image are above\n  a certain value. If the image shape is static, this function will perform the\n  check at graph construction time. Otherwise, if the image shape varies, an\n  Assertion control dependency will be added to the graph.\n  Args:\n    min_dim: The minimum number of pixels along the width and height of the\n             image.\n    image_tensor: The image tensor to check size for.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "assert_shape_equal",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def assert_shape_equal(shape_a, shape_b):\n  \"\"\"Asserts that shape_a and shape_b are equal.\n  If the shapes are static, raises a ValueError when the shapes\n  mismatch.\n  If the shapes are dynamic, raises a tf InvalidArgumentError when the shapes\n  mismatch.\n  Args:\n    shape_a: a list containing shape of the first tensor.\n    shape_b: a list containing shape of the second tensor.\n  Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "assert_shape_equal_along_first_dimension",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def assert_shape_equal_along_first_dimension(shape_a, shape_b):\n  \"\"\"Asserts that shape_a and shape_b are the same along the 0th-dimension.\n  If the shapes are static, raises a ValueError when the shapes\n  mismatch.\n  If the shapes are dynamic, raises a tf InvalidArgumentError when the shapes\n  mismatch.\n  Args:\n    shape_a: a list containing shape of the first tensor.\n    shape_b: a list containing shape of the second tensor.\n  Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "assert_box_normalized",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def assert_box_normalized(boxes, maximum_normalized_coordinate=1.1):\n  \"\"\"Asserts the input box tensor is normalized.\n  Args:\n    boxes: a tensor of shape [N, 4] where N is the number of boxes.\n    maximum_normalized_coordinate: Maximum coordinate value to be considered\n      as normalized, default to 1.1.\n  Returns:\n    a tf.Assert op which fails when the input box tensor is not normalized.\n  Raises:\n    ValueError: When the input box tensor is not normalized.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "flatten_dimensions",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def flatten_dimensions(inputs, first, last):\n  \"\"\"Flattens `K-d` tensor along [first, last) dimensions.\n  Converts `inputs` with shape [D0, D1, ..., D(K-1)] into a tensor of shape\n  [D0, D1, ..., D(first) * D(first+1) * ... * D(last-1), D(last), ..., D(K-1)].\n  Example:\n  `inputs` is a tensor with initial shape [10, 5, 20, 20, 3].\n  new_tensor = flatten_dimensions(inputs, first=1, last=3)\n  new_tensor.shape -> [10, 100, 20, 3].\n  Args:\n    inputs: a tensor with shape [D0, D1, ..., D(K-1)].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "flatten_first_n_dimensions",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def flatten_first_n_dimensions(inputs, n):\n  \"\"\"Flattens `K-d` tensor along first n dimension to be a `(K-n+1)-d` tensor.\n  Converts `inputs` with shape [D0, D1, ..., D(K-1)] into a tensor of shape\n  [D0 * D1 * ... * D(n-1), D(n), ... D(K-1)].\n  Example:\n  `inputs` is a tensor with initial shape [10, 5, 20, 20, 3].\n  new_tensor = flatten_first_n_dimensions(inputs, 2)\n  new_tensor.shape -> [50, 20, 20, 3].\n  Args:\n    inputs: a tensor with shape [D0, D1, ..., D(K-1)].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "expand_first_dimension",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def expand_first_dimension(inputs, dims):\n  \"\"\"Expands `K-d` tensor along first dimension to be a `(K+n-1)-d` tensor.\n  Converts `inputs` with shape [D0, D1, ..., D(K-1)] into a tensor of shape\n  [dims[0], dims[1], ..., dims[-1], D1, ..., D(k-1)].\n  Example:\n  `inputs` is a tensor with shape [50, 20, 20, 3].\n  new_tensor = expand_first_dimension(inputs, [10, 5]).\n  new_tensor.shape -> [10, 5, 20, 20, 3].\n  Args:\n    inputs: a tensor with shape [D0, D1, ..., D(K-1)].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "resize_images_and_return_shapes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "peekOfCode": "def resize_images_and_return_shapes(inputs, image_resizer_fn):\n  \"\"\"Resizes images using the given function and returns their true shapes.\n  Args:\n    inputs: a float32 Tensor representing a batch of inputs of shape\n      [batch_size, height, width, channels].\n    image_resizer_fn: a function which takes in a single image and outputs\n      a resized image and its original shape.\n  Returns:\n    resized_inputs: The inputs resized according to image_resizer_fn.\n    true_image_shapes: A integer tensor of shape [batch_size, 3]",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.shape_utils",
        "documentation": {}
    },
    {
        "label": "InputDataFields",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "peekOfCode": "class InputDataFields(object):\n  \"\"\"Names for the input tensors.\n  Holds the standard data field names to use for identifying input tensors. This\n  should be used by the decoder to identify keys for the returned tensor_dict\n  containing input tensors. And it should be used by the model to identify the\n  tensors it needs.\n  Attributes:\n    image: image.\n    image_additional_channels: additional channels.\n    original_image: image in the original input size.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "documentation": {}
    },
    {
        "label": "DetectionResultFields",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "peekOfCode": "class DetectionResultFields(object):\n  \"\"\"Naming conventions for storing the output of the detector.\n  Attributes:\n    source_id: source of the original image.\n    key: unique key corresponding to image.\n    detection_boxes: coordinates of the detection boxes in the image.\n    detection_scores: detection scores for the detection boxes in the image.\n    detection_multiclass_scores: class score distribution (including background)\n      for detection boxes in the image including background class.\n    detection_classes: detection-level class labels.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "documentation": {}
    },
    {
        "label": "BoxListFields",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "peekOfCode": "class BoxListFields(object):\n  \"\"\"Naming conventions for BoxLists.\n  Attributes:\n    boxes: bounding box coordinates.\n    classes: classes per bounding box.\n    scores: scores per bounding box.\n    weights: sample weights per bounding box.\n    objectness: objectness score per bounding box.\n    masks: masks per bounding box.\n    boundaries: boundaries per bounding box.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "documentation": {}
    },
    {
        "label": "PredictionFields",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "peekOfCode": "class PredictionFields(object):\n  \"\"\"Naming conventions for standardized prediction outputs.\n  Attributes:\n    feature_maps: List of feature maps for prediction.\n    anchors: Generated anchors.\n    raw_detection_boxes: Decoded detection boxes without NMS.\n    raw_detection_feature_map_indices: Feature map indices from which each raw\n      detection box was produced.\n  \"\"\"\n  feature_maps = 'feature_maps'",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "documentation": {}
    },
    {
        "label": "TfExampleFields",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "peekOfCode": "class TfExampleFields(object):\n  \"\"\"TF-example proto feature names for object detection.\n  Holds the standard feature names to load from an Example proto for object\n  detection.\n  Attributes:\n    image_encoded: JPEG encoded string\n    image_format: image format, e.g. \"JPEG\"\n    filename: filename\n    channels: number of channels of image\n    colorspace: colorspace, e.g. \"RGB\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.standard_fields",
        "documentation": {}
    },
    {
        "label": "get_dim_as_int",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "peekOfCode": "def get_dim_as_int(dim):\n  \"\"\"Utility to get v1 or v2 TensorShape dim as an int.\n  Args:\n    dim: The TensorShape dimension to get as an int\n  Returns:\n    None or an int.\n  \"\"\"\n  try:\n    return dim.value\n  except AttributeError:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "documentation": {}
    },
    {
        "label": "get_batch_size",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "peekOfCode": "def get_batch_size(tensor_shape):\n  \"\"\"Returns batch size from the tensor shape.\n  Args:\n    tensor_shape: A rank 4 TensorShape.\n  Returns:\n    An integer representing the batch size of the tensor.\n  \"\"\"\n  tensor_shape.assert_has_rank(rank=4)\n  return get_dim_as_int(tensor_shape[0])\ndef get_height(tensor_shape):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "documentation": {}
    },
    {
        "label": "get_height",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "peekOfCode": "def get_height(tensor_shape):\n  \"\"\"Returns height from the tensor shape.\n  Args:\n    tensor_shape: A rank 4 TensorShape.\n  Returns:\n    An integer representing the height of the tensor.\n  \"\"\"\n  tensor_shape.assert_has_rank(rank=4)\n  return get_dim_as_int(tensor_shape[1])\ndef get_width(tensor_shape):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "documentation": {}
    },
    {
        "label": "get_width",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "peekOfCode": "def get_width(tensor_shape):\n  \"\"\"Returns width from the tensor shape.\n  Args:\n    tensor_shape: A rank 4 TensorShape.\n  Returns:\n    An integer representing the width of the tensor.\n  \"\"\"\n  tensor_shape.assert_has_rank(rank=4)\n  return get_dim_as_int(tensor_shape[2])\ndef get_depth(tensor_shape):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "documentation": {}
    },
    {
        "label": "get_depth",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "peekOfCode": "def get_depth(tensor_shape):\n  \"\"\"Returns depth from the tensor shape.\n  Args:\n    tensor_shape: A rank 4 TensorShape.\n  Returns:\n    An integer representing the depth of the tensor.\n  \"\"\"\n  tensor_shape.assert_has_rank(rank=4)\n  return get_dim_as_int(tensor_shape[3])",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.static_shape",
        "documentation": {}
    },
    {
        "label": "EvalMetricOpsVisualization",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "class EvalMetricOpsVisualization(six.with_metaclass(abc.ABCMeta, object)):\n  \"\"\"Abstract base class responsible for visualizations during evaluation.\n  Currently, summary images are not run during evaluation. One way to produce\n  evaluation images in Tensorboard is to provide tf.summary.image strings as\n  `value_ops` in tf.estimator.EstimatorSpec's `eval_metric_ops`. This class is\n  responsible for accruing images (with overlaid detections and groundtruth)\n  and returning a dictionary that can be passed to `eval_metric_ops`.\n  \"\"\"\n  def __init__(self,\n               category_index,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "VisualizeSingleFrameDetections",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "class VisualizeSingleFrameDetections(EvalMetricOpsVisualization):\n  \"\"\"Class responsible for single-frame object detection visualizations.\"\"\"\n  def __init__(self,\n               category_index,\n               max_examples_to_draw=5,\n               max_boxes_to_draw=20,\n               min_score_thresh=0.2,\n               use_normalized_coordinates=True,\n               summary_name_prefix='Detections_Left_Groundtruth_Right',\n               keypoint_edges=None):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "save_image_array_as_png",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def save_image_array_as_png(image, output_path):\n  \"\"\"Saves an image (represented as a numpy array) to PNG.\n  Args:\n    image: a numpy array with shape [height, width, 3].\n    output_path: path to which image should be written.\n  \"\"\"\n  image_pil = Image.fromarray(np.uint8(image)).convert('RGB')\n  with tf.gfile.Open(output_path, 'w') as fid:\n    image_pil.save(fid, 'PNG')\ndef encode_image_array_as_png_str(image):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "encode_image_array_as_png_str",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def encode_image_array_as_png_str(image):\n  \"\"\"Encodes a numpy array into a PNG string.\n  Args:\n    image: a numpy array with shape [height, width, 3].\n  Returns:\n    PNG encoded image string.\n  \"\"\"\n  image_pil = Image.fromarray(np.uint8(image))\n  output = six.BytesIO()\n  image_pil.save(output, format='PNG')",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "draw_bounding_box_on_image_array",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def draw_bounding_box_on_image_array(image,\n                                     ymin,\n                                     xmin,\n                                     ymax,\n                                     xmax,\n                                     color='red',\n                                     thickness=4,\n                                     display_str_list=(),\n                                     use_normalized_coordinates=True):\n  \"\"\"Adds a bounding box to an image (numpy array).",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "draw_bounding_box_on_image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def draw_bounding_box_on_image(image,\n                               ymin,\n                               xmin,\n                               ymax,\n                               xmax,\n                               color='red',\n                               thickness=4,\n                               display_str_list=(),\n                               use_normalized_coordinates=True):\n  \"\"\"Adds a bounding box to an image.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "draw_bounding_boxes_on_image_array",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def draw_bounding_boxes_on_image_array(image,\n                                       boxes,\n                                       color='red',\n                                       thickness=4,\n                                       display_str_list_list=()):\n  \"\"\"Draws bounding boxes on image (numpy array).\n  Args:\n    image: a numpy array object.\n    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax). The\n      coordinates are in normalized format between [0, 1].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "draw_bounding_boxes_on_image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def draw_bounding_boxes_on_image(image,\n                                 boxes,\n                                 color='red',\n                                 thickness=4,\n                                 display_str_list_list=()):\n  \"\"\"Draws bounding boxes on image.\n  Args:\n    image: a PIL.Image object.\n    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax). The\n      coordinates are in normalized format between [0, 1].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "create_visualization_fn",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def create_visualization_fn(category_index,\n                            include_masks=False,\n                            include_keypoints=False,\n                            include_track_ids=False,\n                            **kwargs):\n  \"\"\"Constructs a visualization function that can be wrapped in a py_func.\n  py_funcs only accept positional arguments. This function returns a suitable\n  function with the correct positional argument mapping. The positional\n  arguments in order are:\n  0: image",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "draw_bounding_boxes_on_image_tensors",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def draw_bounding_boxes_on_image_tensors(images,\n                                         boxes,\n                                         classes,\n                                         scores,\n                                         category_index,\n                                         original_image_spatial_shape=None,\n                                         true_image_shape=None,\n                                         instance_masks=None,\n                                         keypoints=None,\n                                         keypoint_edges=None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "draw_side_by_side_evaluation_image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def draw_side_by_side_evaluation_image(eval_dict,\n                                       category_index,\n                                       max_boxes_to_draw=20,\n                                       min_score_thresh=0.2,\n                                       use_normalized_coordinates=True,\n                                       keypoint_edges=None):\n  \"\"\"Creates a side-by-side image with detections and groundtruth.\n  Bounding boxes (and instance masks, if available) are visualized on both\n  subimages.\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "draw_keypoints_on_image_array",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def draw_keypoints_on_image_array(image,\n                                  keypoints,\n                                  color='red',\n                                  radius=2,\n                                  use_normalized_coordinates=True,\n                                  keypoint_edges=None,\n                                  keypoint_edge_color='green',\n                                  keypoint_edge_width=2):\n  \"\"\"Draws keypoints on an image (numpy array).\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "draw_keypoints_on_image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def draw_keypoints_on_image(image,\n                            keypoints,\n                            color='red',\n                            radius=2,\n                            use_normalized_coordinates=True,\n                            keypoint_edges=None,\n                            keypoint_edge_color='green',\n                            keypoint_edge_width=2):\n  \"\"\"Draws keypoints on an image.\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "draw_mask_on_image_array",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def draw_mask_on_image_array(image, mask, color='red', alpha=0.4):\n  \"\"\"Draws mask on an image.\n  Args:\n    image: uint8 numpy array with shape (img_height, img_height, 3)\n    mask: a uint8 numpy array of shape (img_height, img_height) with values\n      between either 0 or 1.\n    color: color to draw the keypoints with. Default is red.\n    alpha: transparency value between 0 and 1. (default: 0.4)\n  Raises:\n    ValueError: On incorrect data type for image or masks.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "visualize_boxes_and_labels_on_image_array",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def visualize_boxes_and_labels_on_image_array(\n    image,\n    boxes,\n    classes,\n    scores,\n    category_index,\n    instance_masks=None,\n    instance_boundaries=None,\n    keypoints=None,\n    keypoint_edges=None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "add_cdf_image_summary",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def add_cdf_image_summary(values, name):\n  \"\"\"Adds a tf.summary.image for a CDF plot of the values.\n  Normalizes `values` such that they sum to 1, plots the cumulative distribution\n  function and creates a tf image summary.\n  Args:\n    values: a 1-D float32 tensor containing the values.\n    name: name for the image summary.\n  \"\"\"\n  _force_matplotlib_backend()\n  def cdf_plot(values):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "add_hist_image_summary",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "def add_hist_image_summary(values, bins, name):\n  \"\"\"Adds a tf.summary.image for a histogram plot of the values.\n  Plots the histogram of values and creates a tf image summary.\n  Args:\n    values: a 1-D float32 tensor containing the values.\n    bins: bin edges which will be directly passed to np.histogram.\n    name: name for the image summary.\n  \"\"\"\n  _force_matplotlib_backend()\n  def hist_plot(values, bins):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "_TITLE_LEFT_MARGIN",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "_TITLE_LEFT_MARGIN = 10\n_TITLE_TOP_MARGIN = 10\nSTANDARD_COLORS = [\n    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "_TITLE_TOP_MARGIN",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "_TITLE_TOP_MARGIN = 10\nSTANDARD_COLORS = [\n    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "STANDARD_COLORS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "peekOfCode": "STANDARD_COLORS = [\n    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.visualize.vis_utils",
        "documentation": {}
    },
    {
        "label": "EvaluationMetric",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.coco_metric",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.coco_metric",
        "peekOfCode": "class EvaluationMetric():\n  \"\"\"COCO evaluation metric class.\n  This class cannot inherit from tf.keras.metrics.Metric due to numpy.\n  \"\"\"\n  def __init__(self, filename=None, testdev_dir=None, label_map=None):\n    \"\"\"Constructs COCO evaluation class.\n    The class provides the interface to metrics_fn in TPUEstimator. The\n    _update_op() takes detections from each image and push them to\n    self.detections. The _evaluate() loads a JSON file in COCO annotation format\n    as the groundtruth and runs COCO evaluation.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.coco_metric",
        "documentation": {}
    },
    {
        "label": "block_print",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.coco_metric",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.coco_metric",
        "peekOfCode": "def block_print(log_level):\n  \"\"\"Disables print function when current logging level > log_level.\"\"\"\n  if tf.get_logger().getEffectiveLevel() > log_level:\n    sys.stdout = open(os.devnull, 'w')\ndef enable_print(original_stdout):\n  \"\"\"Enables print function.\"\"\"\n  sys.stdout = original_stdout\nclass EvaluationMetric():\n  \"\"\"COCO evaluation metric class.\n  This class cannot inherit from tf.keras.metrics.Metric due to numpy.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.coco_metric",
        "documentation": {}
    },
    {
        "label": "enable_print",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.coco_metric",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.coco_metric",
        "peekOfCode": "def enable_print(original_stdout):\n  \"\"\"Enables print function.\"\"\"\n  sys.stdout = original_stdout\nclass EvaluationMetric():\n  \"\"\"COCO evaluation metric class.\n  This class cannot inherit from tf.keras.metrics.Metric due to numpy.\n  \"\"\"\n  def __init__(self, filename=None, testdev_dir=None, label_map=None):\n    \"\"\"Constructs COCO evaluation class.\n    The class provides the interface to metrics_fn in TPUEstimator. The",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.coco_metric",
        "documentation": {}
    },
    {
        "label": "InputProcessor",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataloader",
        "peekOfCode": "class InputProcessor:\n  \"\"\"Base class of Input processor.\"\"\"\n  def __init__(self, image, output_size):\n    \"\"\"Initializes a new `InputProcessor`.\n    Args:\n      image: The input image before processing.\n      output_size: The output image size after calling resize_and_crop_image\n        function.\n    \"\"\"\n    self._image = image",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataloader",
        "documentation": {}
    },
    {
        "label": "DetectionInputProcessor",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataloader",
        "peekOfCode": "class DetectionInputProcessor(InputProcessor):\n  \"\"\"Input processor for object detection.\"\"\"\n  def __init__(self, image, output_size, boxes=None, classes=None):\n    InputProcessor.__init__(self, image, output_size)\n    self._boxes = boxes\n    self._classes = classes\n  def random_horizontal_flip(self):\n    \"\"\"Randomly flip input image and bounding boxes.\"\"\"\n    self._image, self._boxes = preprocessor.random_horizontal_flip(\n        self._image, boxes=self._boxes)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataloader",
        "documentation": {}
    },
    {
        "label": "InputReader",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataloader",
        "peekOfCode": "class InputReader:\n  \"\"\"Input reader for dataset.\"\"\"\n  def __init__(self,\n               file_pattern,\n               is_training,\n               use_fake_data=False,\n               max_instances_per_image=None,\n               debug=False):\n    self._file_pattern = file_pattern\n    self._is_training = is_training",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataloader",
        "documentation": {}
    },
    {
        "label": "pad_to_fixed_size",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataloader",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataloader",
        "peekOfCode": "def pad_to_fixed_size(data, pad_value, output_shape):\n  \"\"\"Pad data to a fixed length at the first dimension.\n  Args:\n    data: Tensor to be padded to output_shape.\n    pad_value: A constant value assigned to the paddings.\n    output_shape: The output shape of a 2D tensor.\n  Returns:\n    The Padded tensor with output_shape [max_instances_per_image, dimension].\n  \"\"\"\n  max_instances_per_image = output_shape[0]",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.dataloader",
        "documentation": {}
    },
    {
        "label": "update_learning_rate_schedule_parameters",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "peekOfCode": "def update_learning_rate_schedule_parameters(params):\n  \"\"\"Updates params that are related to the learning rate schedule.\"\"\"\n  # params['batch_size'] is per-shard within model_fn if strategy=tpu.\n  batch_size = (\n      params['batch_size'] * params['num_shards']\n      if params['strategy'] in ['tpu', 'gpus'] else params['batch_size'])\n  # Learning rate is proportional to the batch size\n  params['adjusted_learning_rate'] = (\n      params['learning_rate'] * batch_size / _DEFAULT_BATCH_SIZE)\n  if 'lr_warmup_init' in params:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "documentation": {}
    },
    {
        "label": "stepwise_lr_schedule",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "peekOfCode": "def stepwise_lr_schedule(adjusted_learning_rate, adjusted_lr_warmup_init,\n                         lr_warmup_step, first_lr_drop_step,\n                         second_lr_drop_step, global_step):\n  \"\"\"Handles linear scaling rule, gradual warmup, and LR decay.\"\"\"\n  # adjusted_lr_warmup_init is the starting learning rate; LR is linearly\n  # scaled up to the full learning rate after `lr_warmup_step` before decaying.\n  logging.info('LR schedule method: stepwise')\n  linear_warmup = (\n      adjusted_lr_warmup_init +\n      (tf.cast(global_step, dtype=tf.float32) / lr_warmup_step *",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "documentation": {}
    },
    {
        "label": "cosine_lr_schedule",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "peekOfCode": "def cosine_lr_schedule(adjusted_lr, adjusted_lr_warmup_init, lr_warmup_step,\n                       total_steps, step):\n  \"\"\"Cosine learning rate scahedule.\"\"\"\n  logging.info('LR schedule method: cosine')\n  linear_warmup = (\n      adjusted_lr_warmup_init +\n      (tf.cast(step, dtype=tf.float32) / lr_warmup_step *\n       (adjusted_lr - adjusted_lr_warmup_init)))\n  decay_steps = tf.cast(total_steps - lr_warmup_step, tf.float32)\n  cosine_lr = 0.5 * adjusted_lr * (",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "documentation": {}
    },
    {
        "label": "polynomial_lr_schedule",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "peekOfCode": "def polynomial_lr_schedule(adjusted_lr, adjusted_lr_warmup_init, lr_warmup_step,\n                           power, total_steps, step):\n  logging.info('LR schedule method: polynomial')\n  linear_warmup = (\n      adjusted_lr_warmup_init +\n      (tf.cast(step, dtype=tf.float32) / lr_warmup_step *\n       (adjusted_lr - adjusted_lr_warmup_init)))\n  polynomial_lr = adjusted_lr * tf.pow(\n      1 - (tf.cast(step, tf.float32) / total_steps), power)\n  return tf.where(step < lr_warmup_step, linear_warmup, polynomial_lr)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "documentation": {}
    },
    {
        "label": "learning_rate_schedule",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "peekOfCode": "def learning_rate_schedule(params, global_step):\n  \"\"\"Learning rate schedule based on global step.\"\"\"\n  lr_decay_method = params['lr_decay_method']\n  if lr_decay_method == 'stepwise':\n    return stepwise_lr_schedule(params['adjusted_learning_rate'],\n                                params['adjusted_lr_warmup_init'],\n                                params['lr_warmup_step'],\n                                params['first_lr_drop_step'],\n                                params['second_lr_drop_step'], global_step)\n  if lr_decay_method == 'cosine':",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "documentation": {}
    },
    {
        "label": "focal_loss",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "peekOfCode": "def focal_loss(y_pred, y_true, alpha, gamma, normalizer, label_smoothing=0.0):\n  \"\"\"Compute the focal loss between `logits` and the golden `target` values.\n  Focal loss = -(1-pt)^gamma * log(pt)\n  where pt is the probability of being classified to the true class.\n  Args:\n    y_pred: A float tensor of size [batch, height_in, width_in,\n      num_predictions].\n    y_true: A float tensor of size [batch, height_in, width_in,\n      num_predictions].\n    alpha: A float scalar multiplying alpha to the loss from positive examples",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "documentation": {}
    },
    {
        "label": "detection_loss",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "peekOfCode": "def detection_loss(cls_outputs, box_outputs, labels, params):\n  \"\"\"Computes total detection loss.\n  Computes total detection loss including box and class loss from all levels.\n  Args:\n    cls_outputs: an OrderDict with keys representing levels and values\n      representing logits in [batch_size, height, width, num_anchors].\n    box_outputs: an OrderDict with keys representing levels and values\n      representing box regression targets in [batch_size, height, width,\n      num_anchors * 4].\n    labels: the dictionary that returned from dataloader that includes",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "documentation": {}
    },
    {
        "label": "reg_l2_loss",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "peekOfCode": "def reg_l2_loss(weight_decay, regex=r'.*(kernel|weight):0$'):\n  \"\"\"Return regularization l2 loss loss.\"\"\"\n  var_match = re.compile(regex)\n  return weight_decay * tf.add_n([\n      tf.nn.l2_loss(v)\n      for v in tf.trainable_variables()\n      if var_match.match(v.name)\n  ])\n@tf.autograph.experimental.do_not_convert\ndef _model_fn(features, labels, mode, params, model, variable_filter_fn=None):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "documentation": {}
    },
    {
        "label": "efficientdet_model_fn",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "peekOfCode": "def efficientdet_model_fn(features, labels, mode, params):\n  \"\"\"EfficientDet model.\"\"\"\n  variable_filter_fn = functools.partial(\n      efficientdet_arch.freeze_vars, pattern=params['var_freeze_expr'])\n  return _model_fn(\n      features,\n      labels,\n      mode,\n      params,\n      model=efficientdet_arch.efficientdet,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "documentation": {}
    },
    {
        "label": "get_model_arch",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "peekOfCode": "def get_model_arch(model_name='efficientdet-d0'):\n  \"\"\"Get model architecture for a given model name.\"\"\"\n  if 'efficientdet' in model_name:\n    return efficientdet_arch.efficientdet\n  raise ValueError('Invalide model name {}'.format(model_name))\ndef get_model_fn(model_name='efficientdet-d0'):\n  \"\"\"Get model fn for a given model name.\"\"\"\n  if 'efficientdet' in model_name:\n    return efficientdet_model_fn\n  raise ValueError('Invalide model name {}'.format(model_name))",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "documentation": {}
    },
    {
        "label": "get_model_fn",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "peekOfCode": "def get_model_fn(model_name='efficientdet-d0'):\n  \"\"\"Get model fn for a given model name.\"\"\"\n  if 'efficientdet' in model_name:\n    return efficientdet_model_fn\n  raise ValueError('Invalide model name {}'.format(model_name))",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "documentation": {}
    },
    {
        "label": "_DEFAULT_BATCH_SIZE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "peekOfCode": "_DEFAULT_BATCH_SIZE = 64\ndef update_learning_rate_schedule_parameters(params):\n  \"\"\"Updates params that are related to the learning rate schedule.\"\"\"\n  # params['batch_size'] is per-shard within model_fn if strategy=tpu.\n  batch_size = (\n      params['batch_size'] * params['num_shards']\n      if params['strategy'] in ['tpu', 'gpus'] else params['batch_size'])\n  # Learning rate is proportional to the batch size\n  params['adjusted_learning_rate'] = (\n      params['learning_rate'] * batch_size / _DEFAULT_BATCH_SIZE)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.det_model_fn",
        "documentation": {}
    },
    {
        "label": "freeze_vars",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "peekOfCode": "def freeze_vars(variables, pattern):\n  \"\"\"Removes backbone+fpn variables from the input.\n  Args:\n    variables: all the variables in training\n    pattern: a reg experession such as \".*(efficientnet|fpn_cells).*\".\n  Returns:\n    var_list: a list containing variables for training\n  \"\"\"\n  if pattern:\n    filtered_vars = [v for v in variables if not re.match(pattern, v.name)]",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "documentation": {}
    },
    {
        "label": "resample_feature_map",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "peekOfCode": "def resample_feature_map(feat,\n                         name,\n                         target_height,\n                         target_width,\n                         target_num_channels,\n                         apply_bn=False,\n                         is_training=None,\n                         conv_after_downsample=False,\n                         strategy=None,\n                         data_format='channels_last',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "documentation": {}
    },
    {
        "label": "class_net",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "peekOfCode": "def class_net(images,\n              level,\n              num_classes,\n              num_anchors,\n              num_filters,\n              is_training,\n              act_type,\n              separable_conv=True,\n              repeats=4,\n              survival_prob=None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "documentation": {}
    },
    {
        "label": "box_net",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "peekOfCode": "def box_net(images,\n            level,\n            num_anchors,\n            num_filters,\n            is_training,\n            act_type,\n            repeats=4,\n            separable_conv=True,\n            survival_prob=None,\n            strategy=None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "documentation": {}
    },
    {
        "label": "build_class_and_box_outputs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "peekOfCode": "def build_class_and_box_outputs(feats, config):\n  \"\"\"Builds box net and class net.\n  Args:\n   feats: input tensor.\n   config: a dict-like config, including all parameters.\n  Returns:\n   A tuple (class_outputs, box_outputs) for class/box predictions.\n  \"\"\"\n  class_outputs = {}\n  box_outputs = {}",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "peekOfCode": "def build_backbone(features, config):\n  \"\"\"Builds backbone model.\n  Args:\n   features: input tensor.\n   config: config for backbone, such as is_training_bn and backbone name.\n  Returns:\n    A dict from levels to the feature maps from the output of the backbone model\n    with strides of 8, 16 and 32.\n  Raises:\n    ValueError: if backbone_name is not supported.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "documentation": {}
    },
    {
        "label": "build_feature_network",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "peekOfCode": "def build_feature_network(features, config):\n  \"\"\"Build FPN input features.\n  Args:\n   features: input tensor.\n   config: a dict-like config, including all parameters.\n  Returns:\n    A dict from levels to the feature maps processed after feature network.\n  \"\"\"\n  feat_sizes = utils.get_feat_sizes(config.image_size, config.max_level)\n  feats = []",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "documentation": {}
    },
    {
        "label": "fuse_features",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "peekOfCode": "def fuse_features(nodes, weight_method):\n  \"\"\"Fuse features from different resolutions and return a weighted sum.\n  Args:\n    nodes: a list of tensorflow features at different levels\n    weight_method: feature fusion method. One of:\n      - \"attn\" - Softmax weighted fusion\n      - \"fastattn\" - Fast normalzied feature fusion\n      - \"sum\" - a sum of inputs\n  Returns:\n    A tensor denoting the fused feature.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "documentation": {}
    },
    {
        "label": "build_bifpn_layer",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "peekOfCode": "def build_bifpn_layer(feats, feat_sizes, config):\n  \"\"\"Builds a feature pyramid given previous feature pyramid and config.\"\"\"\n  p = config  # use p to denote the network config.\n  if p.fpn_config:\n    fpn_config = p.fpn_config\n  else:\n    fpn_config = fpn_configs.get_fpn_config(p.fpn_name, p.min_level,\n                                            p.max_level, p.fpn_weight_method)\n  num_output_connections = [0 for _ in feats]\n  for i, fnode in enumerate(fpn_config.nodes):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "documentation": {}
    },
    {
        "label": "efficientdet",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "peekOfCode": "def efficientdet(features, model_name=None, config=None, **kwargs):\n  \"\"\"Build EfficientDet model.\"\"\"\n  if not config and not model_name:\n    raise ValueError('please specify either model name or config')\n  if not config:\n    config = hparams_config.get_efficientdet_config(model_name)\n  elif isinstance(config, dict):\n    config = hparams_config.Config(config)  # wrap dict in Config object\n  if kwargs:\n    config.override(kwargs)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.efficientdet_arch",
        "documentation": {}
    },
    {
        "label": "Config",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "peekOfCode": "class Config(object):\n  \"\"\"A config utility class.\"\"\"\n  def __init__(self, config_dict=None):\n    self.update(config_dict)\n  def __setattr__(self, k, v):\n    self.__dict__[k] = Config(v) if isinstance(v, dict) else copy.deepcopy(v)\n  def __getattr__(self, k):\n    return self.__dict__[k]\n  def __getitem__(self, k):\n    return self.__dict__[k]",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "documentation": {}
    },
    {
        "label": "eval_str_fn",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "peekOfCode": "def eval_str_fn(val):\n  if val in {'true', 'false'}:\n    return val == 'true'\n  try:\n    return ast.literal_eval(val)\n  except (ValueError, SyntaxError):\n    return val\n# pylint: disable=protected-access\nclass Config(object):\n  \"\"\"A config utility class.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "documentation": {}
    },
    {
        "label": "default_detection_configs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "peekOfCode": "def default_detection_configs():\n  \"\"\"Returns a default detection configs.\"\"\"\n  h = Config()\n  # model name.\n  h.name = 'efficientdet-d1'\n  # activation type: see activation_fn in utils.py.\n  h.act_type = 'swish'\n  # input preprocessing parameters\n  h.image_size = 640  # An integer or a string WxH such as 640x320.\n  h.target_size = None",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "documentation": {}
    },
    {
        "label": "get_efficientdet_config",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "peekOfCode": "def get_efficientdet_config(model_name='efficientdet-d1'):\n  \"\"\"Get the default config for EfficientDet based on model name.\"\"\"\n  h = default_detection_configs()\n  if model_name in efficientdet_model_param_dict:\n    h.override(efficientdet_model_param_dict[model_name])\n  elif model_name in efficientdet_lite_param_dict:\n    h.override(efficientdet_lite_param_dict[model_name])\n  else:\n    raise ValueError('Unknown model name: {}'.format(model_name))\n  return h",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "documentation": {}
    },
    {
        "label": "get_detection_config",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "peekOfCode": "def get_detection_config(model_name):\n  if model_name.startswith('efficientdet'):\n    return get_efficientdet_config(model_name)\n  else:\n    raise ValueError('model name must start with efficientdet.')",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "documentation": {}
    },
    {
        "label": "efficientdet_model_param_dict",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "peekOfCode": "efficientdet_model_param_dict = {\n    'efficientdet-d0':\n        dict(\n            name='efficientdet-d0',\n            backbone_name='efficientnet-b0',\n            image_size=512,\n            fpn_num_filters=64,\n            fpn_cell_repeats=3,\n            box_class_repeats=3,\n        ),",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "documentation": {}
    },
    {
        "label": "lite_common_param",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "peekOfCode": "lite_common_param = dict(\n    mean_rgb=127.0,\n    stddev_rgb=128.0,\n    act_type='relu6',\n    fpn_weight_method='sum',\n)\nefficientdet_lite_param_dict = {\n    # lite models are in progress and subject to changes.\n    # mean_rgb and stddev_rgb are consistent with EfficientNet-Lite models in\n    # https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/lite/efficientnet_lite_builder.py#L28",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "documentation": {}
    },
    {
        "label": "efficientdet_lite_param_dict",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "peekOfCode": "efficientdet_lite_param_dict = {\n    # lite models are in progress and subject to changes.\n    # mean_rgb and stddev_rgb are consistent with EfficientNet-Lite models in\n    # https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/lite/efficientnet_lite_builder.py#L28\n    'efficientdet-lite0':\n        dict(\n            name='efficientdet-lite0',\n            backbone_name='efficientnet-lite0',\n            image_size=320,\n            fpn_num_filters=64,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.hparams_config",
        "documentation": {}
    },
    {
        "label": "ServingDriver",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "peekOfCode": "class ServingDriver(object):\n  \"\"\"A driver for serving single or batch images.\n  This driver supports serving with image files or arrays, with configurable\n  batch size.\n  Example 1. Serving streaming image contents:\n    driver = inference.ServingDriver(\n      'efficientdet-d0', '/tmp/efficientdet-d0', batch_size=1)\n    driver.build()\n    for m in image_iterator():\n      predictions = driver.serve_files([m])",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "documentation": {}
    },
    {
        "label": "InferenceDriver",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "peekOfCode": "class InferenceDriver(object):\n  \"\"\"A driver for doing batch inference.\n  Example usage:\n   driver = inference.InferenceDriver('efficientdet-d0', '/tmp/efficientdet-d0')\n   driver.inference('/tmp/*.jpg', '/tmp/outputdir')\n  \"\"\"\n  def __init__(self,\n               model_name: Text,\n               ckpt_path: Text,\n               model_params: Dict[Text, Any] = None):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "documentation": {}
    },
    {
        "label": "image_preprocess",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "peekOfCode": "def image_preprocess(image, image_size, mean_rgb, stddev_rgb):\n  \"\"\"Preprocess image for inference.\n  Args:\n    image: input image, can be a tensor or a numpy arary.\n    image_size: single integer of image size for square image or tuple of two\n      integers, in the format of (image_height, image_width).\n    mean_rgb: Mean value of RGB, can be a list of float or a float value.\n    stddev_rgb: Standard deviation of RGB, can be a list of float or a float\n      value.\n  Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "documentation": {}
    },
    {
        "label": "batch_image_files_decode",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "peekOfCode": "def batch_image_files_decode(image_files):\n  raw_images = tf.TensorArray(tf.uint8, size=0, dynamic_size=True)\n  for i in tf.range(tf.shape(image_files)[0]):\n    image = tf.io.decode_image(image_files[i])\n    image.set_shape([None, None, None])\n    raw_images = raw_images.write(i, image)\n  return raw_images.stack()\ndef batch_image_preprocess(raw_images,\n                           image_size: Union[int, Tuple[int, int]],\n                           mean_rgb,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "documentation": {}
    },
    {
        "label": "batch_image_preprocess",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "peekOfCode": "def batch_image_preprocess(raw_images,\n                           image_size: Union[int, Tuple[int, int]],\n                           mean_rgb,\n                           stddev_rgb,\n                           batch_size: int = None):\n  \"\"\"Preprocess batched images for inference.\n  Args:\n    raw_images: a list of images, each image can be a tensor or a numpy arary.\n    image_size: single integer of image size for square image or tuple of two\n      integers, in the format of (image_height, image_width).",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "documentation": {}
    },
    {
        "label": "build_inputs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "peekOfCode": "def build_inputs(\n    image_path_pattern: Text,\n    image_size: Union[int, Tuple[int, int]],\n    mean_rgb,\n    stddev_rgb,\n):\n  \"\"\"Read and preprocess input images.\n  Args:\n    image_path_pattern: a path to indicate a single or multiple files.\n    image_size: single integer of image size for square image or tuple of two",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "documentation": {}
    },
    {
        "label": "build_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "peekOfCode": "def build_model(model_name: Text, inputs: tf.Tensor, **kwargs):\n  \"\"\"Build model for a given model name.\n  Args:\n    model_name: the name of the model.\n    inputs: an image tensor or a numpy array.\n    **kwargs: extra parameters for model builder.\n  Returns:\n    (cls_outputs, box_outputs): the outputs for class and box predictions.\n    Each is a dictionary with key as feature level and value as predictions.\n  \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "documentation": {}
    },
    {
        "label": "restore_ckpt",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "peekOfCode": "def restore_ckpt(sess, ckpt_path, ema_decay=0.9998, export_ckpt=None):\n  \"\"\"Restore variables from a given checkpoint.\n  Args:\n    sess: a tf session for restoring or exporting models.\n    ckpt_path: the path of the checkpoint. Can be a file path or a folder path.\n    ema_decay: ema decay rate. If None or zero or negative value, disable ema.\n    export_ckpt: whether to export the restored model.\n  \"\"\"\n  sess.run(tf.global_variables_initializer())\n  if tf.io.gfile.isdir(ckpt_path):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "documentation": {}
    },
    {
        "label": "det_post_process",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "peekOfCode": "def det_post_process(params: Dict[Any, Any], cls_outputs: Dict[int, tf.Tensor],\n                     box_outputs: Dict[int, tf.Tensor], scales: List[float]):\n  \"\"\"Post preprocessing the box/class predictions.\n  Args:\n    params: a parameter dictionary that includes `min_level`, `max_level`,\n      `batch_size`, and `num_classes`.\n    cls_outputs: an OrderDict with keys representing levels and values\n      representing logits in [batch_size, height, width, num_anchors].\n    box_outputs: an OrderDict with keys representing levels and values\n      representing box regression targets in [batch_size, height, width,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "documentation": {}
    },
    {
        "label": "visualize_image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "peekOfCode": "def visualize_image(image,\n                    boxes,\n                    classes,\n                    scores,\n                    label_map=None,\n                    min_score_thresh=0.01,\n                    max_boxes_to_draw=1000,\n                    line_thickness=2,\n                    **kwargs):\n  \"\"\"Visualizes a given image.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "documentation": {}
    },
    {
        "label": "visualize_image_prediction",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "peekOfCode": "def visualize_image_prediction(image,\n                               prediction,\n                               label_map=None,\n                               **kwargs):\n  \"\"\"Viusalize detections on a given image.\n  Args:\n    image: Image content in shape of [height, width, 3].\n    prediction: a list of vector, with each vector has the format of [image_id,\n      ymin, xmin, ymax, xmax, score, class].\n    label_map: a map from label id to name.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.inference",
        "documentation": {}
    },
    {
        "label": "iou_loss",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.iou_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.iou_utils",
        "peekOfCode": "def iou_loss(pred_boxes: FloatType,\n             target_boxes: FloatType,\n             iou_type: Text = 'iou') -> tf.Tensor:\n  \"\"\"A unified interface for computing various IoU losses.\n  Let B and B_gt denotes the pred_box and B_gt is the target box (ground truth):\n    IoU = |B & B_gt| / |B | B_gt|\n    GIoU = IoU - |C - B U B_gt| / C, where C is the smallest box covering B and\n    B_gt.\n    DIoU = IoU - E(B, B_gt)^2 / c^2, E is the Euclidean distance of the center\n    points of B and B_gt, and c is the diagonal length of the smallest box",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.iou_utils",
        "documentation": {}
    },
    {
        "label": "FloatType",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.iou_utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.iou_utils",
        "peekOfCode": "FloatType = Union[tf.Tensor, float, np.float32, np.float64]\ndef _get_v(b1_height: FloatType, b1_width: FloatType, b2_height: FloatType,\n           b2_width: FloatType) -> tf.Tensor:\n  \"\"\"Get the consistency measurement of aspect ratio for ciou.\"\"\"\n  @tf.custom_gradient\n  def _get_grad_v(height, width):\n    \"\"\"backpropogate gradient.\"\"\"\n    arctan = tf.atan(tf.math.divide_no_nan(b1_width, b1_height)) - tf.atan(\n        tf.math.divide_no_nan(width, height))\n    v = 4 * ((arctan / math.pi)**2)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.iou_utils",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.main",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.main",
        "peekOfCode": "def main(_):\n  if FLAGS.strategy == 'tpu':\n    tf.disable_eager_execution()\n    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n    tpu_grpc_url = tpu_cluster_resolver.get_master()\n    tf.Session.reset(tpu_grpc_url)\n  else:\n    tpu_cluster_resolver = None\n  # Check data path",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.main",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.main",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.main",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef main(_):\n  if FLAGS.strategy == 'tpu':\n    tf.disable_eager_execution()\n    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n    tpu_grpc_url = tpu_cluster_resolver.get_master()\n    tf.Session.reset(tpu_grpc_url)\n  else:\n    tpu_cluster_resolver = None",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.main",
        "documentation": {}
    },
    {
        "label": "ModelInspector",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.model_inspect",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.model_inspect",
        "peekOfCode": "class ModelInspector(object):\n  \"\"\"A simple helper class for inspecting a model.\"\"\"\n  def __init__(self,\n               model_name: Text,\n               logdir: Text,\n               tensorrt: Text = False,\n               use_xla: bool = False,\n               ckpt_path: Text = None,\n               export_ckpt: Text = None,\n               saved_model_dir: Text = None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.model_inspect",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.model_inspect",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.model_inspect",
        "peekOfCode": "def main(_):\n  if tf.io.gfile.exists(FLAGS.logdir) and FLAGS.delete_logdir:\n    logging.info('Deleting log dir ...')\n    tf.io.gfile.rmtree(FLAGS.logdir)\n  inspector = ModelInspector(\n      model_name=FLAGS.model_name,\n      logdir=FLAGS.logdir,\n      tensorrt=FLAGS.tensorrt,\n      use_xla=FLAGS.use_xla,\n      ckpt_path=FLAGS.ckpt_path,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.model_inspect",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.model_inspect",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.model_inspect",
        "peekOfCode": "FLAGS = flags.FLAGS\nclass ModelInspector(object):\n  \"\"\"A simple helper class for inspecting a model.\"\"\"\n  def __init__(self,\n               model_name: Text,\n               logdir: Text,\n               tensorrt: Text = False,\n               use_xla: bool = False,\n               ckpt_path: Text = None,\n               export_ckpt: Text = None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.model_inspect",
        "documentation": {}
    },
    {
        "label": "diou_nms",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "peekOfCode": "def diou_nms(dets, iou_thresh=None):\n  \"\"\"DIOU non-maximum suppression.\n  diou = iou - square of euclidian distance of box centers\n     / square of diagonal of smallest enclosing bounding box\n  Reference: https://arxiv.org/pdf/1911.08287.pdf\n  Args:\n    dets: detection with shape (num, 5) and format [x1, y1, x2, y2, score].\n    iou_thresh: IOU threshold,\n  Returns:\n    numpy.array: Retained boxes.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "documentation": {}
    },
    {
        "label": "hard_nms",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "peekOfCode": "def hard_nms(dets, iou_thresh=None):\n  \"\"\"The basic hard non-maximum suppression.\n  Args:\n    dets: detection with shape (num, 5) and format [x1, y1, x2, y2, score].\n    iou_thresh: IOU threshold,\n  Returns:\n    numpy.array: Retained boxes.\n  \"\"\"\n  iou_thresh = iou_thresh or 0.5\n  x1 = dets[:, 0]",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "documentation": {}
    },
    {
        "label": "soft_nms",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "peekOfCode": "def soft_nms(dets, nms_configs):\n  \"\"\"Soft non-maximum suppression.\n  [1] Soft-NMS -- Improving Object Detection With One Line of Code.\n    https://arxiv.org/abs/1704.04503\n  Args:\n    dets: detection with shape (num, 5) and format [x1, y1, x2, y2, score].\n    nms_configs: a dict config that may contain the following members\n      * method: one of {`linear`, `gaussian`, 'hard'}. Use `gaussian` if None.\n      * iou_thresh (float): IOU threshold, only for `linear`, `hard`.\n      * sigma: Gaussian parameter, only for method 'gaussian'.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "documentation": {}
    },
    {
        "label": "nms",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "peekOfCode": "def nms(dets, nms_configs):\n  \"\"\"Non-maximum suppression.\n  Args:\n    dets: detection with shape (num, 5) and format [x1, y1, x2, y2, score].\n    nms_configs: a dict config that may contain parameters.\n  Returns:\n    numpy.array: Retained boxes.\n  \"\"\"\n  nms_configs = nms_configs or {}\n  method = nms_configs['method']",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "documentation": {}
    },
    {
        "label": "per_class_nms",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "peekOfCode": "def per_class_nms(boxes, scores, classes, image_id, image_scale, num_classes,\n                  max_boxes_to_draw, nms_configs):\n  \"\"\"Perform per class nms.\"\"\"\n  boxes = boxes[:, [1, 0, 3, 2]]\n  detections = []\n  for c in range(num_classes):\n    indices = np.where(classes == c)[0]\n    if indices.shape[0] == 0:\n      continue\n    boxes_cls = boxes[indices, :]",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "documentation": {}
    },
    {
        "label": "MIN_CLASS_SCORE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "peekOfCode": "MIN_CLASS_SCORE = -5.0\n# The score for a dummy detection\n_DUMMY_DETECTION_SCORE = -1e5\n# The maximum number of (anchor,class) pairs to keep for non-max suppression.\nMAX_DETECTION_POINTS = 5000\ndef diou_nms(dets, iou_thresh=None):\n  \"\"\"DIOU non-maximum suppression.\n  diou = iou - square of euclidian distance of box centers\n     / square of diagonal of smallest enclosing bounding box\n  Reference: https://arxiv.org/pdf/1911.08287.pdf",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "documentation": {}
    },
    {
        "label": "_DUMMY_DETECTION_SCORE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "peekOfCode": "_DUMMY_DETECTION_SCORE = -1e5\n# The maximum number of (anchor,class) pairs to keep for non-max suppression.\nMAX_DETECTION_POINTS = 5000\ndef diou_nms(dets, iou_thresh=None):\n  \"\"\"DIOU non-maximum suppression.\n  diou = iou - square of euclidian distance of box centers\n     / square of diagonal of smallest enclosing bounding box\n  Reference: https://arxiv.org/pdf/1911.08287.pdf\n  Args:\n    dets: detection with shape (num, 5) and format [x1, y1, x2, y2, score].",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "documentation": {}
    },
    {
        "label": "MAX_DETECTION_POINTS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "peekOfCode": "MAX_DETECTION_POINTS = 5000\ndef diou_nms(dets, iou_thresh=None):\n  \"\"\"DIOU non-maximum suppression.\n  diou = iou - square of euclidian distance of box centers\n     / square of diagonal of smallest enclosing bounding box\n  Reference: https://arxiv.org/pdf/1911.08287.pdf\n  Args:\n    dets: detection with shape (num, 5) and format [x1, y1, x2, y2, score].\n    iou_thresh: IOU threshold,\n  Returns:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.nms_np",
        "documentation": {}
    },
    {
        "label": "TFLiteRunner",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "peekOfCode": "class TFLiteRunner:\n  \"\"\"Wrapper to run TFLite model.\"\"\"\n  def __init__(self, model_path):\n    \"\"\"Init.\n    Args:\n      model_path: str, path to tflite model.\n    \"\"\"\n    self.interpreter = tf.lite.Interpreter(model_path=model_path)\n    self.interpreter.allocate_tensors()\n    self.input_index = self.interpreter.get_input_details()[0]['index']",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "peekOfCode": "def define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string('tflite_path', None, 'Path of tflite file.')\n  flags.DEFINE_string('sample_image', None, 'Sample image path')\n  flags.DEFINE_string('output_image', None, 'Output image path')\n  flags.DEFINE_string('image_size', '512x512', 'Image size \"WxH\".')\ndef load_image(image_path, image_size):\n  \"\"\"Loads an image, and returns numpy.ndarray.\n  Args:\n    image_path: str, path to image.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "documentation": {}
    },
    {
        "label": "load_image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "peekOfCode": "def load_image(image_path, image_size):\n  \"\"\"Loads an image, and returns numpy.ndarray.\n  Args:\n    image_path: str, path to image.\n    image_size: list of int, representing [width, height].\n  Returns:\n    image_batch: numpy.ndarray of shape [1, H, W, C].\n  \"\"\"\n  input_data = tf.io.gfile.GFile(image_path, 'rb').read()\n  image = tf.io.decode_image(input_data, channels=3, dtype=tf.uint8)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "documentation": {}
    },
    {
        "label": "save_visualized_image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "peekOfCode": "def save_visualized_image(image, prediction, output_path):\n  \"\"\"Saves the visualized image with prediction.\n  Args:\n    image: numpy.ndarray of shape [H, W, C].\n    prediction: numpy.ndarray of shape [num_predictions, 7].\n    output_path: str, output image path.\n  \"\"\"\n  output_image = inference.visualize_image_prediction(\n      image,\n      prediction,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "peekOfCode": "def main(_):\n  image_size = [int(dim) for dim in FLAGS.image_size.split('x')]\n  image = load_image(FLAGS.sample_image, image_size)\n  runner = TFLiteRunner(FLAGS.tflite_path)\n  prediction = runner.run(image)\n  save_visualized_image(image[0], prediction[0], FLAGS.output_image)\nif __name__ == '__main__':\n  define_flags()\n  app.run(main)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string('tflite_path', None, 'Path of tflite file.')\n  flags.DEFINE_string('sample_image', None, 'Sample image path')\n  flags.DEFINE_string('output_image', None, 'Output image path')\n  flags.DEFINE_string('image_size', '512x512', 'Image size \"WxH\".')\ndef load_image(image_path, image_size):\n  \"\"\"Loads an image, and returns numpy.ndarray.\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.run_tflite",
        "documentation": {}
    },
    {
        "label": "convert2trt",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.tensorrt",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.tensorrt",
        "peekOfCode": "def convert2trt(tf_savedmodel_dir: str, trt_savedmodel_dir: str):\n  converter = trt.TrtGraphConverter(\n      input_saved_model_dir=tf_savedmodel_dir,\n      max_workspace_size_bytes=(2 << 20),\n      precision_mode='FP16',\n      maximum_cached_engines=1)\n  converter.convert()\n  converter.save(trt_savedmodel_dir)\ndef benchmark(trt_savedmodel_dir: str, warmup_runs: int = 5, bm_runs: int = 20):\n  \"\"\"Benchmark TRT latency for a given TRT saved model.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.tensorrt",
        "documentation": {}
    },
    {
        "label": "benchmark",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.tensorrt",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.tensorrt",
        "peekOfCode": "def benchmark(trt_savedmodel_dir: str, warmup_runs: int = 5, bm_runs: int = 20):\n  \"\"\"Benchmark TRT latency for a given TRT saved model.\"\"\"\n  with tf.Session() as sess:\n    # First load the Saved Model into the session\n    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING],\n                               trt_savedmodel_dir)\n    graph = tf.get_default_graph()\n    input_shape = graph.get_tensor_by_name('input:0').shape\n    x = np.ones(input_shape).astype(np.float32)\n    ss = lambda i: '' if i == 0 else '_%d' % i",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.tensorrt",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.tensorrt",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.tensorrt",
        "peekOfCode": "def main(_):\n  if FLAGS.tf_savedmodel_dir:\n    convert2trt(FLAGS.tf_savedmodel_dir, FLAGS.trt_savedmodel_dir)\n  benchmark(FLAGS.trt_savedmodel_dir, FLAGS.warmup_runs, FLAGS.bm_runs)\nif __name__ == '__main__':\n  flags.mark_flag_as_required('trt_savedmodel_dir')\n  tf.disable_v2_behavior()\n  app.run(main)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.tensorrt",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.tensorrt",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.tensorrt",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef convert2trt(tf_savedmodel_dir: str, trt_savedmodel_dir: str):\n  converter = trt.TrtGraphConverter(\n      input_saved_model_dir=tf_savedmodel_dir,\n      max_workspace_size_bytes=(2 << 20),\n      precision_mode='FP16',\n      maximum_cached_engines=1)\n  converter.convert()\n  converter.save(trt_savedmodel_dir)\ndef benchmark(trt_savedmodel_dir: str, warmup_runs: int = 5, bm_runs: int = 20):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.tensorrt",
        "documentation": {}
    },
    {
        "label": "TpuBatchNormalization",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "class TpuBatchNormalization(tf.keras.layers.BatchNormalization):\n  \"\"\"Cross replica batch normalization.\"\"\"\n  def __init__(self, fused=False, **kwargs):\n    if not kwargs.get('name', None):\n      kwargs['name'] = 'tpu_batch_normalization'\n    if fused in (True, None):\n      raise ValueError('TpuBatchNormalization does not support fused=True.')\n    super().__init__(fused=fused, **kwargs)\n  def _moments(self, inputs, reduction_axes, keep_dims, mask=None):\n    \"\"\"Compute the mean and variance: it overrides the original _moments.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "SyncBatchNormalization",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "class SyncBatchNormalization(tf.keras.layers.BatchNormalization):\n  \"\"\"Cross replica batch normalization.\"\"\"\n  def __init__(self, fused=False, **kwargs):\n    if not kwargs.get('name', None):\n      kwargs['name'] = 'tpu_batch_normalization'\n    if fused in (True, None):\n      raise ValueError('SyncBatchNormalization does not support fused=True.')\n    super().__init__(fused=fused, **kwargs)\n  def _moments(self, inputs, reduction_axes, keep_dims, mask=None):\n    \"\"\"Compute the mean and variance: it overrides the original _moments.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "BatchNormalization",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "class BatchNormalization(tf.keras.layers.BatchNormalization):\n  \"\"\"Fixed default name of BatchNormalization to match TpuBatchNormalization.\"\"\"\n  def __init__(self, **kwargs):\n    if not kwargs.get('name', None):\n      kwargs['name'] = 'tpu_batch_normalization'\n    super().__init__(**kwargs)\n  def call(self, inputs, mask=None, training=None):\n    outputs = super().call(inputs, mask=mask, training=training)\n    # A temporary hack for tf1 compatibility with keras batch norm.\n    for u in self.updates:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "Pair",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "class Pair(tuple):\n  def __new__(cls, name, value):\n    return super().__new__(cls, (name, value))\n  def __init__(self, name, _):  # pylint: disable=super-init-not-called\n    self.name = name\ndef scalar(name, tensor, is_tpu=True):\n  \"\"\"Stores a (name, Tensor) tuple in a custom collection.\"\"\"\n  logging.info('Adding scale summary {}'.format(Pair(name, tensor)))\n  if is_tpu:\n    tf.add_to_collection('scalar_summaries', Pair(name, tf.reduce_mean(tensor)))",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "srelu_fn",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def srelu_fn(x):\n  \"\"\"Smooth relu: a smooth version of relu.\"\"\"\n  with tf.name_scope('srelu'):\n    beta = tf.Variable(20.0, name='srelu_beta', dtype=tf.float32)**2\n    beta = tf.cast(beta**2, x.dtype)\n    safe_log = tf.math.log(tf.where(x > 0., beta * x + 1., tf.ones_like(x)))\n    return tf.where((x > 0.), x - (1. / beta) * safe_log, tf.zeros_like(x))\ndef activation_fn(features: tf.Tensor, act_type: Text):\n  \"\"\"Customized non-linear activation type.\"\"\"\n  if act_type in ('silu', 'swish'):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "activation_fn",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def activation_fn(features: tf.Tensor, act_type: Text):\n  \"\"\"Customized non-linear activation type.\"\"\"\n  if act_type in ('silu', 'swish'):\n    return tf.nn.swish(features)\n  elif act_type == 'swish_native':\n    return features * tf.sigmoid(features)\n  elif act_type == 'hswish':\n    return features * tf.nn.relu6(features + 3) / 6\n  elif act_type == 'relu':\n    return tf.nn.relu(features)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "cross_replica_mean",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def cross_replica_mean(t, num_shards_per_group=None):\n  \"\"\"Calculates the average value of input tensor across TPU replicas.\"\"\"\n  num_shards = tpu_function.get_tpu_context().number_of_shards\n  if not num_shards:\n    return t\n  if not num_shards_per_group:\n    return tf.tpu.cross_replica_sum(t) / tf.cast(num_shards, t.dtype)\n  group_assignment = None\n  if num_shards_per_group > 1:\n    if num_shards % num_shards_per_group != 0:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "get_ema_vars",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def get_ema_vars():\n  \"\"\"Get all exponential moving average (ema) variables.\"\"\"\n  ema_vars = (\n      tf.trainable_variables() +\n      tf.get_collection(tf.GraphKeys.MOVING_AVERAGE_VARIABLES))\n  for v in tf.global_variables():\n    # We maintain mva for batch norm moving mean and variance as well.\n    if 'moving_mean' in v.name or 'moving_variance' in v.name:\n      ema_vars.append(v)\n  return list(set(ema_vars))",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "get_ckpt_var_map",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def get_ckpt_var_map(ckpt_path, ckpt_scope, var_scope, skip_mismatch=None):\n  \"\"\"Get a var map for restoring from pretrained checkpoints.\n  Args:\n    ckpt_path: string. A pretrained checkpoint path.\n    ckpt_scope: string. Scope name for checkpoint variables.\n    var_scope: string. Scope name for model variables.\n    skip_mismatch: skip variables if shape mismatch.\n  Returns:\n    var_map: a dictionary from checkpoint name to model variables.\n  \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "batch_norm_class",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def batch_norm_class(is_training, strategy=None):\n  if is_training and strategy == 'tpu':\n    return TpuBatchNormalization\n  elif is_training and strategy == 'gpus':\n    return SyncBatchNormalization\n  else:\n    return BatchNormalization\n# A cache of variable scope to BatchNorm layer.\n_BN_LAYER_CACHE = collections.defaultdict(dict)\n@gin.configurable",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "batch_normalization",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def batch_normalization(inputs,\n                        training=False,\n                        strategy=None,\n                        reuse_scope: bool = False,\n                        **kwargs):\n  \"\"\"A wrapper for TpuBatchNormalization.\n  Keras layers are incompatible with automatic tf scope reuse.\n  Supports reuse of the exiting variable scope when a model is called multiple\n  times. Otherwise, checkpoint weights would not be restored correctly.\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "batch_norm_act",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def batch_norm_act(inputs,\n                   is_training_bn: bool,\n                   act_type: Union[Text, None],\n                   init_zero: bool = False,\n                   data_format: Text = 'channels_last',\n                   momentum: float = 0.99,\n                   epsilon: float = 1e-3,\n                   strategy: Text = None,\n                   name: Text = None,\n                   batch_norm_trainable: bool = True):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "drop_connect",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def drop_connect(inputs, is_training, survival_prob):\n  \"\"\"Drop the entire conv with given survival probability.\"\"\"\n  # \"Deep Networks with Stochastic Depth\", https://arxiv.org/pdf/1603.09382.pdf\n  if not is_training:\n    return inputs\n  # Compute tensor.\n  batch_size = tf.shape(inputs)[0]\n  random_tensor = survival_prob\n  random_tensor += tf.random.uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\n  binary_tensor = tf.floor(random_tensor)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "num_params_flops",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def num_params_flops(readable_format=True):\n  \"\"\"Return number of parameters and flops.\"\"\"\n  nparams = np.sum(\n      [np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n  options = tf.profiler.ProfileOptionBuilder.float_operation()\n  options['output'] = 'none'\n  flops = tf.profiler.profile(\n      tf.get_default_graph(), options=options).total_float_ops\n  # We use flops to denote multiply-adds, which is counted as 2 ops in tfprof.\n  flops = flops // 2",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "scalar",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def scalar(name, tensor, is_tpu=True):\n  \"\"\"Stores a (name, Tensor) tuple in a custom collection.\"\"\"\n  logging.info('Adding scale summary {}'.format(Pair(name, tensor)))\n  if is_tpu:\n    tf.add_to_collection('scalar_summaries', Pair(name, tf.reduce_mean(tensor)))\n  else:\n    tf.summary.scalar(name, tf.reduce_mean(tensor))\ndef image(name, tensor, is_tpu=True):\n  logging.info('Adding image summary {}'.format(Pair(name, tensor)))\n  if is_tpu:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def image(name, tensor, is_tpu=True):\n  logging.info('Adding image summary {}'.format(Pair(name, tensor)))\n  if is_tpu:\n    tf.add_to_collection('image_summaries', Pair(name, tensor))\n  else:\n    tf.summary.image(name, tensor)\ndef get_tpu_host_call(global_step, params):\n  \"\"\"Get TPU host call for summaries.\"\"\"\n  scalar_summaries = tf.get_collection('scalar_summaries')\n  if params['img_summary_steps']:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "get_tpu_host_call",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def get_tpu_host_call(global_step, params):\n  \"\"\"Get TPU host call for summaries.\"\"\"\n  scalar_summaries = tf.get_collection('scalar_summaries')\n  if params['img_summary_steps']:\n    image_summaries = tf.get_collection('image_summaries')\n  else:\n    image_summaries = []\n  if not scalar_summaries and not image_summaries:\n    return None  # No summaries to write.\n  model_dir = params['model_dir']",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "archive_ckpt",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def archive_ckpt(ckpt_eval, ckpt_objective, ckpt_path):\n  \"\"\"Archive a checkpoint if the metric is better.\"\"\"\n  ckpt_dir, ckpt_name = os.path.split(ckpt_path)\n  saved_objective_path = os.path.join(ckpt_dir, 'best_objective.txt')\n  saved_objective = float('-inf')\n  if tf.io.gfile.exists(saved_objective_path):\n    with tf.io.gfile.GFile(saved_objective_path, 'r') as f:\n      saved_objective = float(f.read())\n  if saved_objective > ckpt_objective:\n    logging.info('Ckpt {} is worse than {}'.format(ckpt_objective,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "parse_image_size",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def parse_image_size(image_size: Union[Text, int, Tuple[int, int]]):\n  \"\"\"Parse the image size and return (height, width).\n  Args:\n    image_size: A integer, a tuple (H, W), or a string with HxW format.\n  Returns:\n    A tuple of integer (height, width).\n  \"\"\"\n  if isinstance(image_size, int):\n    # image_size is integer, with the same width and height.\n    return (image_size, image_size)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "get_feat_sizes",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def get_feat_sizes(image_size: Union[Text, int, Tuple[int, int]],\n                   max_level: int):\n  \"\"\"Get feat widths and heights for all levels.\n  Args:\n    image_size: A integer, a tuple (H, W), or a string with HxW format.\n    max_level: maximum feature level.\n  Returns:\n    feat_sizes: a list of tuples (height, width) for each level.\n  \"\"\"\n  image_size = parse_image_size(image_size)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "verify_feats_size",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def verify_feats_size(feats,\n                      feat_sizes,\n                      min_level,\n                      max_level,\n                      data_format='channels_last'):\n  \"\"\"Verify the feature map sizes.\"\"\"\n  expected_output_size = feat_sizes[min_level:max_level + 1]\n  for cnt, size in enumerate(expected_output_size):\n    h_id, w_id = (2, 3) if data_format == 'channels_first' else (1, 2)\n    if feats[cnt].shape[h_id] != size['height']:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "get_precision",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def get_precision(strategy: str, mixed_precision: bool = False):\n  \"\"\"Get the precision policy for a given strategy.\"\"\"\n  if mixed_precision:\n    if strategy == 'tpu':\n      return 'mixed_bfloat16'\n    if tf.config.list_physical_devices('GPU'):\n      return 'mixed_float16'\n    # TODO(fsx950223): Fix CPU float16 inference\n    # https://github.com/google/automl/issues/504\n    logging.warning('float16 is not supported for CPU, use float32 instead')",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "float16_scope",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def float16_scope():\n  \"\"\"Scope class for float16.\"\"\"\n  def _custom_getter(getter, *args, **kwargs):\n    \"\"\"Returns a custom getter that methods must be called under.\"\"\"\n    cast_to_float16 = False\n    requested_dtype = kwargs['dtype']\n    if requested_dtype == tf.float16:\n      kwargs['dtype'] = tf.float32\n      cast_to_float16 = True\n    var = getter(*args, **kwargs)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "set_precision_policy",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def set_precision_policy(policy_name: Text = None):\n  \"\"\"Set precision policy according to the name.\n  Args:\n    policy_name: precision policy name, one of 'float32', 'mixed_float16',\n      'mixed_bfloat16', or None.\n  \"\"\"\n  if not policy_name:\n    return\n  assert policy_name in ('mixed_float16', 'mixed_bfloat16', 'float32')\n  logging.info('use mixed precision policy name %s', policy_name)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "build_model_with_precision",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def build_model_with_precision(pp, mm, ii, *args, **kwargs):\n  \"\"\"Build model with its inputs/params for a specified precision context.\n  This is highly specific to this codebase, and not intended to be general API.\n  Advanced users only. DO NOT use it if you don't know what it does.\n  NOTE: short argument names are intended to avoid conficts with kwargs.\n  Args:\n    pp: A string, precision policy name, such as \"mixed_float16\".\n    mm: A function, for rmodel builder.\n    ii: A tensor, for model inputs.\n    *args: A list of model arguments.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "recompute_grad",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "def recompute_grad(recompute=False):\n  \"\"\"Decorator determine whether use gradient checkpoint.\"\"\"\n  def _wrapper(f):\n    if recompute:\n      return _recompute_grad(f)\n    return f\n  return _wrapper",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "_BN_LAYER_CACHE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "_BN_LAYER_CACHE = collections.defaultdict(dict)\n@gin.configurable\ndef batch_normalization(inputs,\n                        training=False,\n                        strategy=None,\n                        reuse_scope: bool = False,\n                        **kwargs):\n  \"\"\"A wrapper for TpuBatchNormalization.\n  Keras layers are incompatible with automatic tf scope reuse.\n  Supports reuse of the exiting variable scope when a model is called multiple",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "conv_kernel_initializer",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "conv_kernel_initializer = tf.initializers.variance_scaling()\ndense_kernel_initializer = tf.initializers.variance_scaling()\nclass Pair(tuple):\n  def __new__(cls, name, value):\n    return super().__new__(cls, (name, value))\n  def __init__(self, name, _):  # pylint: disable=super-init-not-called\n    self.name = name\ndef scalar(name, tensor, is_tpu=True):\n  \"\"\"Stores a (name, Tensor) tuple in a custom collection.\"\"\"\n  logging.info('Adding scale summary {}'.format(Pair(name, tensor)))",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "dense_kernel_initializer",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "peekOfCode": "dense_kernel_initializer = tf.initializers.variance_scaling()\nclass Pair(tuple):\n  def __new__(cls, name, value):\n    return super().__new__(cls, (name, value))\n  def __init__(self, name, _):  # pylint: disable=super-init-not-called\n    self.name = name\ndef scalar(name, tensor, is_tpu=True):\n  \"\"\"Stores a (name, Tensor) tuple in a custom collection.\"\"\"\n  logging.info('Adding scale summary {}'.format(Pair(name, tensor)))\n  if is_tpu:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.efficientdet.utils",
        "documentation": {}
    },
    {
        "label": "_sym_db",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "_sym_db = _symbol_database.Default()\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='input-config.proto',\n  package='tensorflow_examples.lite.examples.recommendation.ml_v2.configs',\n  syntax='proto2',\n  serialized_options=None,\n  serialized_pb=b'\\n\\x12input-config.proto\\x12>tensorflow_examples.lite.examples.recommendation.ml_v2.configs\\\"\\xd9\\x01\\n\\x07\\x46\\x65\\x61ture\\x12\\x14\\n\\x0c\\x66\\x65\\x61ture_name\\x18\\x01 \\x01(\\t\\x12\\x61\\n\\x0c\\x66\\x65\\x61ture_type\\x18\\x02 \\x01(\\x0e\\x32K.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureType\\x12\\x12\\n\\nvocab_name\\x18\\x03 \\x01(\\t\\x12\\x12\\n\\nvocab_size\\x18\\x04 \\x01(\\x03\\x12\\x15\\n\\rembedding_dim\\x18\\x05 \\x01(\\x03\\x12\\x16\\n\\x0e\\x66\\x65\\x61ture_length\\x18\\x06 \\x01(\\x03\\\"\\xcc\\x01\\n\\x0c\\x46\\x65\\x61tureGroup\\x12Y\\n\\x08\\x66\\x65\\x61tures\\x18\\x01 \\x03(\\x0b\\x32G.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature\\x12\\x61\\n\\x0c\\x65ncoder_type\\x18\\x02 \\x01(\\x0e\\x32K.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.EncoderType\\\"\\xc9\\x02\\n\\x0bInputConfig\\x12k\\n\\x15global_feature_groups\\x18\\x01 \\x03(\\x0b\\x32L.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup\\x12m\\n\\x17\\x61\\x63tivity_feature_groups\\x18\\x02 \\x03(\\x0b\\x32L.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup\\x12^\\n\\rlabel_feature\\x18\\x03 \\x01(\\x0b\\x32G.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature*-\\n\\x0b\\x46\\x65\\x61tureType\\x12\\n\\n\\x06STRING\\x10\\x00\\x12\\x07\\n\\x03INT\\x10\\x01\\x12\\t\\n\\x05\\x46LOAT\\x10\\x02*)\\n\\x0b\\x45ncoderType\\x12\\x07\\n\\x03\\x42OW\\x10\\x00\\x12\\x07\\n\\x03\\x43NN\\x10\\x01\\x12\\x08\\n\\x04LSTM\\x10\\x02'\n)\n_FEATURETYPE = _descriptor.EnumDescriptor(\n  name='FeatureType',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "DESCRIPTOR = _descriptor.FileDescriptor(\n  name='input-config.proto',\n  package='tensorflow_examples.lite.examples.recommendation.ml_v2.configs',\n  syntax='proto2',\n  serialized_options=None,\n  serialized_pb=b'\\n\\x12input-config.proto\\x12>tensorflow_examples.lite.examples.recommendation.ml_v2.configs\\\"\\xd9\\x01\\n\\x07\\x46\\x65\\x61ture\\x12\\x14\\n\\x0c\\x66\\x65\\x61ture_name\\x18\\x01 \\x01(\\t\\x12\\x61\\n\\x0c\\x66\\x65\\x61ture_type\\x18\\x02 \\x01(\\x0e\\x32K.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureType\\x12\\x12\\n\\nvocab_name\\x18\\x03 \\x01(\\t\\x12\\x12\\n\\nvocab_size\\x18\\x04 \\x01(\\x03\\x12\\x15\\n\\rembedding_dim\\x18\\x05 \\x01(\\x03\\x12\\x16\\n\\x0e\\x66\\x65\\x61ture_length\\x18\\x06 \\x01(\\x03\\\"\\xcc\\x01\\n\\x0c\\x46\\x65\\x61tureGroup\\x12Y\\n\\x08\\x66\\x65\\x61tures\\x18\\x01 \\x03(\\x0b\\x32G.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature\\x12\\x61\\n\\x0c\\x65ncoder_type\\x18\\x02 \\x01(\\x0e\\x32K.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.EncoderType\\\"\\xc9\\x02\\n\\x0bInputConfig\\x12k\\n\\x15global_feature_groups\\x18\\x01 \\x03(\\x0b\\x32L.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup\\x12m\\n\\x17\\x61\\x63tivity_feature_groups\\x18\\x02 \\x03(\\x0b\\x32L.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup\\x12^\\n\\rlabel_feature\\x18\\x03 \\x01(\\x0b\\x32G.tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature*-\\n\\x0b\\x46\\x65\\x61tureType\\x12\\n\\n\\x06STRING\\x10\\x00\\x12\\x07\\n\\x03INT\\x10\\x01\\x12\\t\\n\\x05\\x46LOAT\\x10\\x02*)\\n\\x0b\\x45ncoderType\\x12\\x07\\n\\x03\\x42OW\\x10\\x00\\x12\\x07\\n\\x03\\x43NN\\x10\\x01\\x12\\x08\\n\\x04LSTM\\x10\\x02'\n)\n_FEATURETYPE = _descriptor.EnumDescriptor(\n  name='FeatureType',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureType',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURETYPE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "_FEATURETYPE = _descriptor.EnumDescriptor(\n  name='FeatureType',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureType',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='STRING', index=0, number=0,\n      serialized_options=None,\n      type=None),",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "FeatureType",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "FeatureType = enum_type_wrapper.EnumTypeWrapper(_FEATURETYPE)\n_ENCODERTYPE = _descriptor.EnumDescriptor(\n  name='EncoderType',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.EncoderType',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='BOW', index=0, number=0,\n      serialized_options=None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "_ENCODERTYPE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "_ENCODERTYPE = _descriptor.EnumDescriptor(\n  name='EncoderType',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.EncoderType',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='BOW', index=0, number=0,\n      serialized_options=None,\n      type=None),",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "EncoderType",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "EncoderType = enum_type_wrapper.EnumTypeWrapper(_ENCODERTYPE)\nSTRING = 0\nINT = 1\nFLOAT = 2\nBOW = 0\nCNN = 1\nLSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "STRING",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "STRING = 0\nINT = 1\nFLOAT = 2\nBOW = 0\nCNN = 1\nLSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "INT",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "INT = 1\nFLOAT = 2\nBOW = 0\nCNN = 1\nLSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,\n  file=DESCRIPTOR,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "FLOAT",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "FLOAT = 2\nBOW = 0\nCNN = 1\nLSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "BOW",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "BOW = 0\nCNN = 1\nLSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "CNN",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "CNN = 1\nLSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "LSTM",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "LSTM = 2\n_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='feature_name', full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature.feature_name', index=0,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "_FEATURE = _descriptor.Descriptor(\n  name='Feature',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='feature_name', full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature.feature_name', index=0,\n      number=1, type=9, cpp_type=9, label=1,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATUREGROUP",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "_FEATUREGROUP = _descriptor.Descriptor(\n  name='FeatureGroup',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='features', full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup.features', index=0,\n      number=1, type=11, cpp_type=10, label=3,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "_INPUTCONFIG",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "_INPUTCONFIG = _descriptor.Descriptor(\n  name='InputConfig',\n  full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.InputConfig',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='global_feature_groups', full_name='tensorflow_examples.lite.examples.recommendation.ml_v2.configs.InputConfig.global_feature_groups', index=0,\n      number=1, type=11, cpp_type=10, label=3,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATURE.fields_by_name['feature_type'].enum_type",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "_FEATURE.fields_by_name['feature_type'].enum_type = _FEATURETYPE\n_FEATUREGROUP.fields_by_name['features'].message_type = _FEATURE\n_FEATUREGROUP.fields_by_name['encoder_type'].enum_type = _ENCODERTYPE\n_INPUTCONFIG.fields_by_name['global_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['activity_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['label_feature'].message_type = _FEATURE\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATUREGROUP.fields_by_name['features'].message_type",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "_FEATUREGROUP.fields_by_name['features'].message_type = _FEATURE\n_FEATUREGROUP.fields_by_name['encoder_type'].enum_type = _ENCODERTYPE\n_INPUTCONFIG.fields_by_name['global_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['activity_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['label_feature'].message_type = _FEATURE\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "_FEATUREGROUP.fields_by_name['encoder_type'].enum_type",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "_FEATUREGROUP.fields_by_name['encoder_type'].enum_type = _ENCODERTYPE\n_INPUTCONFIG.fields_by_name['global_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['activity_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['label_feature'].message_type = _FEATURE\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "_INPUTCONFIG.fields_by_name['global_feature_groups'].message_type",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "_INPUTCONFIG.fields_by_name['global_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['activity_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['label_feature'].message_type = _FEATURE\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "_INPUTCONFIG.fields_by_name['activity_feature_groups'].message_type",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "_INPUTCONFIG.fields_by_name['activity_feature_groups'].message_type = _FEATUREGROUP\n_INPUTCONFIG.fields_by_name['label_feature'].message_type = _FEATURE\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "_INPUTCONFIG.fields_by_name['label_feature'].message_type",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "_INPUTCONFIG.fields_by_name['label_feature'].message_type = _FEATURE\nDESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['Feature']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['Feature'] = _FEATURE\nDESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['FeatureGroup']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['FeatureGroup'] = _FEATUREGROUP\nDESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature)\n  })",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.message_types_by_name['InputConfig']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "DESCRIPTOR.message_types_by_name['InputConfig'] = _INPUTCONFIG\nDESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature)\n  })\n_sym_db.RegisterMessage(Feature)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.enum_types_by_name['FeatureType']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "DESCRIPTOR.enum_types_by_name['FeatureType'] = _FEATURETYPE\nDESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature)\n  })\n_sym_db.RegisterMessage(Feature)\nFeatureGroup = _reflection.GeneratedProtocolMessageType('FeatureGroup', (_message.Message,), {",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "DESCRIPTOR.enum_types_by_name['EncoderType']",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "DESCRIPTOR.enum_types_by_name['EncoderType'] = _ENCODERTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\nFeature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature)\n  })\n_sym_db.RegisterMessage(Feature)\nFeatureGroup = _reflection.GeneratedProtocolMessageType('FeatureGroup', (_message.Message,), {\n  'DESCRIPTOR' : _FEATUREGROUP,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "Feature",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "Feature = _reflection.GeneratedProtocolMessageType('Feature', (_message.Message,), {\n  'DESCRIPTOR' : _FEATURE,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.Feature)\n  })\n_sym_db.RegisterMessage(Feature)\nFeatureGroup = _reflection.GeneratedProtocolMessageType('FeatureGroup', (_message.Message,), {\n  'DESCRIPTOR' : _FEATUREGROUP,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "FeatureGroup",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "FeatureGroup = _reflection.GeneratedProtocolMessageType('FeatureGroup', (_message.Message,), {\n  'DESCRIPTOR' : _FEATUREGROUP,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.FeatureGroup)\n  })\n_sym_db.RegisterMessage(FeatureGroup)\nInputConfig = _reflection.GeneratedProtocolMessageType('InputConfig', (_message.Message,), {\n  'DESCRIPTOR' : _INPUTCONFIG,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.InputConfig)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "InputConfig",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "peekOfCode": "InputConfig = _reflection.GeneratedProtocolMessageType('InputConfig', (_message.Message,), {\n  'DESCRIPTOR' : _INPUTCONFIG,\n  '__module__' : 'input_config_pb2'\n  # @@protoc_insertion_point(class_scope:tensorflow_examples.lite.examples.recommendation.ml_v2.configs.InputConfig)\n  })\n_sym_db.RegisterMessage(InputConfig)\n# @@protoc_insertion_point(module_scope)\n# pyformat: enable",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.input_config_pb2",
        "documentation": {}
    },
    {
        "label": "ModelConfig",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.model_config",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.model_config",
        "peekOfCode": "class ModelConfig(object):\n  \"\"\"Class to hold parameters for model architecture configuration.\n  Attributes:\n      hidden_layer_dims: List of hidden layer dimensions.\n      eval_top_k: Top k to evaluate.\n      conv_num_filter_ratios: Number of filter ratios for the Conv1D layer.\n      conv_kernel_size: Size of the Conv1D layer kernel size.\n      lstm_num_units: Number of units for the LSTM layer.\n      num_predictions: Number of predictions to return with serving mode, which\n      has default value 10.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.configs.model_config",
        "documentation": {}
    },
    {
        "label": "MovieInfo",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "class MovieInfo(\n    collections.namedtuple(\n        \"MovieInfo\", [\"movie_id\", \"timestamp\", \"rating\", \"title\", \"genres\"])):\n  \"\"\"Data holder of basic information of a movie.\"\"\"\n  __slots__ = ()\n  def __new__(cls,\n              movie_id=PAD_MOVIE_ID,\n              timestamp=0,\n              rating=PAD_RATING,\n              title=\"\",",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")\n  flags.DEFINE_string(\"output_dir\", None,\n                      \"Path to the directory of output files.\")\n  flags.DEFINE_bool(\"build_vocabs\", True,\n                    \"If yes, generate movie feature vocabs.\")\n  flags.DEFINE_integer(\"min_timeline_length\", 3,\n                       \"The minimum timeline length to construct examples.\")",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "download_and_extract_data",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def download_and_extract_data(data_directory,\n                              url=MOVIELENS_1M_URL,\n                              fname=MOVIELENS_ZIP_FILENAME,\n                              file_hash=MOVIELENS_ZIP_HASH,\n                              extracted_dir_name=MOVIELENS_EXTRACTED_DIR):\n  \"\"\"Download and extract zip containing MovieLens data to a given directory.\n  Args:\n    data_directory: Local path to extract dataset to.\n    url: Direct path to MovieLens dataset .zip file. See constants above for\n      examples.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "read_data",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def read_data(data_directory, min_rating=None):\n  \"\"\"Read movielens ratings.dat and movies.dat file into dataframe.\"\"\"\n  ratings_df = pd.read_csv(\n      os.path.join(data_directory, RATINGS_FILE_NAME),\n      sep=\"::\",\n      names=RATINGS_DATA_COLUMNS,\n      encoding=\"unicode_escape\")  # May contain unicode. Need to escape.\n  ratings_df[\"Timestamp\"] = ratings_df[\"Timestamp\"].apply(int)\n  if min_rating is not None:\n    ratings_df = ratings_df[ratings_df[\"Rating\"] >= min_rating]",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "convert_to_timelines",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def convert_to_timelines(ratings_df):\n  \"\"\"Convert ratings data to user.\"\"\"\n  timelines = collections.defaultdict(list)\n  movie_counts = collections.Counter()\n  for user_id, movie_id, rating, timestamp in ratings_df.values:\n    timelines[user_id].append(\n        MovieInfo(movie_id=movie_id, timestamp=int(timestamp), rating=rating))\n    movie_counts[movie_id] += 1\n  # Sort per-user timeline by timestamp\n  for (user_id, context) in timelines.items():",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_movies_dict",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_movies_dict(movies_df):\n  \"\"\"Generates movies dictionary from movies dataframe.\"\"\"\n  movies_dict = {\n      movie_id: MovieInfo(movie_id=movie_id, title=title, genres=genres)\n      for movie_id, title, genres in movies_df.values\n  }\n  movies_dict[0] = MovieInfo()\n  return movies_dict\ndef extract_year_from_title(title):\n  year = re.search(r\"\\((\\d{4})\\)\", title)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "extract_year_from_title",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def extract_year_from_title(title):\n  year = re.search(r\"\\((\\d{4})\\)\", title)\n  if year:\n    return int(year.group(1))\n  return 0\ndef generate_feature_of_movie_years(movies_dict, movies):\n  \"\"\"Extracts year feature for movies from movie title.\"\"\"\n  return [\n      extract_year_from_title(movies_dict[movie.movie_id].title)\n      for movie in movies",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_feature_of_movie_years",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_feature_of_movie_years(movies_dict, movies):\n  \"\"\"Extracts year feature for movies from movie title.\"\"\"\n  return [\n      extract_year_from_title(movies_dict[movie.movie_id].title)\n      for movie in movies\n  ]\ndef generate_movie_genres(movies_dict, movies):\n  \"\"\"Create a feature of the genre of each movie.\n  Save genre as a feature for the movies.\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_movie_genres",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_movie_genres(movies_dict, movies):\n  \"\"\"Create a feature of the genre of each movie.\n  Save genre as a feature for the movies.\n  Args:\n    movies_dict: Dict of movies, keyed by movie_id with value of (title, genre)\n    movies: list of movies to extract genres.\n  Returns:\n    movie_genres: list of genres of all input movies.\n  \"\"\"\n  movie_genres = []",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_examples_from_single_timeline",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_examples_from_single_timeline(timeline,\n                                           movies_dict,\n                                           max_context_len=100,\n                                           max_context_movie_genre_len=320):\n  \"\"\"Generate TF examples from a single user timeline.\n  Generate TF examples from a single user timeline. Timeline with length less\n  than minimum timeline length will be skipped. And if context user history\n  length is shorter than max_context_len, features will be padded with default\n  values.\n  Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_examples_from_timelines",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_examples_from_timelines(timelines,\n                                     movies_df,\n                                     min_timeline_len=3,\n                                     max_context_len=100,\n                                     max_context_movie_genre_len=320,\n                                     train_data_fraction=0.9,\n                                     random_seed=None,\n                                     shuffle=True):\n  \"\"\"Convert user timelines to tf examples.\n  Convert user timelines to tf examples by adding all possible context-label",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_movie_feature_vocabs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_movie_feature_vocabs(movies_df, movie_counts):\n  \"\"\"Generate vocabularies for movie features.\n  Generate vocabularies for movie features (movie_id, genre, year), sorted by\n  usage count. Vocab id 0 will be reserved for default padding value.\n  Args:\n    movies_df: Dataframe for movies.\n    movie_counts: Counts that each movie is rated.\n  Returns:\n    movie_id_vocab: List of all movie ids paired with movie usage count, and\n      sorted by counts.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "write_tfrecords",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def write_tfrecords(tf_examples, filename):\n  \"\"\"Writes tf examples to tfrecord file, and returns the count.\"\"\"\n  with tf.io.TFRecordWriter(filename) as file_writer:\n    length = len(tf_examples)\n    progress_bar = tf.keras.utils.Progbar(length)\n    for example in tf_examples:\n      file_writer.write(example.SerializeToString())\n      progress_bar.add(1)\n    return length\ndef write_vocab_json(vocab, filename):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "write_vocab_json",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def write_vocab_json(vocab, filename):\n  \"\"\"Write generated movie vocabulary to specified file.\"\"\"\n  with open(filename, \"w\", encoding=\"utf-8\") as jsonfile:\n    json.dump(vocab, jsonfile, indent=2)\ndef write_vocab_txt(vocab, filename):\n  with open(filename, \"w\", encoding=\"utf-8\") as f:\n    for item in vocab:\n      f.write(str(item) + \"\\n\")\ndef generate_datasets(extracted_data_dir,\n                      output_dir,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "write_vocab_txt",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def write_vocab_txt(vocab, filename):\n  with open(filename, \"w\", encoding=\"utf-8\") as f:\n    for item in vocab:\n      f.write(str(item) + \"\\n\")\ndef generate_datasets(extracted_data_dir,\n                      output_dir,\n                      min_timeline_length,\n                      max_context_length,\n                      max_context_movie_genre_length,\n                      min_rating=None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "generate_datasets",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def generate_datasets(extracted_data_dir,\n                      output_dir,\n                      min_timeline_length,\n                      max_context_length,\n                      max_context_movie_genre_length,\n                      min_rating=None,\n                      build_vocabs=True,\n                      train_data_fraction=0.9,\n                      train_filename=OUTPUT_TRAINING_DATA_FILENAME,\n                      test_filename=OUTPUT_TESTING_DATA_FILENAME,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "def main(_):\n  logging.info(\"Downloading and extracting data.\")\n  extracted_data_dir = download_and_extract_data(data_directory=FLAGS.data_dir)\n  stats = generate_datasets(\n      extracted_data_dir=extracted_data_dir,\n      output_dir=FLAGS.output_dir,\n      min_timeline_length=FLAGS.min_timeline_length,\n      max_context_length=FLAGS.max_context_length,\n      max_context_movie_genre_length=FLAGS.max_context_movie_genre_length,\n      min_rating=FLAGS.min_rating,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "FLAGS = flags.FLAGS\n# Permalinks to download movielens data.\nMOVIELENS_1M_URL = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\nMOVIELENS_ZIP_FILENAME = \"ml-1m.zip\"\nMOVIELENS_ZIP_HASH = \"a6898adb50b9ca05aa231689da44c217cb524e7ebd39d264c56e2832f2c54e20\"\nMOVIELENS_EXTRACTED_DIR = \"ml-1m\"\nRATINGS_FILE_NAME = \"ratings.dat\"\nMOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "MOVIELENS_1M_URL",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "MOVIELENS_1M_URL = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\nMOVIELENS_ZIP_FILENAME = \"ml-1m.zip\"\nMOVIELENS_ZIP_HASH = \"a6898adb50b9ca05aa231689da44c217cb524e7ebd39d264c56e2832f2c54e20\"\nMOVIELENS_EXTRACTED_DIR = \"ml-1m\"\nRATINGS_FILE_NAME = \"ratings.dat\"\nMOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "MOVIELENS_ZIP_FILENAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "MOVIELENS_ZIP_FILENAME = \"ml-1m.zip\"\nMOVIELENS_ZIP_HASH = \"a6898adb50b9ca05aa231689da44c217cb524e7ebd39d264c56e2832f2c54e20\"\nMOVIELENS_EXTRACTED_DIR = \"ml-1m\"\nRATINGS_FILE_NAME = \"ratings.dat\"\nMOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "MOVIELENS_ZIP_HASH",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "MOVIELENS_ZIP_HASH = \"a6898adb50b9ca05aa231689da44c217cb524e7ebd39d264c56e2832f2c54e20\"\nMOVIELENS_EXTRACTED_DIR = \"ml-1m\"\nRATINGS_FILE_NAME = \"ratings.dat\"\nMOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "MOVIELENS_EXTRACTED_DIR",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "MOVIELENS_EXTRACTED_DIR = \"ml-1m\"\nRATINGS_FILE_NAME = \"ratings.dat\"\nMOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "RATINGS_FILE_NAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "RATINGS_FILE_NAME = \"ratings.dat\"\nMOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "MOVIES_FILE_NAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "MOVIES_FILE_NAME = \"movies.dat\"\nRATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "RATINGS_DATA_COLUMNS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "RATINGS_DATA_COLUMNS = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\nMOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "MOVIES_DATA_COLUMNS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "MOVIES_DATA_COLUMNS = [\"MovieID\", \"Title\", \"Genres\"]\nOUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_TRAINING_DATA_FILENAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_TRAINING_DATA_FILENAME = \"train_movielens_1m.tfrecord\"\nOUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_TESTING_DATA_FILENAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_TESTING_DATA_FILENAME = \"test_movielens_1m.tfrecord\"\nOUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_MOVIE_VOCAB_FILENAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_MOVIE_VOCAB_FILENAME = \"movie_vocab.json\"\nOUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_MOVIE_YEAR_VOCAB_FILENAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_MOVIE_YEAR_VOCAB_FILENAME = \"movie_year_vocab.txt\"\nOUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_MOVIE_GENRE_VOCAB_FILENAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_MOVIE_GENRE_VOCAB_FILENAME = \"movie_genre_vocab.txt\"\nOUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_MOVIE_TITLE_UNIGRAM_VOCAB_FILENAME = \"movie_title_unigram_vocab.txt\"\nOUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "OUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "OUTPUT_MOVIE_TITLE_BIGRAM_VOCAB_FILENAME = \"movie_title_bigram_vocab.txt\"\nPAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "PAD_MOVIE_ID",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "PAD_MOVIE_ID = 0\nPAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "PAD_RATING",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "PAD_RATING = 0.0\nPAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")\n  flags.DEFINE_string(\"output_dir\", None,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "PAD_MOVIE_YEAR",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "PAD_MOVIE_YEAR = 0\nUNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")\n  flags.DEFINE_string(\"output_dir\", None,\n                      \"Path to the directory of output files.\")",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "UNKNOWN_STR",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "UNKNOWN_STR = \"UNK\"\nVOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")\n  flags.DEFINE_string(\"output_dir\", None,\n                      \"Path to the directory of output files.\")\n  flags.DEFINE_bool(\"build_vocabs\", True,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "VOCAB_MOVIE_ID_INDEX",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "VOCAB_MOVIE_ID_INDEX = 0\nVOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")\n  flags.DEFINE_string(\"output_dir\", None,\n                      \"Path to the directory of output files.\")\n  flags.DEFINE_bool(\"build_vocabs\", True,\n                    \"If yes, generate movie feature vocabs.\")",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "VOCAB_COUNT_INDEX",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "peekOfCode": "VOCAB_COUNT_INDEX = 3\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string(\"data_dir\", \"/tmp\",\n                      \"Path to download and store movielens data.\")\n  flags.DEFINE_string(\"output_dir\", None,\n                      \"Path to the directory of output files.\")\n  flags.DEFINE_bool(\"build_vocabs\", True,\n                    \"If yes, generate movie feature vocabs.\")\n  flags.DEFINE_integer(\"min_timeline_length\", 3,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens",
        "documentation": {}
    },
    {
        "label": "ExampleGenerationMovielensTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "peekOfCode": "class ExampleGenerationMovielensTest(tf.test.TestCase):\n  def test_example_generation(self):\n    timelines, _ = example_gen.convert_to_timelines(RATINGS_DF)\n    train_examples, test_examples = example_gen.generate_examples_from_timelines(\n        timelines=timelines,\n        movies_df=MOVIES_DF,\n        min_timeline_len=2,\n        max_context_len=5,\n        max_context_movie_genre_len=10,\n        train_data_fraction=0.66,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "documentation": {}
    },
    {
        "label": "MOVIES_DF",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "peekOfCode": "MOVIES_DF = pd.DataFrame([\n    {\n        'MovieId': int(1),\n        'Title': 'Toy Story (1995)',\n        'Genres': 'Animation|Children|Comedy'\n    },\n    {\n        'MovieId': int(2),\n        'Title': 'Four Weddings and a Funeral (1994)',\n        'Genres': 'Comedy|Romance'",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "documentation": {}
    },
    {
        "label": "RATINGS_DF",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "peekOfCode": "RATINGS_DF = pd.DataFrame([\n    {\n        'UserID': int(1),\n        'MovieId': int(1),\n        'Rating': 3.5,\n        'Timestamp': 0\n    },\n    {\n        'UserID': int(1),\n        'MovieId': int(2),",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "documentation": {}
    },
    {
        "label": "EXAMPLE1",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "peekOfCode": "EXAMPLE1 = text_format.Parse(\n    \"\"\"\n    features {\n        feature {\n          key: \"context_movie_id\"\n          value {\n            int64_list {\n              value: [1, 0, 0, 0, 0]\n            }\n          }",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "documentation": {}
    },
    {
        "label": "EXAMPLE2",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "peekOfCode": "EXAMPLE2 = text_format.Parse(\n    \"\"\"\n    features {\n        feature {\n          key: \"context_movie_id\"\n          value {\n            int64_list {\n              value: [1, 2, 0, 0, 0]\n            }\n          }",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "documentation": {}
    },
    {
        "label": "EXAMPLE3",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "peekOfCode": "EXAMPLE3 = text_format.Parse(\n    \"\"\"\n    features {\n        feature {\n          key: \"context_movie_id\"\n          value {\n            int64_list {\n              value: [1, 2, 3, 0, 0]\n            }\n          }",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.data.example_generation_movielens_test",
        "documentation": {}
    },
    {
        "label": "ContextEncoder",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.context_encoder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.context_encoder",
        "peekOfCode": "class ContextEncoder(tf.keras.layers.Layer):\n  \"\"\"Layer to encode context sequence.\n  This encoder layer supports three types: 1) bow: bag of words style averaging\n  sequence embeddings. 2) cnn: use convolutional neural network to encode\n  sequence. 3) rnn: use recurrent neural network to encode sequence.\n  This encoder should be initialized with a predefined embedding layer, encoder\n  type and necessary parameters corresponding to the encoder type.\n  \"\"\"\n  def __init__(self,\n               input_config: input_config_pb2.InputConfig,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.context_encoder",
        "documentation": {}
    },
    {
        "label": "FeatureGroupEncoder",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.context_encoder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.context_encoder",
        "peekOfCode": "class FeatureGroupEncoder(tf.keras.layers.Layer):\n  \"\"\"Layer to generate encoding for the feature group.\n  Layer to generate encoding for the group of features in the feature\n  group with encoder type (BOW/CNN/LSTM) specified in the feature group config.\n  Embeddings of INT or STRING type features are concatenated first, and FLOAT\n  type feature values are appended after. Embedding vector is properly masked.\n  \"\"\"\n  def __init__(self,\n               feature_group: input_config_pb2.FeatureGroup,\n               model_config: model_config_class.ModelConfig,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.context_encoder",
        "documentation": {}
    },
    {
        "label": "safe_div",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.context_encoder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.context_encoder",
        "peekOfCode": "def safe_div(x, y):\n  return tf.where(tf.not_equal(y, 0), tf.divide(x, y), tf.zeros_like(x))\nclass ContextEncoder(tf.keras.layers.Layer):\n  \"\"\"Layer to encode context sequence.\n  This encoder layer supports three types: 1) bow: bag of words style averaging\n  sequence embeddings. 2) cnn: use convolutional neural network to encode\n  sequence. 3) rnn: use recurrent neural network to encode sequence.\n  This encoder should be initialized with a predefined embedding layer, encoder\n  type and necessary parameters corresponding to the encoder type.\n  \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.context_encoder",
        "documentation": {}
    },
    {
        "label": "ContextEncoderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.context_encoder_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.context_encoder_test",
        "peekOfCode": "class ContextEncoderTest(tf.test.TestCase):\n  def _create_test_feature_group(self,\n                                 encoder_type: input_config_pb2.EncoderType):\n    \"\"\"Prepare test feature group.\"\"\"\n    feature_context_movie_id = input_config_pb2.Feature(\n        feature_name='context_movie_id',\n        feature_type=input_config_pb2.FeatureType.INT,\n        vocab_size=3952,\n        embedding_dim=4)\n    feature_context_movie_rating = input_config_pb2.Feature(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.context_encoder_test",
        "documentation": {}
    },
    {
        "label": "DotProductSimilarity",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.dotproduct_similarity",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.dotproduct_similarity",
        "peekOfCode": "class DotProductSimilarity(tf.keras.layers.Layer):\n  \"\"\"Layer to comput dotproduct similarities for context/label embedding.\n    The top_k is an integer to represent top_k ids to compute among label ids.\n    if top_k is None, top_k computation will be ignored.\n  \"\"\"\n  def call(self,\n           context_embeddings: tf.Tensor,\n           label_embeddings: tf.Tensor,\n           top_k: Optional[int] = None):\n    \"\"\"Generate dotproduct similarity matrix and top values/indices.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.dotproduct_similarity",
        "documentation": {}
    },
    {
        "label": "DotproductSimilarityTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.dotproduct_similarity_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.dotproduct_similarity_test",
        "peekOfCode": "class DotproductSimilarityTest(tf.test.TestCase):\n  def test_dotproduct(self):\n    context_embeddings = tf.constant([[0.1, 0.1, 0.1, 0.1],\n                                      [0.2, 0.2, 0.2, 0.2]])\n    label_embeddings = tf.constant([[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.1, 0.1]])\n    similarity_layer = dotproduct_similarity.DotProductSimilarity()\n    dotproduct = similarity_layer(context_embeddings, label_embeddings)\n    dotproduct, top_ids, top_scores = similarity_layer(context_embeddings,\n                                                       label_embeddings, 1)\n    self.assertAllClose(tf.constant([[0.04, 0.04], [0.08, 0.08]]), dotproduct)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.dotproduct_similarity_test",
        "documentation": {}
    },
    {
        "label": "FeaturesAndVocabsByName",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "peekOfCode": "class FeaturesAndVocabsByName(\n    collections.namedtuple(\n        'FeaturesAndVocabsByName', ['features_by_name', 'vocabs_by_name'])):\n  \"\"\"Holder for intermediate data in input processing pipeline.\"\"\"\n  __slots__ = ()\n  def __new__(cls, features_by_name=None, vocabs_by_name=None):\n    return super(FeaturesAndVocabsByName, cls).__new__(cls,\n                                                       features_by_name,\n                                                       vocabs_by_name)\ndef _prepare_feature_vocab_table(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "get_features_and_vocabs_by_name",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "peekOfCode": "def get_features_and_vocabs_by_name(\n    input_config: input_config_pb2.InputConfig,\n    vocab_file_dir: str = '') -> FeaturesAndVocabsByName:\n  \"\"\"Get feature and vocabulary dictionaries according to input config.\n  Args:\n    input_config: The input config input_config_pb2.InputConfig proto.\n    vocab_file_dir: The directory storing vocabulary files.\n  Returns:\n    A FeaturesAndVocabsByName object containing features and vocabs\n    dictionaries keyed feature name for all features according to input",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "get_serving_input_specs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "peekOfCode": "def get_serving_input_specs(\n    input_config: input_config_pb2.InputConfig) -> Dict[str, tf.TensorSpec]:\n  features_by_name = get_features_and_vocabs_by_name(\n      input_config).features_by_name\n  input_specs = collections.OrderedDict()\n  for feature_name, feature in sorted(features_by_name.items()):\n    input_specs[feature_name] = _get_serving_feature_spec(\n        feature_name, feature.feature_type, feature.feature_length)\n  return input_specs\ndef decode_example(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "decode_example",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "peekOfCode": "def decode_example(\n    serialized_proto: str, features_and_vocabs_by_name: FeaturesAndVocabsByName,\n    label_feature_name: str) -> Tuple[Dict[str, tf.Tensor], tf.Tensor]:\n  \"\"\"Decode single serialized example.\n  Decode single serialized example, accoring to specified features in input\n  config. Perform vocabulary lookup if vocabulary is specified.\n  Args:\n    serialized_proto: The serialized proto that needs to be decoded.\n    features_and_vocabs_by_name: A FeaturesAndVocabsByName object containing\n      features and vocabs dictionaries by feature names.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "get_input_dataset",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "peekOfCode": "def get_input_dataset(data_filepattern: str,\n                      input_config: input_config_pb2.InputConfig,\n                      vocab_file_dir: str,\n                      batch_size: int) -> tf.data.Dataset:\n  \"\"\"An input_fn to create input datasets.\n  Args:\n    data_filepattern: The file pattern of the input data.\n    input_config: The input config input_config_pb2.InputConfig proto.\n    vocab_file_dir: The path to the directory storing the vocabulary files.\n    batch_size: Batch size of to-be generated dataset.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "INT_DEFAULT_VALUE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "peekOfCode": "INT_DEFAULT_VALUE = 0\nSTRING_DEFAULT_VALUE = 'UNK'\nFLOAT_DEFAULT_VALUE = 0.0\nclass FeaturesAndVocabsByName(\n    collections.namedtuple(\n        'FeaturesAndVocabsByName', ['features_by_name', 'vocabs_by_name'])):\n  \"\"\"Holder for intermediate data in input processing pipeline.\"\"\"\n  __slots__ = ()\n  def __new__(cls, features_by_name=None, vocabs_by_name=None):\n    return super(FeaturesAndVocabsByName, cls).__new__(cls,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "STRING_DEFAULT_VALUE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "peekOfCode": "STRING_DEFAULT_VALUE = 'UNK'\nFLOAT_DEFAULT_VALUE = 0.0\nclass FeaturesAndVocabsByName(\n    collections.namedtuple(\n        'FeaturesAndVocabsByName', ['features_by_name', 'vocabs_by_name'])):\n  \"\"\"Holder for intermediate data in input processing pipeline.\"\"\"\n  __slots__ = ()\n  def __new__(cls, features_by_name=None, vocabs_by_name=None):\n    return super(FeaturesAndVocabsByName, cls).__new__(cls,\n                                                       features_by_name,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "FLOAT_DEFAULT_VALUE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "peekOfCode": "FLOAT_DEFAULT_VALUE = 0.0\nclass FeaturesAndVocabsByName(\n    collections.namedtuple(\n        'FeaturesAndVocabsByName', ['features_by_name', 'vocabs_by_name'])):\n  \"\"\"Holder for intermediate data in input processing pipeline.\"\"\"\n  __slots__ = ()\n  def __new__(cls, features_by_name=None, vocabs_by_name=None):\n    return super(FeaturesAndVocabsByName, cls).__new__(cls,\n                                                       features_by_name,\n                                                       vocabs_by_name)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline",
        "documentation": {}
    },
    {
        "label": "InputPipelineTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline_test",
        "peekOfCode": "class InputPipelineTest(tf.test.TestCase):\n  def _AssertSparseTensorValueEqual(self, a, b):\n    self.assertAllEqual(a.indices, b.indices)\n    self.assertAllEqual(a.values, b.values)\n    self.assertAllEqual(a.dense_shape, b.dense_shape)\n  def setUp(self):\n    super(InputPipelineTest, self).setUp()\n    self.tmp_dir = self.create_tempdir()\n    self.test_movie_genre_vocab_file = os.path.join(self.tmp_dir,\n                                                    'movie_genre_vocab.txt')",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline_test",
        "documentation": {}
    },
    {
        "label": "FAKE_MOVIE_GENRE_VOCAB",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline_test",
        "peekOfCode": "FAKE_MOVIE_GENRE_VOCAB = [\n    'UNK', 'Comedy', 'Drama', 'Romance', 'Animation', 'Children'\n]\nTEST_INPUT_CONFIG = text_format.Parse(\n    \"\"\"\n    activity_feature_groups {\n      features {\n        feature_name: \"context_movie_id\"\n        feature_type: INT\n        vocab_size: 3952",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline_test",
        "documentation": {}
    },
    {
        "label": "TEST_INPUT_CONFIG",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline_test",
        "peekOfCode": "TEST_INPUT_CONFIG = text_format.Parse(\n    \"\"\"\n    activity_feature_groups {\n      features {\n        feature_name: \"context_movie_id\"\n        feature_type: INT\n        vocab_size: 3952\n        embedding_dim: 32\n        feature_length: 5\n      }",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline_test",
        "documentation": {}
    },
    {
        "label": "EXAMPLE1",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline_test",
        "peekOfCode": "EXAMPLE1 = text_format.Parse(\n    \"\"\"\n    features {\n        feature {\n          key: \"context_movie_id\"\n          value {\n            int64_list {\n              value: [1, 2, 0, 0, 0]\n            }\n          }",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.input_pipeline_test",
        "documentation": {}
    },
    {
        "label": "LabelEncoder",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.label_encoder",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.label_encoder",
        "peekOfCode": "class LabelEncoder(tf.keras.layers.Layer):\n  \"\"\"Layer to encode label feature.\n  Currently only id-based label encoder is supported. With this encoder, label\n  embedding layer will be created based on input config. And label embedding\n  will be generated by feeding input label feature to label embedding layer.\n  \"\"\"\n  def __init__(self,\n               input_config: input_config_pb2.InputConfig):\n    \"\"\"Initialize LabelEncoder layer based on input config.\n    Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.label_encoder",
        "documentation": {}
    },
    {
        "label": "LabelEncoderTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.label_encoder_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.label_encoder_test",
        "peekOfCode": "class LabelEncoderTest(tf.test.TestCase):\n  def _create_test_input_config(self):\n    \"\"\"Generate test input_config_pb2.InputConfig proto.\"\"\"\n    feature_context_movie_id = input_config_pb2.Feature(\n        feature_name='context_movie_id',\n        feature_type=input_config_pb2.FeatureType.INT,\n        vocab_size=3952,\n        embedding_dim=4)\n    feature_context_movie_rating = input_config_pb2.Feature(\n        feature_name='context_movie_rating',",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.label_encoder_test",
        "documentation": {}
    },
    {
        "label": "BatchSoftmax",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.losses",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.losses",
        "peekOfCode": "class BatchSoftmax(tf.keras.losses.Loss):\n  \"\"\"Compute batch softmax over batch similarities.\n  This softmax loss takes in-batch negatives without considering\n  negatives out of batch.\n  \"\"\"\n  def __init__(self, name='batch_softmax', **kwargs):\n    super(BatchSoftmax, self).__init__(name=name, **kwargs)\n  @tf.function\n  def call(self, y_true: tf.Tensor, y_pred: tf.Tensor):\n    \"\"\"Compute in batch softmax loss.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.losses",
        "documentation": {}
    },
    {
        "label": "GlobalSoftmax",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.losses",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.losses",
        "peekOfCode": "class GlobalSoftmax(tf.keras.losses.Loss):\n  \"\"\"Compute softmax over similarities.\n  This loss fuction computes softmax over similarities between context and\n  full vocab label embeddings, considering full label vocab non-label\n  predictions as negatives. This is currently the default loss in the model.\n  \"\"\"\n  def __init__(self, name='global_softmax', **kwargs):\n    super(GlobalSoftmax, self).__init__(name=name, **kwargs)\n  @tf.function\n  def call(self, y_true: tf.Tensor, y_pred: tf.Tensor):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.losses",
        "documentation": {}
    },
    {
        "label": "KerasLossesTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.losses_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.losses_test",
        "peekOfCode": "class KerasLossesTest(tf.test.TestCase):\n  def test_batch_softmax_loss(self):\n    batch_softmax = losses.BatchSoftmax()\n    true_label = tf.constant([[2], [0], [1]], dtype=tf.int32)\n    logits = tf.constant([\n        [0.8, 0.1, 0.2, 0.3],\n        [0.2, 0.7, 0.1, 0.5],\n        [0.5, 0.4, 0.9, 0.2]\n    ], dtype=tf.float32)\n    self.assertBetween(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.losses_test",
        "documentation": {}
    },
    {
        "label": "BatchRecall",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics",
        "peekOfCode": "class BatchRecall(tf.keras.metrics.Recall):\n  \"\"\"Compute batch recall for top_k.\"\"\"\n  def __init__(self, top_k=1, name='batch_recall'):\n    super().__init__(name=name, top_k=top_k)\n  def update_state(self, y_true, y_pred, sample_weight=None):\n    \"\"\"Update state of the metric.\n    Args:\n      y_true: the true labels with shape [batch_size, 1].\n      y_pred: model output, which is the similarity matrix with shape\n        [batch_size, label_embedding_vocab_size] between context and full vocab",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics",
        "documentation": {}
    },
    {
        "label": "GlobalRecall",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics",
        "peekOfCode": "class GlobalRecall(tf.keras.metrics.Recall):\n  \"\"\"Compute global recall for top_k.\"\"\"\n  def __init__(self, top_k=1, name='global_recall'):\n    super().__init__(name=name, top_k=top_k)\n  def update_state(self, y_true, y_pred, sample_weight=None):\n    \"\"\"Update state of the metric.\n    Args:\n      y_true: the true labels with shape [batch_size, 1].\n      y_pred: model output, which is the similarity matrix with shape\n        [batch_size, label_embedding_vocab_size] between context and full vocab",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics",
        "documentation": {}
    },
    {
        "label": "BatchMeanRank",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics",
        "peekOfCode": "class BatchMeanRank(tf.keras.metrics.Mean):\n  \"\"\"Keras metric computing mean rank of correct label within batch.\"\"\"\n  def __init__(self, name='batch_mean_rank', **kwargs):\n    super().__init__(name=name, **kwargs)\n  def update_state(self, y_true, y_pred, sample_weight=None):\n    \"\"\"Update state of the metric.\n    Args:\n      y_true: the true labels with shape [batch_size, 1].\n      y_pred: model output, which is the similarity matrix with shape\n        [batch_size, label_embedding_vocab_size] between context and full vocab",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics",
        "documentation": {}
    },
    {
        "label": "GlobalMeanRank",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics",
        "peekOfCode": "class GlobalMeanRank(tf.keras.metrics.Mean):\n  \"\"\"Keras metric computing mean rank of correct label globally.\"\"\"\n  def __init__(self, name='global_mean_rank', **kwargs):\n    super().__init__(name=name, **kwargs)\n  def update_state(self, y_true, y_pred, sample_weight=None):\n    \"\"\"Update state of the metric.\n    Args:\n      y_true: the true labels with shape [batch_size, 1].\n      y_pred: model output, which is the similarity matrix with shape\n        [batch_size, label_embedding_vocab_size] between context and full vocab",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics",
        "documentation": {}
    },
    {
        "label": "KerasMetricsTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics_test",
        "peekOfCode": "class KerasMetricsTest(tf.test.TestCase):\n  def test_batch_recall_and_mean_rank(self):\n    batch_recall = metrics.BatchRecall(top_k=2)\n    batch_mean_rank = metrics.BatchMeanRank()\n    true_label = tf.constant([[2], [0], [1]], dtype=tf.int32)\n    logits = tf.constant([\n        [0.8, 0.1, 1.1, 0.3],\n        [0.2, 0.7, 0.1, 0.5],\n        [0.7, 0.4, 0.9, 0.2]\n    ], dtype=tf.float32)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.metrics_test",
        "documentation": {}
    },
    {
        "label": "RecommendationModel",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model",
        "peekOfCode": "class RecommendationModel(tf.keras.Model):\n  \"\"\"Personalized dual-encoder style recommendation model.\"\"\"\n  def __init__(self,\n               input_config: input_config_pb2.InputConfig,\n               model_config: model_config_class.ModelConfig):\n    \"\"\"Initializes RecommendationModel according to input and model configs.\n    Takes in input and model configs to initialize the recommendation model.\n    Context encoder layer, label encoder layer and dotproduct similarity layer\n    will be prepared.\n    Args:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model",
        "documentation": {}
    },
    {
        "label": "SimpleCheckpoint",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "class SimpleCheckpoint(tf.keras.callbacks.Callback):\n  \"\"\"Keras callback to save tf.train.Checkpoints.\"\"\"\n  def __init__(self, checkpoint_manager):\n    super(SimpleCheckpoint, self).__init__()\n    self.checkpoint_manager = checkpoint_manager\n  def on_epoch_end(self, epoch, logs=None):\n    step_counter = self.checkpoint_manager._step_counter.numpy()  # pylint: disable=protected-access\n    self.checkpoint_manager.save(checkpoint_number=step_counter)\ndef _get_optimizer(learning_rate: float, gradient_clip_norm: float):\n  \"\"\"Gets model optimizer.\"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "define_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string('training_data_filepattern', None,\n                      'File pattern of the training data.')\n  flags.DEFINE_string('testing_data_filepattern', None,\n                      'File pattern of the training data.')\n  flags.DEFINE_string('model_dir', None, 'Directory to store checkpoints.')\n  flags.DEFINE_string('export_dir', None, 'Directory for the exported model.')\n  flags.DEFINE_integer('batch_size', 1, 'Training batch size.')\n  flags.DEFINE_float('learning_rate', 0.1, 'Learning rate.')",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "compile_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def compile_model(model, eval_top_k, learning_rate, gradient_clip_norm):\n  \"\"\"Compile keras model.\"\"\"\n  model.compile(\n      optimizer=_get_optimizer(\n          learning_rate=learning_rate, gradient_clip_norm=gradient_clip_norm),\n      loss=losses.GlobalSoftmax(),\n      metrics=_get_metrics(eval_top_k))\ndef build_keras_model(input_config: input_config_pb2.InputConfig,\n                      model_config: model_config_class.ModelConfig):\n  \"\"\"Construct and compile recommendation keras model.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "build_keras_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def build_keras_model(input_config: input_config_pb2.InputConfig,\n                      model_config: model_config_class.ModelConfig):\n  \"\"\"Construct and compile recommendation keras model.\n  Construct recommendation model according to input config and model config.\n  Compile the model with optimizer, loss function and eval metrics.\n  Args:\n    input_config: The configuration object(input_config_pb2.InputConfig) that\n      holds parameters for model input feature processing.\n    model_config: A ModelConfig object that holds parameters to set up the\n      model architecture.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "get_callbacks",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def get_callbacks(keras_model: tf.keras.Model,\n                  model_dir: str):\n  \"\"\"Sets up callbacks for training and evaluation.\"\"\"\n  summary_dir = os.path.join(model_dir, 'summaries')\n  summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)\n  checkpoint = tf.train.Checkpoint(\n      model=keras_model, optimizer=keras_model.optimizer)\n  checkpoint_manager = tf.train.CheckpointManager(\n      checkpoint,\n      directory=model_dir,",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "train_and_eval",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def train_and_eval(model: tf.keras.Model,\n                   model_dir: str,\n                   train_input_dataset: tf.data.Dataset,\n                   eval_input_dataset: tf.data.Dataset,\n                   steps_per_epoch: int,\n                   epochs: int,\n                   eval_steps: int):\n  \"\"\"Train and evaluate.\"\"\"\n  callbacks = get_callbacks(model, model_dir)\n  history = model.fit(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "save_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def save_model(checkpoint_path: str, export_dir: str,\n               input_config: input_config_pb2.InputConfig,\n               model_config: model_config_class.ModelConfig):\n  \"\"\"Export to savedmodel.\n  Args:\n    checkpoint_path: The path to the checkpoint that the model will be exported\n      based on.\n    export_dir: The directory to export models to.\n    input_config: The input config of the model.\n    model_config: The configuration to set up the model.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "export_tflite",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def export_tflite(export_dir):\n  \"\"\"Export to TFLite model.\n  Args:\n    export_dir: the model exportation dir, where saved_model is located.\n  \"\"\"\n  converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n  tflite_model = converter.convert()\n  tflite_model_path = os.path.join(export_dir, 'model.tflite')\n  with tf.io.gfile.GFile(tflite_model_path, 'wb') as f:\n    f.write(tflite_model)",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "export",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def export(checkpoint_path: str, input_config: input_config_pb2.InputConfig,\n           model_config: model_config_class.ModelConfig, export_dir: str):\n  \"\"\"Export to tensorflow saved model and TFLite model.\n  Args:\n    checkpoint_path: The path to the checkpoint that the model will be exported\n      based on.\n    input_config: The input config of the model.\n    model_config: The configuration to set up the model.\n    export_dir: The directory to store the exported model, If not set, model is\n      exported to the model_dir with timestamp.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "load_input_config",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def load_input_config():\n  \"\"\"Load input config.\"\"\"\n  assert FLAGS.input_config_file, 'input_config_file cannot be empty.'\n  with tf.io.gfile.GFile(FLAGS.input_config_file, 'rb') as reader:\n    return text_format.Parse(reader.read(), input_config_pb2.InputConfig())\ndef prepare_model_config():\n  \"\"\"Prepare model config.\"\"\"\n  return model_config_class.ModelConfig(\n      hidden_layer_dims=[int(x) for x in FLAGS.hidden_layer_dims],\n      eval_top_k=[int(x) for x in FLAGS.eval_top_k],",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "prepare_model_config",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def prepare_model_config():\n  \"\"\"Prepare model config.\"\"\"\n  return model_config_class.ModelConfig(\n      hidden_layer_dims=[int(x) for x in FLAGS.hidden_layer_dims],\n      eval_top_k=[int(x) for x in FLAGS.eval_top_k],\n      conv_num_filter_ratios=[int(x) for x in FLAGS.conv_num_filter_ratios],\n      conv_kernel_size=FLAGS.conv_kernel_size,\n      lstm_num_units=FLAGS.lstm_num_units,\n      num_predictions=FLAGS.num_predictions)\ndef main(_):",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "def main(_):\n  logger = tf.get_logger()\n  if not tf.io.gfile.exists(FLAGS.model_dir):\n    tf.io.gfile.mkdir(FLAGS.model_dir)\n  if not tf.io.gfile.exists(FLAGS.export_dir):\n    tf.io.gfile.mkdir(FLAGS.export_dir)\n  input_config = load_input_config()\n  model_config = prepare_model_config()\n  logger.info('Setting up train and eval input datasets.')\n  train_input_dataset = input_pipeline.get_input_dataset(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef define_flags():\n  \"\"\"Define flags.\"\"\"\n  flags.DEFINE_string('training_data_filepattern', None,\n                      'File pattern of the training data.')\n  flags.DEFINE_string('testing_data_filepattern', None,\n                      'File pattern of the training data.')\n  flags.DEFINE_string('model_dir', None, 'Directory to store checkpoints.')\n  flags.DEFINE_string('export_dir', None, 'Directory for the exported model.')\n  flags.DEFINE_integer('batch_size', 1, 'Training batch size.')",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher",
        "documentation": {}
    },
    {
        "label": "RecommendationModelLauncherTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "peekOfCode": "class RecommendationModelLauncherTest(tf.test.TestCase):\n  def _AssertSparseTensorValueEqual(self, a, b):\n    self.assertAllEqual(a.indices, b.indices)\n    self.assertAllEqual(a.values, b.values)\n    self.assertAllEqual(a.dense_shape, b.dense_shape)\n  def _assertInputDetail(self, input_details, index, name, shape):\n    self.assertEqual(name, input_details[index]['name'])\n    self.assertEqual(shape, input_details[index]['shape'])\n  def setUp(self):\n    super().setUp()",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "peekOfCode": "FLAGS = flags.FLAGS\nFAKE_MOVIE_GENRE_VOCAB = [\n    'UNK',\n    'Comedy',\n    'Drama',\n    'Romance',\n    'Animation',\n    'Children'\n]\nTEST_INPUT_CONFIG = \"\"\"",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "documentation": {}
    },
    {
        "label": "FAKE_MOVIE_GENRE_VOCAB",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "peekOfCode": "FAKE_MOVIE_GENRE_VOCAB = [\n    'UNK',\n    'Comedy',\n    'Drama',\n    'Romance',\n    'Animation',\n    'Children'\n]\nTEST_INPUT_CONFIG = \"\"\"\n    activity_feature_groups {",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "documentation": {}
    },
    {
        "label": "TEST_INPUT_CONFIG",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "peekOfCode": "TEST_INPUT_CONFIG = \"\"\"\n    activity_feature_groups {\n      features {\n        feature_name: \"context_movie_id\"\n        feature_type: INT\n        vocab_size: 3952\n        embedding_dim: 8\n        feature_length: 5\n      }\n      features {",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "documentation": {}
    },
    {
        "label": "EXAMPLE1",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "peekOfCode": "EXAMPLE1 = text_format.Parse(\n    \"\"\"\n    features {\n        feature {\n          key: \"context_movie_id\"\n          value {\n            int64_list {\n              value: [1, 2, 0, 0, 0]\n            }\n          }",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_launcher_test",
        "documentation": {}
    },
    {
        "label": "RecommendationModelTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_test",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_test",
        "peekOfCode": "class RecommendationModelTest(tf.test.TestCase):\n  def _create_test_input_config(self,\n                                encoder_type: input_config_pb2.EncoderType):\n    \"\"\"Generate test input_config_pb2.InputConfig proto.\"\"\"\n    feature_context_movie_id = input_config_pb2.Feature(\n        feature_name='context_movie_id',\n        feature_type=input_config_pb2.FeatureType.INT,\n        vocab_size=20,\n        embedding_dim=4)\n    feature_context_movie_rating = input_config_pb2.Feature(",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.recommendation_model_test",
        "documentation": {}
    },
    {
        "label": "GetShardFilenames",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.utils",
        "peekOfCode": "def GetShardFilenames(filepattern):\n  \"\"\"Get a list of filenames given a pattern.\n  This will also check whether the files exist on the filesystem. The pattern\n  can be either of the glob form, or the 'basename@num_shards' form.\n  Args:\n    filepattern: File pattern.\n  Returns:\n    A list of shard patterns.\n  Raises:\n    ValueError: if using the shard pattern, if some shards don't exist.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.utils",
        "documentation": {}
    },
    {
        "label": "ClipGradient",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.utils",
        "peekOfCode": "def ClipGradient(\n    grads_and_vars: Iterable[Tuple[TFGradient, tf.Variable]],\n    clip_val: Scalar = 1.0,\n    include_histogram_summary: bool = False\n) -> Tuple[Tuple[TFGradient, tf.Variable], ...]:\n  \"\"\"Clips all gradients by global norm, reducing norm to clip_val.\n  Args:\n    grads_and_vars: Gradients and vars list input.\n    clip_val: A 0-D (scalar) `Tensor` > 0. Value to clip to.\n    include_histogram_summary: Flag indicates adding clipped gradients to",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.utils",
        "documentation": {}
    },
    {
        "label": "TFGradient",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.utils",
        "peekOfCode": "TFGradient = TypeVar('TFGradient', tf.Tensor, tf.IndexedSlices)\nScalar = TypeVar('Scalar', tf.Variable, tf.Tensor, float, int)\ndef GetShardFilenames(filepattern):\n  \"\"\"Get a list of filenames given a pattern.\n  This will also check whether the files exist on the filesystem. The pattern\n  can be either of the glob form, or the 'basename@num_shards' form.\n  Args:\n    filepattern: File pattern.\n  Returns:\n    A list of shard patterns.",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.utils",
        "documentation": {}
    },
    {
        "label": "Scalar",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.utils",
        "description": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.utils",
        "peekOfCode": "Scalar = TypeVar('Scalar', tf.Variable, tf.Tensor, float, int)\ndef GetShardFilenames(filepattern):\n  \"\"\"Get a list of filenames given a pattern.\n  This will also check whether the files exist on the filesystem. The pattern\n  can be either of the glob form, or the 'basename@num_shards' form.\n  Args:\n    filepattern: File pattern.\n  Returns:\n    A list of shard patterns.\n  Raises:",
        "detail": "examples.tensorflow_examples.lite.model_maker.third_party.recommendation.ml.model.utils",
        "documentation": {}
    },
    {
        "label": "Dcgan",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan",
        "peekOfCode": "class Dcgan(object):\n  \"\"\"Dcgan class.\n  Args:\n    epochs: Number of epochs.\n    enable_function: If true, train step is decorated with tf.function.\n    batch_size: Batch size.\n  \"\"\"\n  def __init__(self, epochs, enable_function, batch_size):\n    self.epochs = epochs\n    self.enable_function = enable_function",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan",
        "documentation": {}
    },
    {
        "label": "scale",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan",
        "peekOfCode": "def scale(image, label):\n  image = tf.cast(image, tf.float32)\n  image = (image - 127.5) / 127.5\n  return image, label\ndef create_dataset(buffer_size, batch_size):\n  train_dataset = tfds.load(\n      'mnist', split='train', as_supervised=True, shuffle_files=True)\n  train_dataset = train_dataset.map(scale, num_parallel_calls=AUTOTUNE)\n  train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size)\n  return train_dataset",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan",
        "documentation": {}
    },
    {
        "label": "create_dataset",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan",
        "peekOfCode": "def create_dataset(buffer_size, batch_size):\n  train_dataset = tfds.load(\n      'mnist', split='train', as_supervised=True, shuffle_files=True)\n  train_dataset = train_dataset.map(scale, num_parallel_calls=AUTOTUNE)\n  train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size)\n  return train_dataset\ndef make_generator_model():\n  \"\"\"Generator.\n  Returns:\n    Keras Sequential model",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan",
        "documentation": {}
    },
    {
        "label": "make_generator_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan",
        "peekOfCode": "def make_generator_model():\n  \"\"\"Generator.\n  Returns:\n    Keras Sequential model\n  \"\"\"\n  model = tf.keras.Sequential([\n      tf.keras.layers.Dense(7*7*256, use_bias=False),\n      tf.keras.layers.BatchNormalization(),\n      tf.keras.layers.LeakyReLU(),\n      tf.keras.layers.Reshape((7, 7, 256)),",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan",
        "documentation": {}
    },
    {
        "label": "make_discriminator_model",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan",
        "peekOfCode": "def make_discriminator_model():\n  \"\"\"Discriminator.\n  Returns:\n    Keras Sequential model\n  \"\"\"\n  model = tf.keras.Sequential([\n      tf.keras.layers.Conv2D(64, 5, strides=(2, 2), padding='same'),\n      tf.keras.layers.LeakyReLU(),\n      tf.keras.layers.Dropout(0.3),\n      tf.keras.layers.Conv2D(128, 5, strides=(2, 2), padding='same'),",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan",
        "documentation": {}
    },
    {
        "label": "get_checkpoint_prefix",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan",
        "peekOfCode": "def get_checkpoint_prefix():\n  checkpoint_dir = './training_checkpoints'\n  checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n  return checkpoint_prefix\nclass Dcgan(object):\n  \"\"\"Dcgan class.\n  Args:\n    epochs: Number of epochs.\n    enable_function: If true, train step is decorated with tf.function.\n    batch_size: Batch size.",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan",
        "documentation": {}
    },
    {
        "label": "run_main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan",
        "peekOfCode": "def run_main(argv):\n  del argv\n  kwargs = {'epochs': FLAGS.epochs, 'enable_function': FLAGS.enable_function,\n            'buffer_size': FLAGS.buffer_size, 'batch_size': FLAGS.batch_size}\n  main(**kwargs)\ndef main(epochs, enable_function, buffer_size, batch_size):\n  train_dataset = create_dataset(buffer_size, batch_size)\n  checkpoint_pr = get_checkpoint_prefix()\n  dcgan_obj = Dcgan(epochs, enable_function, batch_size)\n  print ('Training ...')",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan",
        "peekOfCode": "def main(epochs, enable_function, buffer_size, batch_size):\n  train_dataset = create_dataset(buffer_size, batch_size)\n  checkpoint_pr = get_checkpoint_prefix()\n  dcgan_obj = Dcgan(epochs, enable_function, batch_size)\n  print ('Training ...')\n  return dcgan_obj.train(train_dataset, checkpoint_pr)\nif __name__ == '__main__':\n  app.run(run_main)",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan",
        "peekOfCode": "FLAGS = flags.FLAGS\nflags.DEFINE_integer('buffer_size', 10000, 'Shuffle buffer size')\nflags.DEFINE_integer('batch_size', 64, 'Batch Size')\nflags.DEFINE_integer('epochs', 1, 'Number of epochs')\nflags.DEFINE_boolean('enable_function', True, 'Enable Function?')\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ndef scale(image, label):\n  image = tf.cast(image, tf.float32)\n  image = (image - 127.5) / 127.5\n  return image, label",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan",
        "documentation": {}
    },
    {
        "label": "AUTOTUNE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan",
        "peekOfCode": "AUTOTUNE = tf.data.experimental.AUTOTUNE\ndef scale(image, label):\n  image = tf.cast(image, tf.float32)\n  image = (image - 127.5) / 127.5\n  return image, label\ndef create_dataset(buffer_size, batch_size):\n  train_dataset = tfds.load(\n      'mnist', split='train', as_supervised=True, shuffle_files=True)\n  train_dataset = train_dataset.map(scale, num_parallel_calls=AUTOTUNE)\n  train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size)",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan",
        "documentation": {}
    },
    {
        "label": "DcganTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan_test",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan_test",
        "peekOfCode": "class DcganTest(tf.test.TestCase):\n  def test_one_epoch_with_function(self):\n    epochs = 1\n    batch_size = 1\n    enable_function = True\n    input_image = tf.random.uniform((28, 28, 1))\n    label = tf.zeros((1,))\n    train_dataset = tf.data.Dataset.from_tensors(\n        (input_image, label)).batch(batch_size)\n    checkpoint_pr = dcgan.get_checkpoint_prefix()",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan_test",
        "documentation": {}
    },
    {
        "label": "DCGANBenchmark",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan_test",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan_test",
        "peekOfCode": "class DCGANBenchmark(tf.test.Benchmark):\n  def __init__(self, output_dir=None, **kwargs):\n    self.output_dir = output_dir\n  def benchmark_with_function(self):\n    kwargs = {\"epochs\": 6, \"enable_function\": True,\n              \"buffer_size\": 10000, \"batch_size\": 64}\n    self._run_and_report_benchmark(**kwargs)\n  def benchmark_without_function(self):\n    kwargs = {\"epochs\": 6, \"enable_function\": False,\n              \"buffer_size\": 10000, \"batch_size\": 64}",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan_test",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.dcgan.dcgan_test",
        "description": "examples.tensorflow_examples.models.dcgan.dcgan_test",
        "peekOfCode": "FLAGS = flags.FLAGS\nclass DcganTest(tf.test.TestCase):\n  def test_one_epoch_with_function(self):\n    epochs = 1\n    batch_size = 1\n    enable_function = True\n    input_image = tf.random.uniform((28, 28, 1))\n    label = tf.zeros((1,))\n    train_dataset = tf.data.Dataset.from_tensors(\n        (input_image, label)).batch(batch_size)",
        "detail": "examples.tensorflow_examples.models.dcgan.dcgan_test",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.densenet.densenet",
        "description": "examples.tensorflow_examples.models.densenet.densenet",
        "peekOfCode": "class ConvBlock(tf.keras.Model):\n  \"\"\"Convolutional Block consisting of (batchnorm->relu->conv).\n  Arguments:\n    num_filters: number of filters passed to a convolutional layer.\n    data_format: \"channels_first\" or \"channels_last\"\n    bottleneck: if True, then a 1x1 Conv is performed followed by 3x3 Conv.\n    weight_decay: weight decay\n    dropout_rate: dropout rate.\n  \"\"\"\n  def __init__(self, num_filters, data_format, bottleneck, weight_decay=1e-4,",
        "detail": "examples.tensorflow_examples.models.densenet.densenet",
        "documentation": {}
    },
    {
        "label": "TransitionBlock",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.densenet.densenet",
        "description": "examples.tensorflow_examples.models.densenet.densenet",
        "peekOfCode": "class TransitionBlock(tf.keras.Model):\n  \"\"\"Transition Block to reduce the number of features.\n  Arguments:\n    num_filters: number of filters passed to a convolutional layer.\n    data_format: \"channels_first\" or \"channels_last\"\n    weight_decay: weight decay\n    dropout_rate: dropout rate.\n  \"\"\"\n  def __init__(self, num_filters, data_format,\n               weight_decay=1e-4, dropout_rate=0):",
        "detail": "examples.tensorflow_examples.models.densenet.densenet",
        "documentation": {}
    },
    {
        "label": "DenseBlock",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.densenet.densenet",
        "description": "examples.tensorflow_examples.models.densenet.densenet",
        "peekOfCode": "class DenseBlock(tf.keras.Model):\n  \"\"\"Dense Block.\n  It consists of ConvBlocks where each block's output is concatenated\n  with its input.\n  Arguments:\n    num_layers: Number of layers in each block.\n    growth_rate: number of filters to add per conv block.\n    data_format: \"channels_first\" or \"channels_last\"\n    bottleneck: boolean, that decides which part of ConvBlock to call.\n    weight_decay: weight decay",
        "detail": "examples.tensorflow_examples.models.densenet.densenet",
        "documentation": {}
    },
    {
        "label": "DenseNet",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.densenet.densenet",
        "description": "examples.tensorflow_examples.models.densenet.densenet",
        "peekOfCode": "class DenseNet(tf.keras.Model):\n  \"\"\"Creating the Densenet Architecture.\n  Arguments:\n    mode: mode could be:\n        - from_depth: num_layers_in_each_block will be calculated from the depth\n                      and number of blocks.\n        - from_list: pass num_layers_in_each_block as a list. depth and number\n                     of blocks should not be specified\n        - from_integer: pass num_layers_in_each_block as an integer. Number of\n                        layers in each block will be calculated using",
        "detail": "examples.tensorflow_examples.models.densenet.densenet",
        "documentation": {}
    },
    {
        "label": "calc_from_depth",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.densenet.densenet",
        "description": "examples.tensorflow_examples.models.densenet.densenet",
        "peekOfCode": "def calc_from_depth(depth, num_blocks, bottleneck):\n  \"\"\"Calculate number of layers in each block from the depth.\n  Args:\n    depth: Depth of model\n    num_blocks: Number of dense blocks\n    bottleneck: If True, num_layers will be halved\n  Returns:\n    Number of layers in each block as a list\n  Raises:\n    ValueError: If depth or num_blocks is None and num_blocks is not 3.",
        "detail": "examples.tensorflow_examples.models.densenet.densenet",
        "documentation": {}
    },
    {
        "label": "calc_from_list",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.densenet.densenet",
        "description": "examples.tensorflow_examples.models.densenet.densenet",
        "peekOfCode": "def calc_from_list(depth, num_blocks, layers_per_block):\n  \"\"\"Calculate number of layers in each block.\n  Args:\n    depth: Depth of model\n    num_blocks: Number of dense blocks\n    layers_per_block: Number of layers per block as a list or tuple\n  Returns:\n    Number of layers in each block as a list\n  Raises:\n    ValueError: If depth or num_blocks is not None and",
        "detail": "examples.tensorflow_examples.models.densenet.densenet",
        "documentation": {}
    },
    {
        "label": "calc_from_integer",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.densenet.densenet",
        "description": "examples.tensorflow_examples.models.densenet.densenet",
        "peekOfCode": "def calc_from_integer(depth, num_blocks, layers_per_block):\n  \"\"\"Calculate number of layers in each block.\n  Args:\n    depth: Depth of model\n    num_blocks: Number of dense blocks\n    layers_per_block: Number of layers per block as an integer.\n  Returns:\n    Number of layers in each block as a list\n  Raises:\n    ValueError: If depth is not None and",
        "detail": "examples.tensorflow_examples.models.densenet.densenet",
        "documentation": {}
    },
    {
        "label": "l2",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.densenet.densenet",
        "description": "examples.tensorflow_examples.models.densenet.densenet",
        "peekOfCode": "l2 = tf.keras.regularizers.l2\ndef calc_from_depth(depth, num_blocks, bottleneck):\n  \"\"\"Calculate number of layers in each block from the depth.\n  Args:\n    depth: Depth of model\n    num_blocks: Number of dense blocks\n    bottleneck: If True, num_layers will be halved\n  Returns:\n    Number of layers in each block as a list\n  Raises:",
        "detail": "examples.tensorflow_examples.models.densenet.densenet",
        "documentation": {}
    },
    {
        "label": "DenseNetDistributedBenchmark",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.densenet.densenet_distributed_test",
        "description": "examples.tensorflow_examples.models.densenet.densenet_distributed_test",
        "peekOfCode": "class DenseNetDistributedBenchmark(tf.test.Benchmark):\n  def __init__(self, output_dir=None, **kwargs):\n    self.output_dir = output_dir\n  def benchmark_with_function_custom_loops(self):\n    kwargs = utils.get_cifar10_kwargs()\n    self._run_and_report_benchmark(**kwargs)\n  def benchmark_with_function_custom_loops_300_epochs_2_gpus(self):\n    kwargs = utils.get_cifar10_kwargs()\n    kwargs.update({'epochs': 300, 'data_format': 'channels_first',\n                   'bottleneck': False, 'compression': 1., 'num_gpu': 2,",
        "detail": "examples.tensorflow_examples.models.densenet.densenet_distributed_test",
        "documentation": {}
    },
    {
        "label": "DensenetTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.densenet.densenet_test",
        "description": "examples.tensorflow_examples.models.densenet.densenet_test",
        "peekOfCode": "class DensenetTest(tf.test.TestCase):\n  def test_one_epoch_with_function_custom_loop(self):\n    epochs = 1\n    enable_function = True\n    depth_of_model = 7\n    growth_rate = 2\n    num_of_blocks = 3\n    output_classes = 10\n    mode = 'from_depth'\n    data_format = 'channels_last'",
        "detail": "examples.tensorflow_examples.models.densenet.densenet_test",
        "documentation": {}
    },
    {
        "label": "DenseNetBenchmark",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.densenet.densenet_test",
        "description": "examples.tensorflow_examples.models.densenet.densenet_test",
        "peekOfCode": "class DenseNetBenchmark(tf.test.Benchmark):\n  def __init__(self, output_dir=None, **kwargs):\n    self.output_dir = output_dir\n  def benchmark_with_function_custom_loops(self):\n    kwargs = utils.get_cifar10_kwargs()\n    self._run_and_report_benchmark(**kwargs)\n  def benchmark_without_function_custom_loops(self):\n    kwargs = utils.get_cifar10_kwargs()\n    kwargs.update({'enable_function': False})\n    self._run_and_report_benchmark(**kwargs)",
        "detail": "examples.tensorflow_examples.models.densenet.densenet_test",
        "documentation": {}
    },
    {
        "label": "create_sample_dataset",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.densenet.densenet_test",
        "description": "examples.tensorflow_examples.models.densenet.densenet_test",
        "peekOfCode": "def create_sample_dataset(batch_size):\n  input_image = tf.random.uniform((32, 32, 3))\n  label = tf.zeros((1,))\n  dataset = tf.data.Dataset.from_tensors(\n      (input_image, label)).batch(batch_size)\n  return dataset\nclass DensenetTest(tf.test.TestCase):\n  def test_one_epoch_with_function_custom_loop(self):\n    epochs = 1\n    enable_function = True",
        "detail": "examples.tensorflow_examples.models.densenet.densenet_test",
        "documentation": {}
    },
    {
        "label": "Train",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.densenet.distributed_train",
        "description": "examples.tensorflow_examples.models.densenet.distributed_train",
        "peekOfCode": "class Train(object):\n  \"\"\"Train class.\n  Args:\n    epochs: Number of epochs\n    enable_function: If True, wraps the train_step and test_step in tf.function\n    model: Densenet model.\n    batch_size: Batch size.\n    strategy: Distribution strategy in use.\n  \"\"\"\n  def __init__(self, epochs, enable_function, model, batch_size, strategy):",
        "detail": "examples.tensorflow_examples.models.densenet.distributed_train",
        "documentation": {}
    },
    {
        "label": "run_main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.densenet.distributed_train",
        "description": "examples.tensorflow_examples.models.densenet.distributed_train",
        "peekOfCode": "def run_main(argv):\n  \"\"\"Passes the flags to main.\n  Args:\n    argv: argv\n  \"\"\"\n  del argv\n  kwargs = utils.flags_dict()\n  kwargs.update({'num_gpu': FLAGS.num_gpu})\n  main(**kwargs)\ndef main(epochs,",
        "detail": "examples.tensorflow_examples.models.densenet.distributed_train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.densenet.distributed_train",
        "description": "examples.tensorflow_examples.models.densenet.distributed_train",
        "peekOfCode": "def main(epochs,\n         enable_function,\n         buffer_size,\n         batch_size,\n         mode,\n         growth_rate,\n         output_classes,\n         depth_of_model=None,\n         num_of_blocks=None,\n         num_layers_in_each_block=None,",
        "detail": "examples.tensorflow_examples.models.densenet.distributed_train",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.densenet.distributed_train",
        "description": "examples.tensorflow_examples.models.densenet.distributed_train",
        "peekOfCode": "FLAGS = flags.FLAGS\n# if additional flags are needed, define it here.\nflags.DEFINE_integer('num_gpu', 1, 'Number of GPUs to use')\nclass Train(object):\n  \"\"\"Train class.\n  Args:\n    epochs: Number of epochs\n    enable_function: If True, wraps the train_step and test_step in tf.function\n    model: Densenet model.\n    batch_size: Batch size.",
        "detail": "examples.tensorflow_examples.models.densenet.distributed_train",
        "documentation": {}
    },
    {
        "label": "Train",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.densenet.train",
        "description": "examples.tensorflow_examples.models.densenet.train",
        "peekOfCode": "class Train(object):\n  \"\"\"Train class.\n  Args:\n    epochs: Number of epochs\n    enable_function: If True, wraps the train_step and test_step in tf.function\n    model: Densenet model.\n  \"\"\"\n  def __init__(self, epochs, enable_function, model):\n    self.epochs = epochs\n    self.enable_function = enable_function",
        "detail": "examples.tensorflow_examples.models.densenet.train",
        "documentation": {}
    },
    {
        "label": "run_main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.densenet.train",
        "description": "examples.tensorflow_examples.models.densenet.train",
        "peekOfCode": "def run_main(argv):\n  \"\"\"Passes the flags to main.\n  Args:\n    argv: argv\n  \"\"\"\n  del argv\n  kwargs = utils.flags_dict()\n  main(**kwargs)\ndef main(epochs,\n         enable_function,",
        "detail": "examples.tensorflow_examples.models.densenet.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.densenet.train",
        "description": "examples.tensorflow_examples.models.densenet.train",
        "peekOfCode": "def main(epochs,\n         enable_function,\n         buffer_size,\n         batch_size,\n         mode,\n         growth_rate,\n         output_classes,\n         depth_of_model=None,\n         num_of_blocks=None,\n         num_layers_in_each_block=None,",
        "detail": "examples.tensorflow_examples.models.densenet.train",
        "documentation": {}
    },
    {
        "label": "Preprocess",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.densenet.utils",
        "description": "examples.tensorflow_examples.models.densenet.utils",
        "peekOfCode": "class Preprocess(object):\n  \"\"\"Preprocess images.\n  Args:\n    data_format: channels_first or channels_last\n  \"\"\"\n  def __init__(self, data_format, train):\n    self._data_format = data_format\n    self._train = train\n  def __call__(self, image, label):\n    image = tf.cast(image, tf.float32)",
        "detail": "examples.tensorflow_examples.models.densenet.utils",
        "documentation": {}
    },
    {
        "label": "define_densenet_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.densenet.utils",
        "description": "examples.tensorflow_examples.models.densenet.utils",
        "peekOfCode": "def define_densenet_flags():\n  \"\"\"Defining all the necessary flags.\"\"\"\n  flags.DEFINE_integer('buffer_size', 50000, 'Shuffle buffer size')\n  flags.DEFINE_integer('batch_size', 64, 'Batch Size')\n  flags.DEFINE_integer('epochs', 1, 'Number of epochs')\n  flags.DEFINE_boolean('enable_function', True, 'Enable Function?')\n  flags.DEFINE_string('data_dir', None, 'Directory to store the dataset')\n  flags.DEFINE_string('mode', 'from_depth', 'Deciding how to build the model')\n  flags.DEFINE_integer('depth_of_model', 7, 'Number of layers in the model')\n  flags.DEFINE_integer('growth_rate', 12, 'Filters to add per dense block')",
        "detail": "examples.tensorflow_examples.models.densenet.utils",
        "documentation": {}
    },
    {
        "label": "create_dataset",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.densenet.utils",
        "description": "examples.tensorflow_examples.models.densenet.utils",
        "peekOfCode": "def create_dataset(buffer_size, batch_size, data_format, data_dir=None):\n  \"\"\"Creates a tf.data Dataset.\n  Args:\n    buffer_size: Shuffle buffer size.\n    batch_size: Batch size\n    data_format: channels_first or channels_last\n    data_dir: directory to store the dataset.\n  Returns:\n    train dataset, test dataset, metadata\n  \"\"\"",
        "detail": "examples.tensorflow_examples.models.densenet.utils",
        "documentation": {}
    },
    {
        "label": "flags_dict",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.densenet.utils",
        "description": "examples.tensorflow_examples.models.densenet.utils",
        "peekOfCode": "def flags_dict():\n  \"\"\"Define the flags.\n  Returns:\n    Command line arguments as Flags.\n  \"\"\"\n  kwargs = {\n      'epochs': FLAGS.epochs,\n      'enable_function': FLAGS.enable_function,\n      'buffer_size': FLAGS.buffer_size,\n      'batch_size': FLAGS.batch_size,",
        "detail": "examples.tensorflow_examples.models.densenet.utils",
        "documentation": {}
    },
    {
        "label": "get_cifar10_kwargs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.densenet.utils",
        "description": "examples.tensorflow_examples.models.densenet.utils",
        "peekOfCode": "def get_cifar10_kwargs():\n  return {'epochs': 1, 'enable_function': True, 'buffer_size': 50000,\n          'batch_size': 64, 'depth_of_model': 40, 'growth_rate': 12,\n          'num_of_blocks': 3, 'output_classes': 10, 'mode': 'from_depth',\n          'data_format': 'channels_last', 'dropout_rate': 0.}",
        "detail": "examples.tensorflow_examples.models.densenet.utils",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.densenet.utils",
        "description": "examples.tensorflow_examples.models.densenet.utils",
        "peekOfCode": "FLAGS = flags.FLAGS\ndef define_densenet_flags():\n  \"\"\"Defining all the necessary flags.\"\"\"\n  flags.DEFINE_integer('buffer_size', 50000, 'Shuffle buffer size')\n  flags.DEFINE_integer('batch_size', 64, 'Batch Size')\n  flags.DEFINE_integer('epochs', 1, 'Number of epochs')\n  flags.DEFINE_boolean('enable_function', True, 'Enable Function?')\n  flags.DEFINE_string('data_dir', None, 'Directory to store the dataset')\n  flags.DEFINE_string('mode', 'from_depth', 'Deciding how to build the model')\n  flags.DEFINE_integer('depth_of_model', 7, 'Number of layers in the model')",
        "detail": "examples.tensorflow_examples.models.densenet.utils",
        "documentation": {}
    },
    {
        "label": "AUTOTUNE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.densenet.utils",
        "description": "examples.tensorflow_examples.models.densenet.utils",
        "peekOfCode": "AUTOTUNE = tf.data.experimental.AUTOTUNE\nCIFAR_MEAN = [125.3, 123.0, 113.9]\nCIFAR_STD = [63.0, 62.1, 66.7]\nHEIGHT = 32\nWIDTH = 32\nclass Preprocess(object):\n  \"\"\"Preprocess images.\n  Args:\n    data_format: channels_first or channels_last\n  \"\"\"",
        "detail": "examples.tensorflow_examples.models.densenet.utils",
        "documentation": {}
    },
    {
        "label": "CIFAR_MEAN",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.densenet.utils",
        "description": "examples.tensorflow_examples.models.densenet.utils",
        "peekOfCode": "CIFAR_MEAN = [125.3, 123.0, 113.9]\nCIFAR_STD = [63.0, 62.1, 66.7]\nHEIGHT = 32\nWIDTH = 32\nclass Preprocess(object):\n  \"\"\"Preprocess images.\n  Args:\n    data_format: channels_first or channels_last\n  \"\"\"\n  def __init__(self, data_format, train):",
        "detail": "examples.tensorflow_examples.models.densenet.utils",
        "documentation": {}
    },
    {
        "label": "CIFAR_STD",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.densenet.utils",
        "description": "examples.tensorflow_examples.models.densenet.utils",
        "peekOfCode": "CIFAR_STD = [63.0, 62.1, 66.7]\nHEIGHT = 32\nWIDTH = 32\nclass Preprocess(object):\n  \"\"\"Preprocess images.\n  Args:\n    data_format: channels_first or channels_last\n  \"\"\"\n  def __init__(self, data_format, train):\n    self._data_format = data_format",
        "detail": "examples.tensorflow_examples.models.densenet.utils",
        "documentation": {}
    },
    {
        "label": "HEIGHT",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.densenet.utils",
        "description": "examples.tensorflow_examples.models.densenet.utils",
        "peekOfCode": "HEIGHT = 32\nWIDTH = 32\nclass Preprocess(object):\n  \"\"\"Preprocess images.\n  Args:\n    data_format: channels_first or channels_last\n  \"\"\"\n  def __init__(self, data_format, train):\n    self._data_format = data_format\n    self._train = train",
        "detail": "examples.tensorflow_examples.models.densenet.utils",
        "documentation": {}
    },
    {
        "label": "WIDTH",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.densenet.utils",
        "description": "examples.tensorflow_examples.models.densenet.utils",
        "peekOfCode": "WIDTH = 32\nclass Preprocess(object):\n  \"\"\"Preprocess images.\n  Args:\n    data_format: channels_first or channels_last\n  \"\"\"\n  def __init__(self, data_format, train):\n    self._data_format = data_format\n    self._train = train\n  def __call__(self, image, label):",
        "detail": "examples.tensorflow_examples.models.densenet.utils",
        "documentation": {}
    },
    {
        "label": "NmtDistributedTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.distributed_test",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.distributed_test",
        "peekOfCode": "class NmtDistributedTest(tf.test.TestCase):\n  def test_one_epoch_multi_device(self):\n    if tf.test.is_gpu_available():\n      print('Using 2 virtual GPUs.')\n      device = tf.config.experimental.list_physical_devices('GPU')[0]\n      tf.config.experimental.set_virtual_device_configuration(\n          device, [\n              tf.config.experimental.VirtualDeviceConfiguration(\n                  memory_limit=8192),\n              tf.config.experimental.VirtualDeviceConfiguration(",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.distributed_test",
        "documentation": {}
    },
    {
        "label": "NmtDistributedBenchmark",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.distributed_test",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.distributed_test",
        "peekOfCode": "class NmtDistributedBenchmark(tf.test.Benchmark):\n  def __init__(self, output_dir=None, **kwargs):\n    self.output_dir = output_dir\n  def benchmark_one_epoch_1_gpu(self):\n    kwargs = utils.get_common_kwargs()\n    kwargs.update({'enable_function': False})\n    self._run_and_report_benchmark(**kwargs)\n  def benchmark_one_epoch_1_gpu_function(self):\n    kwargs = utils.get_common_kwargs()\n    self._run_and_report_benchmark(**kwargs)",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.distributed_test",
        "documentation": {}
    },
    {
        "label": "DistributedTrain",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.distributed_train",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.distributed_train",
        "peekOfCode": "class DistributedTrain(Train):\n  \"\"\"Distributed Train class.\n  Attributes:\n    epochs: Number of epochs.\n    enable_function: Decorate function with tf.function.\n    encoder: Encoder.\n    decoder: Decoder.\n    inp_lang: Input language tokenizer.\n    targ_lang: Target language tokenizer.\n    batch_size: Batch size.",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.distributed_train",
        "documentation": {}
    },
    {
        "label": "run_main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.distributed_train",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.distributed_train",
        "peekOfCode": "def run_main(argv):\n  del argv\n  kwargs = utils.flags_dict()\n  main(**kwargs)\ndef main(epochs, enable_function, buffer_size, batch_size, download_path,\n         num_examples=70000, embedding_dim=256, enc_units=1024, dec_units=1024):\n  strategy = tf.distribute.MirroredStrategy()\n  num_replicas = strategy.num_replicas_in_sync\n  file_path = utils.download(download_path)\n  train_ds, test_ds, inp_lang, targ_lang = utils.create_dataset(",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.distributed_train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.distributed_train",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.distributed_train",
        "peekOfCode": "def main(epochs, enable_function, buffer_size, batch_size, download_path,\n         num_examples=70000, embedding_dim=256, enc_units=1024, dec_units=1024):\n  strategy = tf.distribute.MirroredStrategy()\n  num_replicas = strategy.num_replicas_in_sync\n  file_path = utils.download(download_path)\n  train_ds, test_ds, inp_lang, targ_lang = utils.create_dataset(\n      file_path, num_examples, buffer_size, batch_size)\n  with strategy.scope():\n    vocab_inp_size = len(inp_lang.word_index) + 1\n    vocab_tar_size = len(targ_lang.word_index) + 1",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.distributed_train",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.distributed_train",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.distributed_train",
        "peekOfCode": "FLAGS = flags.FLAGS\nclass DistributedTrain(Train):\n  \"\"\"Distributed Train class.\n  Attributes:\n    epochs: Number of epochs.\n    enable_function: Decorate function with tf.function.\n    encoder: Encoder.\n    decoder: Decoder.\n    inp_lang: Input language tokenizer.\n    targ_lang: Target language tokenizer.",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.distributed_train",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.nmt",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.nmt",
        "peekOfCode": "class Encoder(tf.keras.Model):\n  \"\"\"Encoder.\n  Args:\n    vocab_size: Vocabulary size.\n    embedding_dim: Embedding dimension.\n    enc_units: Number of encoder units.\n    batch_sz: Batch size.\n  \"\"\"\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n    super(Encoder, self).__init__()",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.nmt",
        "documentation": {}
    },
    {
        "label": "BahdanauAttention",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.nmt",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.nmt",
        "peekOfCode": "class BahdanauAttention(tf.keras.Model):\n  \"\"\"Bahdanau Attention.\n  Args:\n    units: Number of dense units.\n  \"\"\"\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.w1 = tf.keras.layers.Dense(units)\n    self.w2 = tf.keras.layers.Dense(units)\n    self.v = tf.keras.layers.Dense(1)",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.nmt",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.nmt",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.nmt",
        "peekOfCode": "class Decoder(tf.keras.Model):\n  \"\"\"Decoder.\n  Args:\n    vocab_size: Vocabulary size.\n    embedding_dim: Embedding dimension.\n    dec_units: Number of decoder units.\n  \"\"\"\n  def __init__(self, vocab_size, embedding_dim, dec_units):\n    super(Decoder, self).__init__()\n    self.dec_units = dec_units",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.nmt",
        "documentation": {}
    },
    {
        "label": "NmtTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.nmt_test",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.nmt_test",
        "peekOfCode": "class NmtTest(tf.test.TestCase):\n  def test_one_epoch(self):\n    num_examples = 10\n    buffer_size = 10\n    batch_size = 1\n    embedding_dim = 4\n    enc_units = 4\n    dec_units = 4\n    epochs = 1\n    train.main(epochs, True, buffer_size, batch_size, 'datasets', num_examples,",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.nmt_test",
        "documentation": {}
    },
    {
        "label": "NmtBenchmark",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.nmt_test",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.nmt_test",
        "peekOfCode": "class NmtBenchmark(tf.test.Benchmark):\n  def __init__(self, output_dir=None, **kwargs):\n    self.output_dir = output_dir\n  def benchmark_one_epoch(self):\n    kwargs = utils.get_common_kwargs()\n    self._run_and_report_benchmark(**kwargs)\n  def benchmark_ten_epochs(self):\n    kwargs = utils.get_common_kwargs()\n    kwargs.update({'epochs': 10})\n    self._run_and_report_benchmark(**kwargs)",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.nmt_test",
        "documentation": {}
    },
    {
        "label": "Train",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.train",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.train",
        "peekOfCode": "class Train(object):\n  \"\"\"Train class.\n  Attributes:\n    epochs: Number of epochs.\n    enable_function: Decorate function with tf.function.\n    encoder: Encoder.\n    decoder: Decoder.\n    inp_lang: Input language tokenizer.\n    targ_lang: Target language tokenizer.\n    batch_size: Batch size.",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.train",
        "documentation": {}
    },
    {
        "label": "run_main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.train",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.train",
        "peekOfCode": "def run_main(argv):\n  del argv\n  kwargs = utils.flags_dict()\n  main(**kwargs)\ndef main(epochs, enable_function, buffer_size, batch_size, download_path,\n         num_examples=70000, embedding_dim=256, enc_units=1024, dec_units=1024):\n  file_path = utils.download(download_path)\n  train_ds, test_ds, inp_lang, targ_lang = utils.create_dataset(\n      file_path, num_examples, buffer_size, batch_size)\n  vocab_inp_size = len(inp_lang.word_index) + 1",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.train",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.train",
        "peekOfCode": "def main(epochs, enable_function, buffer_size, batch_size, download_path,\n         num_examples=70000, embedding_dim=256, enc_units=1024, dec_units=1024):\n  file_path = utils.download(download_path)\n  train_ds, test_ds, inp_lang, targ_lang = utils.create_dataset(\n      file_path, num_examples, buffer_size, batch_size)\n  vocab_inp_size = len(inp_lang.word_index) + 1\n  vocab_tar_size = len(targ_lang.word_index) + 1\n  encoder = nmt.Encoder(vocab_inp_size, embedding_dim, enc_units, batch_size)\n  decoder = nmt.Decoder(vocab_tar_size, embedding_dim, dec_units)\n  train_obj = Train(epochs, enable_function, encoder, decoder,",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.train",
        "documentation": {}
    },
    {
        "label": "nmt_flags",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "def nmt_flags():\n  flags.DEFINE_string('download_path', 'datasets', 'Download folder')\n  flags.DEFINE_integer('buffer_size', 70000, 'Shuffle buffer size')\n  flags.DEFINE_integer('batch_size', 64, 'Batch Size')\n  flags.DEFINE_integer('epochs', 1, 'Number of epochs')\n  flags.DEFINE_integer('embedding_dim', 256, 'Embedding dimension')\n  flags.DEFINE_integer('enc_units', 1024, 'Encoder GRU units')\n  flags.DEFINE_integer('dec_units', 1024, 'Decoder GRU units')\n  flags.DEFINE_boolean('enable_function', True, 'Enable Function?')\n  flags.DEFINE_integer('num_examples', 70000, 'Number of examples from dataset')",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "download",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "def download(download_path):\n  path_to_zip = tf.keras.utils.get_file(\n      'spa-eng.zip', origin=_URL, cache_subdir=download_path,\n      extract=True)\n  path_to_file = os.path.join(os.path.dirname(path_to_zip), 'spa-eng/spa.txt')\n  return path_to_file\ndef unicode_to_ascii(s):\n  return ''.join(c for c in unicodedata.normalize('NFD', s)\n                 if unicodedata.category(c) != 'Mn')\ndef preprocess_sentence(w):",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "unicode_to_ascii",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "def unicode_to_ascii(s):\n  return ''.join(c for c in unicodedata.normalize('NFD', s)\n                 if unicodedata.category(c) != 'Mn')\ndef preprocess_sentence(w):\n  \"\"\"Preprocessing words in a sentence.\n  Args:\n    w: Word.\n  Returns:\n    Preprocessed words.\n  \"\"\"",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "preprocess_sentence",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "def preprocess_sentence(w):\n  \"\"\"Preprocessing words in a sentence.\n  Args:\n    w: Word.\n  Returns:\n    Preprocessed words.\n  \"\"\"\n  w = unicode_to_ascii(w.lower().strip())\n  # creating a space between a word and the punctuation following it\n  w = re.sub(r'([?.!,])', r' \\1 ', w)",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "create_word_pairs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "def create_word_pairs(path, num_examples):\n  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  # pylint: disable=g-complex-comprehension\n                for l in lines[:num_examples]]\n  return zip(*word_pairs)\ndef max_length(tensor):\n  return max(len(t) for t in tensor)\ndef tokenize(lang):\n  \"\"\"Tokenize the languages.\n  Args:",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "max_length",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "def max_length(tensor):\n  return max(len(t) for t in tensor)\ndef tokenize(lang):\n  \"\"\"Tokenize the languages.\n  Args:\n    lang: Language to be tokenized.\n  Returns:\n    tensor: Tensors generated after tokenization.\n    lang_tokenizer: tokenizer.\n  \"\"\"",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "def tokenize(lang):\n  \"\"\"Tokenize the languages.\n  Args:\n    lang: Language to be tokenized.\n  Returns:\n    tensor: Tensors generated after tokenization.\n    lang_tokenizer: tokenizer.\n  \"\"\"\n  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n      filters='')",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "def load_dataset(path, num_examples):\n  # creating cleaned input, output pairs\n  targ_lang, inp_lang = create_word_pairs(path, num_examples)\n  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\ndef create_dataset(path_to_file, num_examples, buffer_size, batch_size):\n  \"\"\"Create a tf.data Dataset.\n  Args:\n    path_to_file: Path to the file to load the text from.",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "create_dataset",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "def create_dataset(path_to_file, num_examples, buffer_size, batch_size):\n  \"\"\"Create a tf.data Dataset.\n  Args:\n    path_to_file: Path to the file to load the text from.\n    num_examples: Number of examples to sample.\n    buffer_size: Shuffle buffer size.\n    batch_size: Batch size.\n  Returns:\n    train_dataset: Training dataset.\n    test_dataset: Test dataset.",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "get_common_kwargs",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "def get_common_kwargs():\n  return {'epochs': 1, 'enable_function': True, 'buffer_size': 70000,\n          'batch_size': 64, 'download_path': 'datasets'}\ndef flags_dict():\n  \"\"\"Define the flags.\n  Returns:\n    Command line arguments as Flags.\n  \"\"\"\n  kwargs = {\n      'epochs': FLAGS.epochs,",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "flags_dict",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "def flags_dict():\n  \"\"\"Define the flags.\n  Returns:\n    Command line arguments as Flags.\n  \"\"\"\n  kwargs = {\n      'epochs': FLAGS.epochs,\n      'enable_function': FLAGS.enable_function,\n      'buffer_size': FLAGS.buffer_size,\n      'batch_size': FLAGS.batch_size,",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "FLAGS = flags.FLAGS\n_URL = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip'\ndef nmt_flags():\n  flags.DEFINE_string('download_path', 'datasets', 'Download folder')\n  flags.DEFINE_integer('buffer_size', 70000, 'Shuffle buffer size')\n  flags.DEFINE_integer('batch_size', 64, 'Batch Size')\n  flags.DEFINE_integer('epochs', 1, 'Number of epochs')\n  flags.DEFINE_integer('embedding_dim', 256, 'Embedding dimension')\n  flags.DEFINE_integer('enc_units', 1024, 'Encoder GRU units')\n  flags.DEFINE_integer('dec_units', 1024, 'Decoder GRU units')",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "_URL",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "description": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "peekOfCode": "_URL = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip'\ndef nmt_flags():\n  flags.DEFINE_string('download_path', 'datasets', 'Download folder')\n  flags.DEFINE_integer('buffer_size', 70000, 'Shuffle buffer size')\n  flags.DEFINE_integer('batch_size', 64, 'Batch Size')\n  flags.DEFINE_integer('epochs', 1, 'Number of epochs')\n  flags.DEFINE_integer('embedding_dim', 256, 'Embedding dimension')\n  flags.DEFINE_integer('enc_units', 1024, 'Encoder GRU units')\n  flags.DEFINE_integer('dec_units', 1024, 'Decoder GRU units')\n  flags.DEFINE_boolean('enable_function', True, 'Enable Function?')",
        "detail": "examples.tensorflow_examples.models.nmt_with_attention.utils",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.data_download",
        "description": "examples.tensorflow_examples.models.pix2pix.data_download",
        "peekOfCode": "def main(download_path):\n  path_to_zip = tf.keras.utils.get_file(\n      'facades.tar.gz', cache_subdir=download_path,\n      origin=_URL, extract=True)\n  path_to_folder = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n  return path_to_folder\nif __name__ == '__main__':\n  app.run(_main)",
        "detail": "examples.tensorflow_examples.models.pix2pix.data_download",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.pix2pix.data_download",
        "description": "examples.tensorflow_examples.models.pix2pix.data_download",
        "peekOfCode": "FLAGS = flags.FLAGS\nflags.DEFINE_string('download_path', 'datasets', 'Download folder')\n_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\ndef _main(argv):\n  del argv\n  download_path = FLAGS.download_path\n  main(download_path)\ndef main(download_path):\n  path_to_zip = tf.keras.utils.get_file(\n      'facades.tar.gz', cache_subdir=download_path,",
        "detail": "examples.tensorflow_examples.models.pix2pix.data_download",
        "documentation": {}
    },
    {
        "label": "_URL",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.pix2pix.data_download",
        "description": "examples.tensorflow_examples.models.pix2pix.data_download",
        "peekOfCode": "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\ndef _main(argv):\n  del argv\n  download_path = FLAGS.download_path\n  main(download_path)\ndef main(download_path):\n  path_to_zip = tf.keras.utils.get_file(\n      'facades.tar.gz', cache_subdir=download_path,\n      origin=_URL, extract=True)\n  path_to_folder = os.path.join(os.path.dirname(path_to_zip), 'facades/')",
        "detail": "examples.tensorflow_examples.models.pix2pix.data_download",
        "documentation": {}
    },
    {
        "label": "InstanceNormalization",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "class InstanceNormalization(tf.keras.layers.Layer):\n  \"\"\"Instance Normalization Layer (https://arxiv.org/abs/1607.08022).\"\"\"\n  def __init__(self, epsilon=1e-5):\n    super(InstanceNormalization, self).__init__()\n    self.epsilon = epsilon\n  def build(self, input_shape):\n    self.scale = self.add_weight(\n        name='scale',\n        shape=input_shape[-1:],\n        initializer=tf.random_normal_initializer(1., 0.02),",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "Pix2pix",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "class Pix2pix(object):\n  \"\"\"Pix2pix class.\n  Args:\n    epochs: Number of epochs.\n    enable_function: If true, train step is decorated with tf.function.\n    buffer_size: Shuffle buffer size..\n    batch_size: Batch size.\n  \"\"\"\n  def __init__(self, epochs, enable_function):\n    self.epochs = epochs",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "load",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def load(image_file):\n  \"\"\"Loads the image and generates input and target image.\n  Args:\n    image_file: .jpeg file\n  Returns:\n    Input image, target image\n  \"\"\"\n  image = tf.io.read_file(image_file)\n  image = tf.image.decode_jpeg(image)\n  w = tf.shape(image)[1]",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def resize(input_image, real_image, height, width):\n  input_image = tf.image.resize(input_image, [height, width],\n                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n  real_image = tf.image.resize(real_image, [height, width],\n                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n  return input_image, real_image\ndef random_crop(input_image, real_image):\n  stacked_image = tf.stack([input_image, real_image], axis=0)\n  cropped_image = tf.image.random_crop(\n      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "random_crop",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def random_crop(input_image, real_image):\n  stacked_image = tf.stack([input_image, real_image], axis=0)\n  cropped_image = tf.image.random_crop(\n      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n  return cropped_image[0], cropped_image[1]\ndef normalize(input_image, real_image):\n  input_image = (input_image / 127.5) - 1\n  real_image = (real_image / 127.5) - 1\n  return input_image, real_image\n@tf.function",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "normalize",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def normalize(input_image, real_image):\n  input_image = (input_image / 127.5) - 1\n  real_image = (real_image / 127.5) - 1\n  return input_image, real_image\n@tf.function\ndef random_jitter(input_image, real_image):\n  \"\"\"Random jittering.\n  Resizes to 286 x 286 and then randomly crops to IMG_HEIGHT x IMG_WIDTH.\n  Args:\n    input_image: Input Image",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "random_jitter",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def random_jitter(input_image, real_image):\n  \"\"\"Random jittering.\n  Resizes to 286 x 286 and then randomly crops to IMG_HEIGHT x IMG_WIDTH.\n  Args:\n    input_image: Input Image\n    real_image: Real Image\n  Returns:\n    Input Image, real image\n  \"\"\"\n  # resizing to 286 x 286 x 3",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "load_image_train",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def load_image_train(image_file):\n  input_image, real_image = load(image_file)\n  input_image, real_image = random_jitter(input_image, real_image)\n  input_image, real_image = normalize(input_image, real_image)\n  return input_image, real_image\ndef load_image_test(image_file):\n  input_image, real_image = load(image_file)\n  input_image, real_image = resize(input_image, real_image,\n                                   IMG_HEIGHT, IMG_WIDTH)\n  input_image, real_image = normalize(input_image, real_image)",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "load_image_test",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def load_image_test(image_file):\n  input_image, real_image = load(image_file)\n  input_image, real_image = resize(input_image, real_image,\n                                   IMG_HEIGHT, IMG_WIDTH)\n  input_image, real_image = normalize(input_image, real_image)\n  return input_image, real_image\ndef create_dataset(path_to_train_images, path_to_test_images, buffer_size,\n                   batch_size):\n  \"\"\"Creates a tf.data Dataset.\n  Args:",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "create_dataset",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def create_dataset(path_to_train_images, path_to_test_images, buffer_size,\n                   batch_size):\n  \"\"\"Creates a tf.data Dataset.\n  Args:\n    path_to_train_images: Path to train images folder.\n    path_to_test_images: Path to test images folder.\n    buffer_size: Shuffle buffer size.\n    batch_size: Batch size\n  Returns:\n    train dataset, test dataset",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "downsample",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def downsample(filters, size, norm_type='batchnorm', apply_norm=True):\n  \"\"\"Downsamples an input.\n  Conv2D => Batchnorm => LeakyRelu\n  Args:\n    filters: number of filters\n    size: filter size\n    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n    apply_norm: If True, adds the batchnorm layer\n  Returns:\n    Downsample Sequential Model",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "upsample",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n  \"\"\"Upsamples an input.\n  Conv2DTranspose => Batchnorm => Dropout => Relu\n  Args:\n    filters: number of filters\n    size: filter size\n    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n    apply_dropout: If True, adds the dropout layer\n  Returns:\n    Upsample Sequential Model",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "unet_generator",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def unet_generator(output_channels, norm_type='batchnorm'):\n  \"\"\"Modified u-net generator model (https://arxiv.org/abs/1611.07004).\n  Args:\n    output_channels: Output channels\n    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n  Returns:\n    Generator model\n  \"\"\"\n  down_stack = [\n      downsample(64, 4, norm_type, apply_norm=False),  # (bs, 128, 128, 64)",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "discriminator",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def discriminator(norm_type='batchnorm', target=True):\n  \"\"\"PatchGan discriminator model (https://arxiv.org/abs/1611.07004).\n  Args:\n    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n    target: Bool, indicating whether target image is an input or not.\n  Returns:\n    Discriminator model\n  \"\"\"\n  initializer = tf.random_normal_initializer(0., 0.02)\n  inp = tf.keras.layers.Input(shape=[None, None, 3], name='input_image')",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "get_checkpoint_prefix",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def get_checkpoint_prefix():\n  checkpoint_dir = './training_checkpoints'\n  checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n  return checkpoint_prefix\nclass Pix2pix(object):\n  \"\"\"Pix2pix class.\n  Args:\n    epochs: Number of epochs.\n    enable_function: If true, train step is decorated with tf.function.\n    buffer_size: Shuffle buffer size..",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "run_main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def run_main(argv):\n  del argv\n  kwargs = {'epochs': FLAGS.epochs, 'enable_function': FLAGS.enable_function,\n            'path': FLAGS.path, 'buffer_size': FLAGS.buffer_size,\n            'batch_size': FLAGS.batch_size}\n  main(**kwargs)\ndef main(epochs, enable_function, path, buffer_size, batch_size):\n  path_to_folder = path\n  pix2pix_object = Pix2pix(epochs, enable_function)\n  train_dataset, _ = create_dataset(",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "def main(epochs, enable_function, path, buffer_size, batch_size):\n  path_to_folder = path\n  pix2pix_object = Pix2pix(epochs, enable_function)\n  train_dataset, _ = create_dataset(\n      os.path.join(path_to_folder, 'train/*.jpg'),\n      os.path.join(path_to_folder, 'test/*.jpg'),\n      buffer_size, batch_size)\n  checkpoint_pr = get_checkpoint_prefix()\n  print ('Training ...')\n  return pix2pix_object.train(train_dataset, checkpoint_pr)",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "FLAGS = flags.FLAGS\nflags.DEFINE_integer('buffer_size', 400, 'Shuffle buffer size')\nflags.DEFINE_integer('batch_size', 1, 'Batch Size')\nflags.DEFINE_integer('epochs', 1, 'Number of epochs')\nflags.DEFINE_string('path', None, 'Path to the data folder')\nflags.DEFINE_boolean('enable_function', True, 'Enable Function?')\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ndef load(image_file):",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "IMG_WIDTH",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "IMG_WIDTH = 256\nIMG_HEIGHT = 256\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ndef load(image_file):\n  \"\"\"Loads the image and generates input and target image.\n  Args:\n    image_file: .jpeg file\n  Returns:\n    Input image, target image\n  \"\"\"",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "IMG_HEIGHT",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "IMG_HEIGHT = 256\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ndef load(image_file):\n  \"\"\"Loads the image and generates input and target image.\n  Args:\n    image_file: .jpeg file\n  Returns:\n    Input image, target image\n  \"\"\"\n  image = tf.io.read_file(image_file)",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "AUTOTUNE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "peekOfCode": "AUTOTUNE = tf.data.experimental.AUTOTUNE\ndef load(image_file):\n  \"\"\"Loads the image and generates input and target image.\n  Args:\n    image_file: .jpeg file\n  Returns:\n    Input image, target image\n  \"\"\"\n  image = tf.io.read_file(image_file)\n  image = tf.image.decode_jpeg(image)",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix",
        "documentation": {}
    },
    {
        "label": "Pix2PixTest",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix_test",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix_test",
        "peekOfCode": "class Pix2PixTest(tf.test.TestCase):\n  def test_one_step_with_function(self):\n    epochs = 1\n    batch_size = 1\n    enable_function = True\n    input_image = tf.random.uniform((256, 256, 3))\n    target_image = tf.random.uniform((256, 256, 3))\n    train_dataset = tf.data.Dataset.from_tensors(\n        (input_image, target_image)).map(pix2pix.random_jitter).batch(\n            batch_size)",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix_test",
        "documentation": {}
    },
    {
        "label": "Pix2PixBenchmark",
        "kind": 6,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix_test",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix_test",
        "peekOfCode": "class Pix2PixBenchmark(tf.test.Benchmark):\n  def __init__(self, output_dir=None, **kwargs):\n    self.output_dir = output_dir\n  def benchmark_with_function(self):\n    path = data_download.main(\"datasets\")\n    kwargs = {\"epochs\": 6, \"enable_function\": True, \"path\": path,\n              \"buffer_size\": 400, \"batch_size\": 1}\n    self._run_and_report_benchmark(**kwargs)\n  def benchmark_without_function(self):\n    path = data_download.main(\"datasets\")",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix_test",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.models.pix2pix.pix2pix_test",
        "description": "examples.tensorflow_examples.models.pix2pix.pix2pix_test",
        "peekOfCode": "FLAGS = flags.FLAGS\nclass Pix2PixTest(tf.test.TestCase):\n  def test_one_step_with_function(self):\n    epochs = 1\n    batch_size = 1\n    enable_function = True\n    input_image = tf.random.uniform((256, 256, 3))\n    target_image = tf.random.uniform((256, 256, 3))\n    train_dataset = tf.data.Dataset.from_tensors(\n        (input_image, target_image)).map(pix2pix.random_jitter).batch(",
        "detail": "examples.tensorflow_examples.models.pix2pix.pix2pix_test",
        "documentation": {}
    },
    {
        "label": "process_record_dataset",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "def process_record_dataset(dataset,\n                           is_training,\n                           batch_size,\n                           shuffle_buffer,\n                           parse_record_fn,\n                           num_epochs=1,\n                           dtype=tf.float32,\n                           datasets_num_private_threads=None,\n                           drop_remainder=False,\n                           tf_data_experimental_slack=False):",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "get_filenames",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "def get_filenames(is_training, data_dir):\n  \"\"\"Return filenames for dataset.\"\"\"\n  if is_training:\n    return [\n        os.path.join(data_dir, 'train-%05d-of-01024' % i)\n        for i in range(_NUM_TRAIN_FILES)]\n  else:\n    return [\n        os.path.join(data_dir, 'validation-%05d-of-00128' % i)\n        for i in range(128)]",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "parse_example_proto",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "def parse_example_proto(example_serialized):\n  \"\"\"Parses an Example proto containing a training example of an image.\n  The output of the build_image_data.py image preprocessing script is a dataset\n  containing serialized Example protocol buffers. Each Example proto contains\n  the following fields (values are included as examples):\n    image/height: 462\n    image/width: 581\n    image/colorspace: 'RGB'\n    image/channels: 3\n    image/class/label: 615",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "parse_record",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "def parse_record(raw_record, is_training, dtype):\n  \"\"\"Parses a record containing a training example of an image.\n  The input record is parsed into a label and image, and the image is passed\n  through preprocessing steps (cropping, flipping, and so on).\n  Args:\n    raw_record: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n    is_training: A boolean denoting whether the input is for training.\n    dtype: data type to use for images/features.\n  Returns:",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "get_parse_record_fn",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "def get_parse_record_fn(use_keras_image_data_format=False):\n  \"\"\"Get a function for parsing the records, accounting for image format.\n  This is useful by handling different types of Keras models. For instance,\n  the current resnet_model.resnet50 input format is always channel-last,\n  whereas the keras_applications mobilenet input format depends on\n  tf.keras.backend.image_data_format(). We should set\n  use_keras_image_data_format=False for the former and True for the latter.\n  Args:\n    use_keras_image_data_format: A boolean denoting whether data format is keras\n      backend image data format. If False, the image format is channel-last. If",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "input_fn",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "def input_fn(is_training,\n             data_dir,\n             batch_size,\n             num_epochs=1,\n             dtype=tf.float32,\n             datasets_num_private_threads=None,\n             parse_record_fn=parse_record,\n             input_context=None,\n             drop_remainder=False,\n             tf_data_experimental_slack=False,",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "preprocess_image",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "def preprocess_image(image_buffer, bbox, output_height, output_width,\n                     num_channels, is_training=False):\n  \"\"\"Preprocesses the given image.\n  Preprocessing includes decoding, cropping, and resizing for both training\n  and eval images. Training preprocessing, however, introduces some random\n  distortion of the image to improve accuracy.\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_SIZE",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "DEFAULT_IMAGE_SIZE = 224\nNUM_CHANNELS = 3\nNUM_CLASSES = 1001\nNUM_IMAGES = {\n    'train': 1281167,\n    'validation': 50000,\n}\n_NUM_TRAIN_FILES = 1024\n_SHUFFLE_BUFFER = 10000\n_R_MEAN = 123.68",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "NUM_CHANNELS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "NUM_CHANNELS = 3\nNUM_CLASSES = 1001\nNUM_IMAGES = {\n    'train': 1281167,\n    'validation': 50000,\n}\n_NUM_TRAIN_FILES = 1024\n_SHUFFLE_BUFFER = 10000\n_R_MEAN = 123.68\n_G_MEAN = 116.78",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "NUM_CLASSES",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "NUM_CLASSES = 1001\nNUM_IMAGES = {\n    'train': 1281167,\n    'validation': 50000,\n}\n_NUM_TRAIN_FILES = 1024\n_SHUFFLE_BUFFER = 10000\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "NUM_IMAGES",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "NUM_IMAGES = {\n    'train': 1281167,\n    'validation': 50000,\n}\n_NUM_TRAIN_FILES = 1024\n_SHUFFLE_BUFFER = 10000\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\nCHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "_NUM_TRAIN_FILES",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "_NUM_TRAIN_FILES = 1024\n_SHUFFLE_BUFFER = 10000\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\nCHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]\n# The lower bound for the smallest side of the image for aspect-preserving\n# resizing. For example, if an image is 500 x 1000, it will be resized to\n# _RESIZE_MIN x (_RESIZE_MIN * 2).\n_RESIZE_MIN = 256",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "_SHUFFLE_BUFFER",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "_SHUFFLE_BUFFER = 10000\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\nCHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]\n# The lower bound for the smallest side of the image for aspect-preserving\n# resizing. For example, if an image is 500 x 1000, it will be resized to\n# _RESIZE_MIN x (_RESIZE_MIN * 2).\n_RESIZE_MIN = 256\ndef process_record_dataset(dataset,",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "_R_MEAN",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\nCHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]\n# The lower bound for the smallest side of the image for aspect-preserving\n# resizing. For example, if an image is 500 x 1000, it will be resized to\n# _RESIZE_MIN x (_RESIZE_MIN * 2).\n_RESIZE_MIN = 256\ndef process_record_dataset(dataset,\n                           is_training,",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "_G_MEAN",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "_G_MEAN = 116.78\n_B_MEAN = 103.94\nCHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]\n# The lower bound for the smallest side of the image for aspect-preserving\n# resizing. For example, if an image is 500 x 1000, it will be resized to\n# _RESIZE_MIN x (_RESIZE_MIN * 2).\n_RESIZE_MIN = 256\ndef process_record_dataset(dataset,\n                           is_training,\n                           batch_size,",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "_B_MEAN",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "_B_MEAN = 103.94\nCHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]\n# The lower bound for the smallest side of the image for aspect-preserving\n# resizing. For example, if an image is 500 x 1000, it will be resized to\n# _RESIZE_MIN x (_RESIZE_MIN * 2).\n_RESIZE_MIN = 256\ndef process_record_dataset(dataset,\n                           is_training,\n                           batch_size,\n                           shuffle_buffer,",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "CHANNEL_MEANS",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "CHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]\n# The lower bound for the smallest side of the image for aspect-preserving\n# resizing. For example, if an image is 500 x 1000, it will be resized to\n# _RESIZE_MIN x (_RESIZE_MIN * 2).\n_RESIZE_MIN = 256\ndef process_record_dataset(dataset,\n                           is_training,\n                           batch_size,\n                           shuffle_buffer,\n                           parse_record_fn,",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "_RESIZE_MIN",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "description": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "peekOfCode": "_RESIZE_MIN = 256\ndef process_record_dataset(dataset,\n                           is_training,\n                           batch_size,\n                           shuffle_buffer,\n                           parse_record_fn,\n                           num_epochs=1,\n                           dtype=tf.float32,\n                           datasets_num_private_threads=None,\n                           drop_remainder=False,",
        "detail": "examples.tensorflow_examples.profiling.imagenet_preprocessing_ineffecient_input_pipeline",
        "documentation": {}
    },
    {
        "label": "change_keras_layer",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.profiling.resnet_model",
        "description": "examples.tensorflow_examples.profiling.resnet_model",
        "peekOfCode": "def change_keras_layer(use_tf_keras_layers=False):\n  \"\"\"Change layers to either tf.keras.layers or tf.python.keras.layers.\n  Layer version of  tf.keras.layers is depends on tensorflow version, but\n  tf.python.keras.layers checks environment variable TF2_BEHAVIOR.\n  This function is a temporal function to use tf.keras.layers.\n  Currently, tf v2 batchnorm layer is slower than tf v1 batchnorm layer.\n  this function is useful for tracking benchmark result for each version.\n  This function will be removed when we use tf.keras.layers as default.\n  TODO(b/146939027): Remove this function when tf v2 batchnorm reaches training\n  speed parity with tf v1 batchnorm.",
        "detail": "examples.tensorflow_examples.profiling.resnet_model",
        "documentation": {}
    },
    {
        "label": "identity_block",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.profiling.resnet_model",
        "description": "examples.tensorflow_examples.profiling.resnet_model",
        "peekOfCode": "def identity_block(input_tensor,\n                   kernel_size,\n                   filters,\n                   stage,\n                   block,\n                   use_l2_regularizer=True):\n  \"\"\"The identity block is the block that has no conv layer at shortcut.\n  Args:\n    input_tensor: input tensor\n    kernel_size: default 3, the kernel size of middle conv layer at main path",
        "detail": "examples.tensorflow_examples.profiling.resnet_model",
        "documentation": {}
    },
    {
        "label": "conv_block",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.profiling.resnet_model",
        "description": "examples.tensorflow_examples.profiling.resnet_model",
        "peekOfCode": "def conv_block(input_tensor,\n               kernel_size,\n               filters,\n               stage,\n               block,\n               strides=(2, 2),\n               use_l2_regularizer=True):\n  \"\"\"A block that has a conv layer at shortcut.\n  Note that from stage 3,\n  the second conv layer at main path is with strides=(2, 2)",
        "detail": "examples.tensorflow_examples.profiling.resnet_model",
        "documentation": {}
    },
    {
        "label": "resnet50",
        "kind": 2,
        "importPath": "examples.tensorflow_examples.profiling.resnet_model",
        "description": "examples.tensorflow_examples.profiling.resnet_model",
        "peekOfCode": "def resnet50(num_classes,\n             batch_size=None,\n             use_l2_regularizer=True,\n             rescale_inputs=False):\n  \"\"\"Instantiates the ResNet50 architecture.\n  Args:\n    num_classes: `int` number of classes for image classification.\n    batch_size: Size of the batches for each step.\n    use_l2_regularizer: whether to use L2 regularizer on Conv/Dense layer.\n    rescale_inputs: whether to rescale inputs from 0 to 1.",
        "detail": "examples.tensorflow_examples.profiling.resnet_model",
        "documentation": {}
    },
    {
        "label": "L2_WEIGHT_DECAY",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.resnet_model",
        "description": "examples.tensorflow_examples.profiling.resnet_model",
        "peekOfCode": "L2_WEIGHT_DECAY = 1e-4\nBATCH_NORM_DECAY = 0.9\nBATCH_NORM_EPSILON = 1e-5\nlayers = tf_python_keras_layers\ndef change_keras_layer(use_tf_keras_layers=False):\n  \"\"\"Change layers to either tf.keras.layers or tf.python.keras.layers.\n  Layer version of  tf.keras.layers is depends on tensorflow version, but\n  tf.python.keras.layers checks environment variable TF2_BEHAVIOR.\n  This function is a temporal function to use tf.keras.layers.\n  Currently, tf v2 batchnorm layer is slower than tf v1 batchnorm layer.",
        "detail": "examples.tensorflow_examples.profiling.resnet_model",
        "documentation": {}
    },
    {
        "label": "BATCH_NORM_DECAY",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.resnet_model",
        "description": "examples.tensorflow_examples.profiling.resnet_model",
        "peekOfCode": "BATCH_NORM_DECAY = 0.9\nBATCH_NORM_EPSILON = 1e-5\nlayers = tf_python_keras_layers\ndef change_keras_layer(use_tf_keras_layers=False):\n  \"\"\"Change layers to either tf.keras.layers or tf.python.keras.layers.\n  Layer version of  tf.keras.layers is depends on tensorflow version, but\n  tf.python.keras.layers checks environment variable TF2_BEHAVIOR.\n  This function is a temporal function to use tf.keras.layers.\n  Currently, tf v2 batchnorm layer is slower than tf v1 batchnorm layer.\n  this function is useful for tracking benchmark result for each version.",
        "detail": "examples.tensorflow_examples.profiling.resnet_model",
        "documentation": {}
    },
    {
        "label": "BATCH_NORM_EPSILON",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.resnet_model",
        "description": "examples.tensorflow_examples.profiling.resnet_model",
        "peekOfCode": "BATCH_NORM_EPSILON = 1e-5\nlayers = tf_python_keras_layers\ndef change_keras_layer(use_tf_keras_layers=False):\n  \"\"\"Change layers to either tf.keras.layers or tf.python.keras.layers.\n  Layer version of  tf.keras.layers is depends on tensorflow version, but\n  tf.python.keras.layers checks environment variable TF2_BEHAVIOR.\n  This function is a temporal function to use tf.keras.layers.\n  Currently, tf v2 batchnorm layer is slower than tf v1 batchnorm layer.\n  this function is useful for tracking benchmark result for each version.\n  This function will be removed when we use tf.keras.layers as default.",
        "detail": "examples.tensorflow_examples.profiling.resnet_model",
        "documentation": {}
    },
    {
        "label": "layers",
        "kind": 5,
        "importPath": "examples.tensorflow_examples.profiling.resnet_model",
        "description": "examples.tensorflow_examples.profiling.resnet_model",
        "peekOfCode": "layers = tf_python_keras_layers\ndef change_keras_layer(use_tf_keras_layers=False):\n  \"\"\"Change layers to either tf.keras.layers or tf.python.keras.layers.\n  Layer version of  tf.keras.layers is depends on tensorflow version, but\n  tf.python.keras.layers checks environment variable TF2_BEHAVIOR.\n  This function is a temporal function to use tf.keras.layers.\n  Currently, tf v2 batchnorm layer is slower than tf v1 batchnorm layer.\n  this function is useful for tracking benchmark result for each version.\n  This function will be removed when we use tf.keras.layers as default.\n  TODO(b/146939027): Remove this function when tf v2 batchnorm reaches training",
        "detail": "examples.tensorflow_examples.profiling.resnet_model",
        "documentation": {}
    },
    {
        "label": "nightly",
        "kind": 5,
        "importPath": "examples.setup",
        "description": "examples.setup",
        "peekOfCode": "nightly = False\nif '--nightly' in sys.argv:\n  nightly = True\n  sys.argv.remove('--nightly')\nproject_name = 'tensorflow-examples'\n# Get the current commit hash\nversion = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8')\nif nightly:\n  project_name = 'tensorflow-examples-nightly'\n  datestring = datetime.datetime.now().strftime('%Y%m%d%H%M')",
        "detail": "examples.setup",
        "documentation": {}
    },
    {
        "label": "project_name",
        "kind": 5,
        "importPath": "examples.setup",
        "description": "examples.setup",
        "peekOfCode": "project_name = 'tensorflow-examples'\n# Get the current commit hash\nversion = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8')\nif nightly:\n  project_name = 'tensorflow-examples-nightly'\n  datestring = datetime.datetime.now().strftime('%Y%m%d%H%M')\n  version = '%s-dev%s' % (version, datestring)\nDOCLINES = __doc__.split('\\n')\nREQUIRED_PKGS = [\n    'absl-py',",
        "detail": "examples.setup",
        "documentation": {}
    },
    {
        "label": "version",
        "kind": 5,
        "importPath": "examples.setup",
        "description": "examples.setup",
        "peekOfCode": "version = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8')\nif nightly:\n  project_name = 'tensorflow-examples-nightly'\n  datestring = datetime.datetime.now().strftime('%Y%m%d%H%M')\n  version = '%s-dev%s' % (version, datestring)\nDOCLINES = __doc__.split('\\n')\nREQUIRED_PKGS = [\n    'absl-py',\n    'six',\n]",
        "detail": "examples.setup",
        "documentation": {}
    },
    {
        "label": "DOCLINES",
        "kind": 5,
        "importPath": "examples.setup",
        "description": "examples.setup",
        "peekOfCode": "DOCLINES = __doc__.split('\\n')\nREQUIRED_PKGS = [\n    'absl-py',\n    'six',\n]\nTESTS_REQUIRE = [\n    'jupyter',\n]\nif sys.version_info.major == 3:\n  # Packages only for Python 3",
        "detail": "examples.setup",
        "documentation": {}
    },
    {
        "label": "REQUIRED_PKGS",
        "kind": 5,
        "importPath": "examples.setup",
        "description": "examples.setup",
        "peekOfCode": "REQUIRED_PKGS = [\n    'absl-py',\n    'six',\n]\nTESTS_REQUIRE = [\n    'jupyter',\n]\nif sys.version_info.major == 3:\n  # Packages only for Python 3\n  pass",
        "detail": "examples.setup",
        "documentation": {}
    },
    {
        "label": "TESTS_REQUIRE",
        "kind": 5,
        "importPath": "examples.setup",
        "description": "examples.setup",
        "peekOfCode": "TESTS_REQUIRE = [\n    'jupyter',\n]\nif sys.version_info.major == 3:\n  # Packages only for Python 3\n  pass\nelse:\n  # Packages only for Python 2\n  TESTS_REQUIRE.append('mock')\n  REQUIRED_PKGS.append('futures')  # concurrent.futures",
        "detail": "examples.setup",
        "documentation": {}
    },
    {
        "label": "detectPose",
        "kind": 2,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "def detectPose(image, pose, display=True):\n    output_image = image.copy()\n    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    results = pose.process(imageRGB)\n    height, width, _ = image.shape\n    landmarks = []\n    if results.pose_landmarks:\n        mp_drawing.draw_landmarks(\n            image=output_image, landmark_list=results.pose_landmarks, connections=mp_pose.POSE_CONNECTIONS)\n        for landmark in results.pose_landmarks.landmark:",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "calculateAngle",
        "kind": 2,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "def calculateAngle(landmark1, landmark2, landmark3):\n    x1, y1, _ = landmark1\n    x2, y2, _ = landmark2\n    x3, y3, _ = landmark3\n    angle = math.degrees(math.atan2(y3 - y2, x3 - x2) -\n                         math.atan2(y1 - y2, x1 - x2))\n    if angle < 0:\n        angle += 360\n    return angle\ndef inRange(angle, min, max):",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "inRange",
        "kind": 2,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "def inRange(angle, min, max):\n    if angle < min or angle > max:\n        return False\n    return True\ndef VIPSalute(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 150, 180) and inRange(right_elbow_angle, 0, 45) and inRange(left_shoulder_angle, 75, 120) and inRange(right_shoulder_angle, 75, 120):\n        return True\n    return False\ndef Wait(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 30, 90) and inRange(right_elbow_angle, 250, 350) and inRange(left_shoulder_angle, 85, 150) and inRange(right_shoulder_angle, 85, 150):",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "VIPSalute",
        "kind": 2,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "def VIPSalute(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 150, 180) and inRange(right_elbow_angle, 0, 45) and inRange(left_shoulder_angle, 75, 120) and inRange(right_shoulder_angle, 75, 120):\n        return True\n    return False\ndef Wait(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 30, 90) and inRange(right_elbow_angle, 250, 350) and inRange(left_shoulder_angle, 85, 150) and inRange(right_shoulder_angle, 85, 150):\n        return True\n    return False\ndef RightTurnWaiting(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 160, 200) and inRange(right_elbow_angle, 0, 20) and inRange(left_shoulder_angle, 90, 150) and inRange(right_shoulder_angle, 85, 130):",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "Wait",
        "kind": 2,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "def Wait(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 30, 90) and inRange(right_elbow_angle, 250, 350) and inRange(left_shoulder_angle, 85, 150) and inRange(right_shoulder_angle, 85, 150):\n        return True\n    return False\ndef RightTurnWaiting(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 160, 200) and inRange(right_elbow_angle, 0, 20) and inRange(left_shoulder_angle, 90, 150) and inRange(right_shoulder_angle, 85, 130):\n        return True\n    elif inRange(left_elbow_angle, 130, 200) and inRange(right_elbow_angle, 130, 200) and inRange(left_shoulder_angle, 250, 320) and inRange(right_shoulder_angle, 75, 130):\n        return True\n    return False",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "RightTurnWaiting",
        "kind": 2,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "def RightTurnWaiting(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 160, 200) and inRange(right_elbow_angle, 0, 20) and inRange(left_shoulder_angle, 90, 150) and inRange(right_shoulder_angle, 85, 130):\n        return True\n    elif inRange(left_elbow_angle, 130, 200) and inRange(right_elbow_angle, 130, 200) and inRange(left_shoulder_angle, 250, 320) and inRange(right_shoulder_angle, 75, 130):\n        return True\n    return False\ndef RightTurn(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 160, 200) and inRange(right_elbow_angle, 15, 40) and inRange(left_shoulder_angle, 90, 150) and inRange(right_shoulder_angle, 85, 130):\n        return True\n    elif inRange(left_elbow_angle, 300, 360) and inRange(right_elbow_angle, 150, 200) and inRange(left_shoulder_angle, 80, 130) and inRange(right_shoulder_angle, 75, 130):",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "RightTurn",
        "kind": 2,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "def RightTurn(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 160, 200) and inRange(right_elbow_angle, 15, 40) and inRange(left_shoulder_angle, 90, 150) and inRange(right_shoulder_angle, 85, 130):\n        return True\n    elif inRange(left_elbow_angle, 300, 360) and inRange(right_elbow_angle, 150, 200) and inRange(left_shoulder_angle, 80, 130) and inRange(right_shoulder_angle, 75, 130):\n        return True\n    return False\ndef LeftTurnWaiting(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 0, 20) and inRange(right_elbow_angle, 160, 200) and inRange(left_shoulder_angle, 85, 130) and inRange(right_shoulder_angle, 90, 150):\n        return True\n    elif inRange(left_elbow_angle, 130, 200) and inRange(right_elbow_angle, 130, 200) and inRange(left_shoulder_angle, 75, 130) and inRange(right_shoulder_angle, 250, 320):",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "LeftTurnWaiting",
        "kind": 2,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "def LeftTurnWaiting(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 0, 20) and inRange(right_elbow_angle, 160, 200) and inRange(left_shoulder_angle, 85, 130) and inRange(right_shoulder_angle, 90, 150):\n        return True\n    elif inRange(left_elbow_angle, 130, 200) and inRange(right_elbow_angle, 130, 200) and inRange(left_shoulder_angle, 75, 130) and inRange(right_shoulder_angle, 250, 320):\n        return True\n    return False\ndef LeftTurn(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 15, 40) and inRange(right_elbow_angle, 160, 200) and inRange(left_shoulder_angle, 85, 130) and inRange(right_shoulder_angle, 90, 150):\n        return True\n    elif inRange(left_elbow_angle, 150, 200) and inRange(right_elbow_angle, 300, 360) and inRange(left_shoulder_angle, 75, 130) and inRange(right_shoulder_angle, 80, 130):",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "LeftTurn",
        "kind": 2,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "def LeftTurn(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 15, 40) and inRange(right_elbow_angle, 160, 200) and inRange(left_shoulder_angle, 85, 130) and inRange(right_shoulder_angle, 90, 150):\n        return True\n    elif inRange(left_elbow_angle, 150, 200) and inRange(right_elbow_angle, 300, 360) and inRange(left_shoulder_angle, 75, 130) and inRange(right_shoulder_angle, 80, 130):\n        return True\n    elif inRange(left_elbow_angle, 150, 210) and inRange(right_elbow_angle, 150, 210) and inRange(left_shoulder_angle, 85, 150) and inRange(right_shoulder_angle, 180, 250):\n        return True\n    return False\ndef MoveStraight(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 170, 210) and inRange(right_elbow_angle, 170, 200) and inRange(left_shoulder_angle, 150, 200) and inRange(right_shoulder_angle, 0, 45):",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "MoveStraight",
        "kind": 2,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "def MoveStraight(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 170, 210) and inRange(right_elbow_angle, 170, 200) and inRange(left_shoulder_angle, 150, 200) and inRange(right_shoulder_angle, 0, 45):\n        return True\n    elif inRange(left_elbow_angle, 150, 200) and inRange(right_elbow_angle, 160, 200) and inRange(left_shoulder_angle, 60, 110) and inRange(right_shoulder_angle, 0, 30):\n        return True\n    elif inRange(left_elbow_angle, 160, 200) and inRange(right_elbow_angle, 160, 200) and inRange(left_shoulder_angle, 120, 180) and inRange(right_shoulder_angle, 120, 180):\n        return True\n    return False\ndef Stop(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 170, 200) and inRange(right_elbow_angle, 170, 210) and inRange(left_shoulder_angle, 0, 45) and inRange(right_shoulder_angle, 150, 200):",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "Stop",
        "kind": 2,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "def Stop(left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle):\n    if inRange(left_elbow_angle, 170, 200) and inRange(right_elbow_angle, 170, 210) and inRange(left_shoulder_angle, 0, 45) and inRange(right_shoulder_angle, 150, 200):\n        return True\n    elif inRange(left_elbow_angle, 160, 200) and inRange(right_elbow_angle, 150, 200) and inRange(left_shoulder_angle, 0, 30) and inRange(right_shoulder_angle, 60, 110):\n        return True\n    elif inRange(left_elbow_angle, 150, 200) and inRange(right_elbow_angle, 160, 220) and inRange(left_shoulder_angle, 70, 120) and inRange(right_shoulder_angle, 140, 180):\n        return True\n    elif inRange(left_elbow_angle, 160, 220) and inRange(right_elbow_angle, 150, 200) and inRange(left_shoulder_angle, 140, 180) and inRange(right_shoulder_angle, 70, 120):\n        return True\n    elif inRange(left_elbow_angle, 160, 220) and inRange(right_elbow_angle, 0, 20) and inRange(left_shoulder_angle, 140, 200) and inRange(right_shoulder_angle, 60, 120):",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "classifyPose",
        "kind": 2,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "def classifyPose(landmarks, output_image, display=True):\n    label = 'Unknown Pose'\n    color = (0, 0, 255)\n    left_elbow_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value],\n                                      landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value],\n                                      landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value])\n    right_elbow_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value],\n                                       landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value],\n                                       landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value])\n    left_shoulder_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value],",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "mp_pose",
        "kind": 5,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "mp_pose = mp.solutions.pose\npose = mp_pose.Pose(static_image_mode=True,\n                    min_detection_confidence=0.05, model_complexity=2)\nmp_drawing = mp.solutions.drawing_utils\ndef detectPose(image, pose, display=True):\n    output_image = image.copy()\n    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    results = pose.process(imageRGB)\n    height, width, _ = image.shape\n    landmarks = []",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "pose",
        "kind": 5,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "pose = mp_pose.Pose(static_image_mode=True,\n                    min_detection_confidence=0.05, model_complexity=2)\nmp_drawing = mp.solutions.drawing_utils\ndef detectPose(image, pose, display=True):\n    output_image = image.copy()\n    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    results = pose.process(imageRGB)\n    height, width, _ = image.shape\n    landmarks = []\n    if results.pose_landmarks:",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "mp_drawing",
        "kind": 5,
        "importPath": "ClassifyUsingDL",
        "description": "ClassifyUsingDL",
        "peekOfCode": "mp_drawing = mp.solutions.drawing_utils\ndef detectPose(image, pose, display=True):\n    output_image = image.copy()\n    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    results = pose.process(imageRGB)\n    height, width, _ = image.shape\n    landmarks = []\n    if results.pose_landmarks:\n        mp_drawing.draw_landmarks(\n            image=output_image, landmark_list=results.pose_landmarks, connections=mp_pose.POSE_CONNECTIONS)",
        "detail": "ClassifyUsingDL",
        "documentation": {}
    },
    {
        "label": "renameFile",
        "kind": 2,
        "importPath": "Rename",
        "description": "Rename",
        "peekOfCode": "def renameFile(directory, count):\n    for filename in os.listdir(directory):\n        if filename.endswith(\".jpg\"):\n            os.rename(\n                os.path.join(directory, filename),\n                os.path.join(directory, f\"image{str(count)}.jpg\"),\n            )\n            count += 1\n        elif filename.endswith(\".png\"):\n            os.rename(",
        "detail": "Rename",
        "documentation": {}
    },
    {
        "label": "renameTextFiles",
        "kind": 2,
        "importPath": "Rename",
        "description": "Rename",
        "peekOfCode": "def renameTextFiles(directory, count):\n    for filename in os.listdir(directory):\n        if filename.endswith(\".txt\"):\n            os.rename(\n                os.path.join(directory, filename),\n                os.path.join(directory, f\"image{str(count)}.txt\"),\n            )\n            count += 1\n    return count\ndef main():",
        "detail": "Rename",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Rename",
        "description": "Rename",
        "peekOfCode": "def main():\n    mainFolder = \"Dataset\"\n    folders = [mainFolder +\"\\\\\"+ folder for folder in os.listdir(mainFolder)]\n    print(folders)\n    count = 1\n    textCount = 1\n    for folder in folders:\n        count = renameFile(folder, count)\n        textCount = renameTextFiles(folder, textCount)\nif __name__ == \"__main__\":",
        "detail": "Rename",
        "documentation": {}
    }
]